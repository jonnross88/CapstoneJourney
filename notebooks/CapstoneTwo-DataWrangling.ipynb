{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This project is part of a Capstone project for Springboard Data Science Career Track.\n",
    "\n",
    "The goal of this project is to develop a machine learning model to rank and predict the likelihood that an oil company will initiate a frac job in a county within the Permian Basin in the first quarter of 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.3.1'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1;\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n  var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'jspanel': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/jspanel', 'jspanel-modal': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal', 'jspanel-tooltip': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip', 'jspanel-hint': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint', 'jspanel-layout': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout', 'jspanel-contextmenu': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu', 'jspanel-dock': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@7.2.3/dist/gridstack-all', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'jspanel': {'exports': 'jsPanel'}, 'gridstack': {'exports': 'GridStack'}}});\n      require([\"jspanel\"], function(jsPanel) {\n\twindow.jsPanel = jsPanel\n\ton_load()\n      })\n      require([\"jspanel-modal\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-tooltip\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-hint\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-layout\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-contextmenu\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-dock\"], function() {\n\ton_load()\n      })\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 9;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window['jsPanel'] !== undefined) && (!(window['jsPanel'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.3.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/jspanel.js', 'https://cdn.holoviz.org/panel/1.3.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal.js', 'https://cdn.holoviz.org/panel/1.3.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip.js', 'https://cdn.holoviz.org/panel/1.3.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint.js', 'https://cdn.holoviz.org/panel/1.3.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout.js', 'https://cdn.holoviz.org/panel/1.3.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu.js', 'https://cdn.holoviz.org/panel/1.3.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.3.1/dist/bundled/gridstack/gridstack@7.2.3/dist/gridstack-all.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.3.1/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.1.min.js\", \"https://cdn.holoviz.org/panel/1.3.1/dist/panel.min.js\", \"https://cdn.jsdelivr.net/npm/@holoviz/geoviews@1.11.0/dist/geoviews.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      Bokeh = root.Bokeh;\n      bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      if (!reloading && (!bokeh_loaded || is_dev)) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='p1005'>\n",
       "  <div id=\"fa7ca2c6-db17-4bdf-ad25-a3b96a3b7813\" data-root-id=\"p1005\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"ec082c02-1da6-4924-9903-853a012d29b0\":{\"version\":\"3.3.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"p1005\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"p1006\",\"attributes\":{\"plot_id\":\"p1005\",\"comm_id\":\"f49ba18d5783459a811ba868fe7daacb\",\"client_comm_id\":\"c2d04adec9bb40cca7885f6701f5f071\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"toggle_value1\",\"properties\":[{\"name\":\"active_icons\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"options\",\"kind\":\"Any\",\"default\":{\"type\":\"map\",\"entries\":[[\"favorite\",\"heart\"]]}},{\"name\":\"value\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"_reactions\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"_base_url\",\"kind\":\"Any\",\"default\":\"https://tabler-icons.io/static/tabler-icons/icons/\"}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"value\",\"kind\":\"Any\",\"default\":null},{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]}]}};\n",
       "  var render_items = [{\"docid\":\"ec082c02-1da6-4924-9903-853a012d29b0\",\"roots\":{\"p1005\":\"fa7ca2c6-db17-4bdf-ad25-a3b96a3b7813\"},\"root_ids\":[\"p1005\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  const is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1\n",
       "  function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && (id_el.children[0].className === 'bk-root')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version && !is_dev) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "p1005"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.3.1'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1;\n  var reloading = true;\n  var Bokeh = root.Bokeh;\n  var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'jspanel': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/jspanel', 'jspanel-modal': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal', 'jspanel-tooltip': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip', 'jspanel-hint': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint', 'jspanel-layout': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout', 'jspanel-contextmenu': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu', 'jspanel-dock': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@7.2.3/dist/gridstack-all', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'jspanel': {'exports': 'jsPanel'}, 'gridstack': {'exports': 'GridStack'}}});\n      require([\"jspanel\"], function(jsPanel) {\n\twindow.jsPanel = jsPanel\n\ton_load()\n      })\n      require([\"jspanel-modal\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-tooltip\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-hint\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-layout\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-contextmenu\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-dock\"], function() {\n\ton_load()\n      })\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 9;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window['jsPanel'] !== undefined) && (!(window['jsPanel'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.3.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/jspanel.js', 'https://cdn.holoviz.org/panel/1.3.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal.js', 'https://cdn.holoviz.org/panel/1.3.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip.js', 'https://cdn.holoviz.org/panel/1.3.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint.js', 'https://cdn.holoviz.org/panel/1.3.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout.js', 'https://cdn.holoviz.org/panel/1.3.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu.js', 'https://cdn.holoviz.org/panel/1.3.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.3.1/dist/bundled/gridstack/gridstack@7.2.3/dist/gridstack-all.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.3.1/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.jsdelivr.net/npm/@holoviz/geoviews@1.11.0/dist/geoviews.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      Bokeh = root.Bokeh;\n      bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      if (!reloading && (!bokeh_loaded || is_dev)) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.3.1'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1;\n  var reloading = true;\n  var Bokeh = root.Bokeh;\n  var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'jspanel': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/jspanel', 'jspanel-modal': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal', 'jspanel-tooltip': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip', 'jspanel-hint': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint', 'jspanel-layout': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout', 'jspanel-contextmenu': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu', 'jspanel-dock': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@7.2.3/dist/gridstack-all', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'jspanel': {'exports': 'jsPanel'}, 'gridstack': {'exports': 'GridStack'}}});\n      require([\"jspanel\"], function(jsPanel) {\n\twindow.jsPanel = jsPanel\n\ton_load()\n      })\n      require([\"jspanel-modal\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-tooltip\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-hint\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-layout\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-contextmenu\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-dock\"], function() {\n\ton_load()\n      })\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 9;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window['jsPanel'] !== undefined) && (!(window['jsPanel'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.3.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/jspanel.js', 'https://cdn.holoviz.org/panel/1.3.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal.js', 'https://cdn.holoviz.org/panel/1.3.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip.js', 'https://cdn.holoviz.org/panel/1.3.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint.js', 'https://cdn.holoviz.org/panel/1.3.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout.js', 'https://cdn.holoviz.org/panel/1.3.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu.js', 'https://cdn.holoviz.org/panel/1.3.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.3.1/dist/bundled/gridstack/gridstack@7.2.3/dist/gridstack-all.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.3.1/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.jsdelivr.net/npm/@holoviz/geoviews@1.11.0/dist/geoviews.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      Bokeh = root.Bokeh;\n      bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      if (!reloading && (!bokeh_loaded || is_dev)) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import statements\n",
    "import concurrent.futures as cf\n",
    "import random\n",
    "import re\n",
    "import tempfile\n",
    "import warnings\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "import missingno as msno\n",
    "import cartopy.crs as ccrs\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import geopandas as gpd\n",
    "import geoviews as gv\n",
    "import geoviews.tile_sources as gts\n",
    "import colorcet as cc\n",
    "import holoviews as hv\n",
    "import hvplot.pandas  # noqa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyproj\n",
    "from fiona.io import ZipMemoryFile\n",
    "from pyvis.network import Network\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "gv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CapstoneJourney begins!\n"
     ]
    }
   ],
   "source": [
    "# Test initial print statement\n",
    "print(\"CapstoneJourney begins!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants\n",
    "Let's start by defining some constants that will be used throughout this notebook.\n",
    "\n",
    "Most of the data was first downloaded from external websites and then uploaded onto a cloud storage bucket. This was done to ensure consistency and availability during the project. A brief description of the data and its original source link is referenced below.\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "The following table provides an overview of the data sources used in this project:\n",
    "\n",
    "| Dataset Name | Source URL | Original Source | Description | Date Downloaded |\n",
    "|--------------|------------|-----------------|-------------|-----------------|\n",
    "|RegistryUpload Table | [link](https://fracfocus.org/data-download) | FracFocus | This table contains each disclosures header information such as the job date, API number, location, base water volume, and total vertical depth. | 2023-11-11 |\n",
    "RBDMSWells | [link](https://gisdata-occokc.opendata.arcgis.com/datasets/OCCOKC::rbdms-wells/about) | Oklahoma Corporation Commission | This table contains Oklahoma RBDMS statewide well data | 2023-11-23 |\n",
    "| Wolfcamp Delaware Play Boundary | [link]((https://www.eia.gov/maps/maps.htm))| EIA | Permian Basin, Delaware Sub-Basin: Wolfcamp play boundary (9/4/2018) | 2023-11-19 |\n",
    "| Wolfcamp Midland Play Boundaries | [link]((https://www.eia.gov/maps/maps.htm))| EIA | Wolfcamp A, B, C, and D play boundaries, Midland Basin (6/4/2020) | 2023-11-21 |\n",
    "| ShalePlay Delaware | [link]((https://www.eia.gov/maps/maps.htm))| EIA |Delaware play boundary (10/8/2019)  | 2023-11-21 |\n",
    "| AboYeso GlorietaYeso Spraberry | [link]((https://www.eia.gov/maps/maps.htm))| EIA | Abo-Yeso, Glorieta-Yeso, and Spraberry play boundaries (3/11/2016) | 2023-11-21 |\n",
    "| NM SLO OilGas Leases | [link](https://www.nmstatelands.org/maps-gis/gis-data-download/)| New Mexico State Land Office | Active Oil and Gas Leases (11/07/2023) | 2023-11-21 |\n",
    "| NM SLO Geologic Regions | [link]((https://www.nmstatelands.org/maps-gis/gis-data-download/))| New Mexico State Land Office | Geologic Regions (01/04/2010) | 2023-11-21 |\n",
    "| NM SLO STL Status Combined | [link]((https://www.nmstatelands.org/maps-gis/gis-data-download/))| New Mexico State Land Office | New Mexico State Trust Lands By Subdivision (04/14/2022) | 2023-11-21 |\n",
    "| All Layers By County | [link](https://rrc.texas.gov/resource-center/research/data-sets-available-for-download/)  | Railroad Commission of Texas | Map & Associated Data: Base Map, Wells, Surveys & Pipelines layers | 2023-11-17 |\n",
    "| Oil & Gas Leases | [link](https://www.glo.texas.gov/land/land-management/gis/index.html) | Texas General Land Office | Active Leases (11/17/2023) | 2023-11-17 |\n",
    "| Oil & Gas Units | [link](https://www.glo.texas.gov/land/land-management/gis/index.html) | Texas General Land Office | Active Units (11/17/2023) | 2023-11-17 |\n",
    "| U.S. County Boundaries | [link](https://www2.census.gov/geo/tiger/TIGER2022/COUNTY/tl_2022_us_county.zip) | United States Census Bureau | County (2022-10-31). Data is downloaded directly in the code. | N/A |\n",
    "| U.S. County FIPS Codes | [link](https://en.wikipedia.org/wiki/List_of_United_States_FIPS_codes_by_county) | Wikipedia | List of United States FIPS codes by county. Data is downloaded directly in the code. | N/A |\n",
    "\n",
    "Each row in the table represents a different dataset. The columns are:\n",
    "\n",
    "- **Dataset Name**: The name of the dataset.\n",
    "- **Source URL**: The URL where the dataset can be downloaded. Click on \"link\" to access the webpage.\n",
    "- **Original Source**: The original source of the data.\n",
    "- **Description**: A brief description of the dataset.\n",
    "- **Date Downloaded**: The date when the dataset was downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "# This cell generates lists of URLs to CSV files stored in a Google Cloud Storage bucket.\n",
    "# The CSV files contain data from the FracFocus Chemical Disclosure Registry.\n",
    "\n",
    "# Generate a list of URLs to the FracFocusRegistry CSV files.\n",
    "# There are 24 files in total, named FracFocusRegistry_i.csv where i ranges from 1 to 24.\n",
    "DATA_URLS1 = [\n",
    "    f\"https://storage.googleapis.com/mrprime_dataset/fracfocus/FracFocusRegistry_{i}.csv\"\n",
    "    for i in range(1, 25)\n",
    "]\n",
    "\n",
    "# Generate a list of URLs to the registryupload CSV files.\n",
    "# There are 3 files in total, named registryupload_i.csv where i ranges from 1 to 3.\n",
    "DATA_URLS2 = [\n",
    "    f\"https://storage.googleapis.com/mrprime_dataset/fracfocus/registryupload_{j}.csv\"\n",
    "    for j in range(1, 4)\n",
    "]\n",
    "\n",
    "# URL to the readme.txt file in the bucket.\n",
    "DATA_README_URL = [\n",
    "    \"https://storage.googleapis.com/mrprime_dataset/fracfocus/readme.txt\"\n",
    "]\n",
    "\n",
    "# url to the OCC (Oklahoma) well data in th bucket\n",
    "OCC_PARQUET_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/occ/rbdms_wells.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Url for the shapefile for US counties from the Census Bureau's website.\n",
    "CENSUS_COUNTY_MAP_URL = (\n",
    "    \"https://www2.census.gov/geo/tiger/TIGER2022/COUNTY/tl_2022_us_county.zip\"\n",
    ")\n",
    "# Url for a Wikipedia page containing a table of FIPS codes for US counties.\n",
    "FIPS_WIKI_URL = (\n",
    "    \"https://en.wikipedia.org/wiki/List_of_United_States_FIPS_codes_by_county\"\n",
    ")\n",
    "# Bounds of the continental US in longitude and latitude.\n",
    "USA_BOUNDS = (-124.77, 24.52, -66.95, 49.38)\n",
    "# bounds of the continental US in Web Mercator coordinates.\n",
    "USA_BOUNDS_MERCATOR = (-13874905.0, 2870341.0, -7453304.0, 6338219.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# url for the shapefiles of Permian Basin, Delaware Sub-Basin: Wolfcamp play boundary\n",
    "WOLFCAMP_ZIP_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/eia/Wolfcamp_Delaware_Play_Boundary.zip\"\n",
    "MIDLAND_ZIP_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/eia/Wolfcamp_Midland_Play_Boundaries_EIA.zip\"\n",
    "DELAWARE_ZIP_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/eia/ShalePlay_Delaware_EIA.zip\"\n",
    "ABOYESO_ZIP_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/eia/ShalePlays_AboYeso_GlorietaYeso_Spraberry_EIA.zip\"\n",
    "# PB_ZIP_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/eia/PermianBasin_Boundary_Structural_Tectonic.zip\"\n",
    "\n",
    "basins_url_list = [\n",
    "    WOLFCAMP_ZIP_URL,\n",
    "    MIDLAND_ZIP_URL,\n",
    "    DELAWARE_ZIP_URL,\n",
    "    ABOYESO_ZIP_URL,\n",
    "    # PB_ZIP_URL,\n",
    "]\n",
    "\n",
    "\n",
    "# url for shapefiles of Polygon data set intended to delineate active oil and gas leases on New Mexico State Trust Lands.\n",
    "NM_SLO_OIL_LEASE_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/nm_slo/OilGas_Leases.zip\"\n",
    "\n",
    "# url for shapefiles of Polygon layer created to highlight general boundaries of subsurface geologic basins and uplifts of New Mexico\n",
    "NM_SLO_GEO_REGION_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/nm_slo/slo_GeologicRegions.zip\"\n",
    "# url for shapefiles of Polygons of New Mexico State Trust Lands by PLSS subdivision (quarter-quarter, lot, tract, or partial).\n",
    "NM_SLO_STL_PLSS_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/nm_slo/slo_STLStatusCombined.zip\"\n",
    "\n",
    "nm_slo_url_list = [\n",
    "    NM_SLO_OIL_LEASE_URL,\n",
    "    NM_SLO_GEO_REGION_URL,\n",
    "]  # , NM_SLO_STL_PLSS_URL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Define a list of county numbers that we want to test. These numbers correspond to counties\n",
    "# that we did not include in the data folder, but they do not cover all 254 counties.\n",
    "\n",
    "# county numbers are only odd numbers\n",
    "county_nums = [str(i).zfill(3) for i in range(1, 508) if i % 2]\n",
    "\n",
    "# Generate a list of URLs to the shapefile zip files stored in a Google Cloud Storage bucket.\n",
    "# The zip files are named Shp{num}.zip, where {num} is a county number from the county_nums list.\n",
    "SHP_ZIP_URLS = [\n",
    "    f\"https://storage.googleapis.com/mrprime_dataset/capstone_journey/rrc/all_layers_rrc_20231117/Shp{num}.zip\"\n",
    "    for num in county_nums\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# url for the active leases in Texas on State land gdb\n",
    "GDB_ZIP_URLS = [\n",
    "    \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/glo/GDB_ActiveLeases.zip\",\n",
    "    \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/glo/GDB_ActiveUnits.zip\",\n",
    "    # \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/glo/GDB_InactiveLeases.zip\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function definations\n",
    "Next, let's define some functions that will be used throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def read_csv_concurrent(urls_list):\n",
    "    \"\"\"Reads a list of CSV files concurrently\"\"\"\n",
    "    # Create a thread pool\n",
    "    with cf.ThreadPoolExecutor() as executor:\n",
    "        # Use map to apply pd.read_csv to each URL\n",
    "        results = list(tqdm(executor.map(pd.read_csv, urls_list), total=len(urls_list)))\n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def extract_specific_gdf_from_local_zip(\n",
    "    zip_paths: list[str], regex_patterns: list[str]\n",
    ") -> dict[str, gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Reads shapefiles from a list of zip files and returns a dictionary\n",
    "    where the keys are the names of the shapefiles and the values are GeoDataFrames.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store the GeoDataFrames\n",
    "    shp_dict = {}\n",
    "    # compile the regex patterns\n",
    "    patterns = [re.compile(pattern) for pattern in regex_patterns]\n",
    "\n",
    "    # Loop over the list of zip file paths\n",
    "    for zip_path in zip_paths:\n",
    "        # Open the zip file\n",
    "        with ZipFile(zip_path) as z:\n",
    "            # Get the list of files in the zip file\n",
    "            zip_contents = z.namelist()\n",
    "            # Filter the list to get only the shapefiles that match any of the patterns\n",
    "            shp_files = [\n",
    "                f\n",
    "                for f in zip_contents\n",
    "                for pattern in patterns\n",
    "                if pattern.search(f) and f.endswith(\".shp\")\n",
    "            ]\n",
    "            # read the shapefiles into GeoDataFrames\n",
    "            for shp_file in shp_files:\n",
    "                # Get the name of the shapefile\n",
    "                shp_name = Path(shp_file).stem\n",
    "                # Read the shapefile into a GeoDataFrame and add it to the dictionary\n",
    "                shp_dict[shp_name] = gpd.read_file(f\"zip://{zip_path}!{shp_file}\")\n",
    "    # Return the dictionary of GeoDataFrames\n",
    "    return shp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def extract_matching_shp_files_from_zip_urls(\n",
    "    zip_urls: list[str], regex_patterns: list[str]\n",
    ") -> dict[str, gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Reads shapefiles from a list of zip file urls and returns a dictionary\n",
    "    where the keys are the names of the shapefiles and the values are GeoDataFrames.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store the GeoDataFrames\n",
    "    shp_dict = {}\n",
    "    # compile the regex patterns\n",
    "    patterns = [re.compile(pattern) for pattern in regex_patterns]\n",
    "\n",
    "    # Loop over the list of zip file urls\n",
    "    for zip_url in tqdm(zip_urls, desc=\"Processing zip files\"):\n",
    "        # download the zip file\n",
    "        with urlopen(zip_url) as u:\n",
    "            zip_data = u.read()\n",
    "        # create a ZipMemoryFile from the zip data\n",
    "        with ZipMemoryFile(zip_data) as z:\n",
    "            # get the list of files in the zip file\n",
    "            zip_files = z.listdir()\n",
    "            # filter for shapefiles that match any of the patterns\n",
    "            shp_files = [\n",
    "                f\n",
    "                for f in zip_files\n",
    "                for pattern in patterns\n",
    "                if pattern.search(f) and f.endswith(\".shp\")\n",
    "            ]\n",
    "            # read the shapefiles into GeoDataFrames\n",
    "            for shp_file in shp_files:\n",
    "                with z.open(shp_file) as f:\n",
    "                    shp_dict[Path(shp_file).stem] = gpd.GeoDataFrame.from_features(\n",
    "                        f, crs=f.crs\n",
    "                    )\n",
    "    # Return the dictionary of GeoDataFrames\n",
    "    return shp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def process_zip_url(\n",
    "    zip_url: str, patterns: list[re.Pattern]\n",
    ") -> dict[str, gpd.GeoDataFrame]:\n",
    "    \"\"\"Downloads a zip file url and returns a dictionary of GeoDataFrames for shapefiles that match the patterns\"\"\"\n",
    "    shp_dict = {}\n",
    "    with urlopen(zip_url) as u:\n",
    "        zip_data = u.read()\n",
    "    with ZipMemoryFile(zip_data) as z:\n",
    "        zip_files = z.listdir()\n",
    "        shp_files = [\n",
    "            f\n",
    "            for f in zip_files\n",
    "            for pattern in patterns\n",
    "            if pattern.search(f) and f.endswith(\".shp\")\n",
    "        ]\n",
    "        for shp_file in shp_files:\n",
    "            with z.open(shp_file) as f:\n",
    "                shp_dict[Path(shp_file).stem] = gpd.GeoDataFrame.from_features(\n",
    "                    f, crs=f.crs\n",
    "                )\n",
    "    return shp_dict\n",
    "\n",
    "\n",
    "def extract_matching_shp_files_from_zip_urls_concurrent(\n",
    "    zip_urls: list[str], regex_patterns: list[str]\n",
    ") -> dict[str, gpd.GeoDataFrame]:\n",
    "    \"\"\"Reads shapefiles from a list of zip file urls and returns a dictionary\n",
    "    where the keys are the names of the shapefiles and the values are GeoDataFrames.\"\"\"\n",
    "    shp_dict = {}\n",
    "    patterns = [re.compile(pattern) for pattern in regex_patterns]\n",
    "    with cf.ThreadPoolExecutor() as executor:\n",
    "        future_to_url = {\n",
    "            executor.submit(process_zip_url, url, patterns): url for url in zip_urls\n",
    "        }\n",
    "        futures = tqdm(\n",
    "            cf.as_completed(future_to_url),\n",
    "            total=len(future_to_url),\n",
    "            desc=\"Processing URLs\",\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "        for future in futures:\n",
    "            shp_dict.update(future.result())\n",
    "    return shp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def concat_gdf_from_dict(gdf_dict: dict[str, gpd.GeoDataFrame]) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Given a dictionary of GeoDataFrames, returns a single GeoDataFrame\n",
    "    with a new column indicating the source of the data.\n",
    "    \"\"\"\n",
    "    # use a dictionary comprehension to create a new dictionary\n",
    "    gdf_data = {k: gdf.assign(source_file=k) for k, gdf in gdf_dict.items()}\n",
    "    # return the concatenated GeoDataFrame\n",
    "    return pd.concat(gdf_data.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def extract_gdfs_from_zip(zip_path: str) -> Optional[dict[str, gpd.GeoDataFrame]]:\n",
    "    \"\"\"\n",
    "    Reads shapefiles from a zip file and returns a dictionary of GeoDataFrames.\n",
    "    \"\"\"\n",
    "    gdfs = {}\n",
    "    # Open the zip file\n",
    "    with ZipFile(zip_path) as z:\n",
    "        # Get the list of files in the zip file\n",
    "        zip_contents = z.namelist()\n",
    "        # Find the shapefiles\n",
    "        shp_files = [f for f in zip_contents if f.endswith(\".shp\")]\n",
    "        for shp_file in shp_files:\n",
    "            # Read the shapefile into a GeoDataFrame\n",
    "            gdf = gpd.read_file(f\"zip://{zip_path}!{shp_file}\")\n",
    "            gdfs[shp_file] = gdf\n",
    "\n",
    "    # If no shapefile was found, return None\n",
    "    return gdfs if gdfs else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def extract_gdfs_from_zip_url(zip_url: str) -> Optional[dict[str, gpd.GeoDataFrame]]:\n",
    "    \"\"\"\n",
    "    Downloads a ZIP file from a URL, reads shapefiles from the ZIP file, and returns a dictionary of GeoDataFrames.\n",
    "    \"\"\"\n",
    "    gdfs = {}\n",
    "    # Open the URL\n",
    "    with urlopen(zip_url) as u:\n",
    "        # Read the content of the response into a byte stream\n",
    "        zip_data = u.read()\n",
    "        # Open the ZIP file from the byte stream\n",
    "        with ZipMemoryFile(zip_data) as z:\n",
    "            # Get the list of files in the ZIP file\n",
    "            zip_contents = z.listdir()\n",
    "            # Find the shapefiles\n",
    "            shp_files = [f for f in zip_contents if f.endswith(\".shp\")]\n",
    "            for shp_file in shp_files:\n",
    "                # Read the shapefile into a GeoDataFrame\n",
    "                with z.open(shp_file) as f:\n",
    "                    gdf = gpd.GeoDataFrame.from_features(f, crs=f.crs)\n",
    "                gdfs[Path(shp_file).stem] = gdf\n",
    "\n",
    "    # If no shapefile was found, return None\n",
    "    return gdfs if gdfs else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def process_shp_url(zip_url: str):\n",
    "    \"\"\"Downloads a zip file url and returns a dictionary of GeoDataFrames for shapefiles that match the patterns\"\"\"\n",
    "    shp_dict = {}\n",
    "    with urlopen(zip_url) as u:\n",
    "        zip_data = u.read()\n",
    "    with ZipMemoryFile(zip_data) as z:\n",
    "        zip_files = z.listdir()\n",
    "        shp_files = [f for f in zip_files if f.endswith(\".shp\")]\n",
    "        for shp_file in shp_files:\n",
    "            with z.open(shp_file) as f:\n",
    "                shp_dict[Path(shp_file).stem] = gpd.GeoDataFrame.from_features(\n",
    "                    f, crs=f.crs\n",
    "                )\n",
    "    return shp_dict\n",
    "\n",
    "\n",
    "def extract_gdfs_from_zip_url_concurrent(\n",
    "    zip_urls: list[str],\n",
    ") -> dict[str, gpd.GeoDataFrame]:\n",
    "    \"\"\"Reads shapefiles from a list of zip file urls and returns a dictionary\n",
    "    where the keys are the names of the shapefiles and the values are GeoDataFrames.\"\"\"\n",
    "    shp_dict = {}\n",
    "    with cf.ThreadPoolExecutor() as executor:\n",
    "        future_to_url = {executor.submit(process_shp_url, url): url for url in zip_urls}\n",
    "        futures = tqdm(\n",
    "            cf.as_completed(future_to_url),\n",
    "            total=len(future_to_url),\n",
    "            desc=\"Processing URLs\",\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "        for future in futures:\n",
    "            shp_dict.update(future.result())\n",
    "    return shp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def read_gdb_from_zip(gdb_zips_list: list[str]):\n",
    "    \"\"\"Reads a list of zip files containing geodatabases and returns a dictionary of GeoDataFrames\"\"\"\n",
    "    # initialize an empty dictionary\n",
    "    gdb_dict = {}\n",
    "    # loop through each zip file\n",
    "    for gdb_zip in gdb_zips_list:\n",
    "        with ZipFile(gdb_zip, \"r\") as z:\n",
    "            # get list of files in zip\n",
    "            files = z.namelist()\n",
    "            # filter for gdb folders\n",
    "            gdb_folders = [f for f in files if f.endswith(\".gdb/\")]\n",
    "            # if there is a gdb folder in the zip file\n",
    "            if gdb_folders:\n",
    "                # get it and read it into a GeoDataFrame\n",
    "                gdb_folder = gdb_folders[0]\n",
    "                gdb_dict[Path(gdb_folder).stem] = gpd.read_file(\n",
    "                    f\"zip://{gdb_zip}!{gdb_folder}\"\n",
    "                ).to_crs(\"EPSG:4269\")\n",
    "    # return the dictionary of GeoDataFrames\n",
    "    return gdb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def read_gdb_from_zip_url(gdb_urls_list: list[str]):\n",
    "    \"\"\"Reads a list of zip file urls containing geodatabases and returns a dictionary of GeoDataFrames\"\"\"\n",
    "    # initialize an empty dictionary\n",
    "    gdb_dict = {}\n",
    "    # loop through each zip file\n",
    "    for gdb_url in gdb_urls_list:\n",
    "        # create a temporary directory\n",
    "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "            # download the zip file\n",
    "            with urlopen(gdb_url) as u, open(f\"{tmp_dir}/data.zip\", \"wb\") as f_out:\n",
    "                f_out.write(u.read())\n",
    "            # extract the zip file\n",
    "            with ZipFile(f\"{tmp_dir}/data.zip\", \"r\") as zip_ref:\n",
    "                zip_ref.extractall(tmp_dir)\n",
    "            # get the list of extracted files\n",
    "            extracted_files = list(Path(tmp_dir).iterdir())\n",
    "            # filter for gdb folders\n",
    "            gdb_folders = [f for f in extracted_files if f.suffix == \".gdb\"]\n",
    "            # if there is a gdb folder in the extracted files\n",
    "            if gdb_folders:\n",
    "                # get it and read it into a GeoDataFrame\n",
    "                gdb_folder = gdb_folders[0]\n",
    "                gdb_dict[Path(gdb_folder).stem] = gpd.read_file(gdb_folder).to_crs(\n",
    "                    \"EPSG:4269\"\n",
    "                )\n",
    "    # return the dictionary of GeoDataFrames\n",
    "    return gdb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def process_gdb_url(gdb_url):\n",
    "    \"\"\"Downloads a zip file url containing a geodatabase and returns a GeoDataFrame\"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        # download the zip file\n",
    "        with urlopen(gdb_url) as u, open(f\"{tmp_dir}/data.zip\", \"wb\") as f_out:\n",
    "            f_out.write(u.read())\n",
    "        # extract the zip file\n",
    "        with ZipFile(f\"{tmp_dir}/data.zip\", \"r\") as zip_ref:\n",
    "            zip_ref.extractall(tmp_dir)\n",
    "        # get the list of extracted files\n",
    "        extracted_files = list(Path(tmp_dir).iterdir())\n",
    "        # filter for gdb folders\n",
    "        gdb_folders = [f for f in extracted_files if f.suffix == \".gdb\"]\n",
    "        # if there is a gdb folder in the extracted files\n",
    "        if gdb_folders:\n",
    "            # get it and read it into a GeoDataFrame\n",
    "            gdb_folder = gdb_folders[0]\n",
    "            return Path(gdb_folder).stem, gpd.read_file(gdb_folder)\n",
    "\n",
    "\n",
    "def read_gdb_from_zip_url_concurrent(gdb_urls_list: list[str]):\n",
    "    \"\"\"Reads a list of zip file urls containing geodatabases and returns a dictionary of GeoDataFrames\"\"\"\n",
    "    # initialize an empty dictionary\n",
    "    gdb_dict = {}\n",
    "    # create a ThreadPoolExecutor\n",
    "    with cf.ThreadPoolExecutor() as executor:\n",
    "        # submit the process_gdb_url function for each url and gather the results\n",
    "        future_to_url = {\n",
    "            executor.submit(process_gdb_url, url): url for url in gdb_urls_list\n",
    "        }\n",
    "        for future in cf.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                key, data = future.result()\n",
    "                gdb_dict[key] = data\n",
    "            except Exception as exc:\n",
    "                print(f\"{url} generated an exception: {exc}\")\n",
    "    # return the dictionary of GeoDataFrames\n",
    "    return gdb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "def pascal_to_snake(name: str):\n",
    "    \"\"\"Converts a string from PascalCase to snake_case\"\"\"\n",
    "    # (?<=[A-Za-z0-9]) - positive lookbehind for any alphanumeric character\n",
    "    # (?=[A-Z][a-z]) - positive lookahead for any uppercase followed by lowercase\n",
    "    pattern = re.compile(r\"(?<=[A-Za-z0-9])(?=[A-Z][a-z])\")\n",
    "    name = pattern.sub(\"_\", name).lower()\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def unify_crs(\n",
    "    dataframe: pd.DataFrame,\n",
    "    lon_col: str = \"longitude\",\n",
    "    lat_col: str = \"latitude\",\n",
    "    crs_col: str = \"crs\",\n",
    "    final_crs: str = \"EPSG:4269\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a DataFrame with lon/lat or x/y coordinates,\n",
    "    converts the coordinates to a unified crs and combines\n",
    "    into a single GeoDataframe with a geometry column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the main columns that will be used for the conversion\n",
    "    main_cols = [lon_col, lat_col, crs_col]\n",
    "\n",
    "    # Get the other columns in the dataframe\n",
    "    other_cols = list(set(dataframe.columns) - set(main_cols))\n",
    "\n",
    "    # Create a subframe with only the main columns\n",
    "    subframe = dataframe[main_cols]\n",
    "\n",
    "    # Create a list of GeoDataFrames, each with a different CRS\n",
    "    geo_dfs = [\n",
    "        gpd.GeoDataFrame(\n",
    "            # Use the data for this CRS\n",
    "            data=data,\n",
    "            # Create a geometry column from the lon/lat columns\n",
    "            geometry=gpd.points_from_xy(x=data[lon_col].values, y=data[lat_col].values),\n",
    "            # Set the CRS for this GeoDataFrame\n",
    "            crs=pyproj.CRS(crs_val),\n",
    "            # Convert the GeoDataFrame to the final CRS\n",
    "        ).to_crs(final_crs)\n",
    "        # Do this for each unique CRS in the subframe\n",
    "        for crs_val, data in subframe.groupby(crs_col)\n",
    "    ]\n",
    "\n",
    "    # Merge the GeoDataFrames back together and return the result\n",
    "    return pd.merge(\n",
    "        # Concatenate the GeoDataFrames\n",
    "        pd.concat(geo_dfs, sort=True),\n",
    "        # Add the other columns back in\n",
    "        dataframe[other_cols],\n",
    "        # Merge on the index\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# @lru_cache(maxsize=3)\n",
    "def get_background_map(bgcolor=\"black\", alpha=0.5):\n",
    "    \"\"\"Returns a GeoViews background map\"\"\"\n",
    "    return gts.CartoLight().opts(bgcolor=bgcolor, alpha=alpha)\n",
    "\n",
    "\n",
    "def platecaree_to_mercator_vectorised(x, y):\n",
    "    \"\"\"Use Cartopy to convert PlateCarree coordinates to Mercator\"\"\"\n",
    "    return ccrs.GOOGLE_MERCATOR.transform_points(ccrs.PlateCarree(), x, y)[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_in_000(num):\n",
    "    \"\"\"Formats a number in thousands\"\"\"\n",
    "    for unit in [\"\", \"thousand\", \"million\", \"billion\", \"trillion\"]:\n",
    "        if abs(num) < 1000.0:\n",
    "            return f\"{num:3.2f} {unit}\"\n",
    "        num /= 1000.0\n",
    "    return f\"{num:.2f} quadrillion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_datetime(df, column):\n",
    "    \"\"\"Splits a datetime column into year, month, and day columns\"\"\"\n",
    "    # remove '_date' from the column name\n",
    "    column_stem = column.replace(\"_date\", \"\") if \"_date\" in column else column\n",
    "    try:\n",
    "        datetime_series = pd.to_datetime(df[column], errors=\"coerce\")\n",
    "        if datetime_series.isna().any():\n",
    "            print(f\"Errors occurred during conversion of column {column}.\")\n",
    "        df[column_stem + \"_year\"] = datetime_series.dt.year\n",
    "        df[column_stem + \"_month\"] = datetime_series.dt.month\n",
    "        df[column_stem + \"_day\"] = datetime_series.dt.day\n",
    "    except KeyError:\n",
    "        print(f\"Column {column} not found in the DataFrame.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First dataset is from FracFocus. There is also a readme file which contains the data dictionary for the dataset. Let's have a look at both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readme file with data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# get readme data\n",
    "readme = urlopen(DATA_README_URL[0]).read().decode(\"windows-1252\")\n",
    "display(readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print function goes beyond 'hello world' and takes care of the escape characters\n",
    "print(readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# We can collect all the dataframe into a list and then concatenate them\n",
    "df_list = read_csv_concurrent(DATA_URLS2)\n",
    "\n",
    "\n",
    "dfs = pd.concat(df_list).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "registry_df = pd.DataFrame()\n",
    "registry_df = dfs.copy()\n",
    "registry_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the missing values it is interesting to see that most missing values are from the `TVD`, `TotalBaseWaterVolume` and `TotalBaseNonWaterVolume`. One reason for this may be found in the data limitations on terms of use on the FracFocus website. It states:\n",
    "-  Disclosures submitted using the FracFocus 1.0 format (January, 2011 to May 31, 2013) will contain only header data. \n",
    "-  Disclosures submitted using the FracFocus 2.0 format (November 2012 to present) will contain both header and chemical data. NOTE: Between November, 2012 and May 31, 2013 disclosures in both 1.0 and 2.0 formats were submitted to the system. \n",
    "-  After May 31, 2013 only disclosures submitted in the 2.0 format were accepted.\n",
    "-  Data submitted appears as it was submitted by the operator or operators authorized agent. FracFocus does not warrant the data in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of non-missing values in each column\n",
    "missing_data_percent = (registry_df.notna().mean() * 100).rename(\"Percent\")\n",
    "\n",
    "# Create a DataFrame of the counts of non-missing values\n",
    "non_missing_count = registry_df.notna().sum().rename(\"Count\")\n",
    "\n",
    "# Concatenate the two DataFrames along the columns\n",
    "non_missing_data = pd.concat([missing_data_percent, non_missing_count], axis=1)\n",
    "\n",
    "# Create a horizontal bar plot of the percentage of non-missing data\n",
    "hbar_plot = non_missing_data.hvplot.barh(\n",
    "    y=\"Percent\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    title=\"Percentage of Non-Missing Data in Each Column\",\n",
    "    ylabel=\"\",\n",
    "    xlabel=\"\",\n",
    "    xaxis=\"bare\",\n",
    "    hover_cols=\"all\",\n",
    ").opts(\n",
    "    active_tools=[\"box_zoom\"],\n",
    "    toolbar=\"above\",\n",
    ")\n",
    "\n",
    "hbar_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some of the rows of the dataframe\n",
    "display(registry_df.head(3))\n",
    "display(registry_df.sample(5, random_state=628))\n",
    "display(registry_df.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our first look at a few sample rows some things stick out immediately.\n",
    "1. The dataset may be in chronological order and the values of the `JobStartDate`/`JobEndDate` at both of the extremes may be incorrect.\n",
    "2. There may be an abundance for `StateNumber` `42` if 4 out of the 5 draws of the 200k+ rows drawn at random had a `StateNumber` of `42`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we jump into cleaning the data in the columns, let's make the columns look more pythonic by changing the column names to snake_case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "registry_df.columns = [pascal_to_snake(col) for col in registry_df.columns]\n",
    "registry_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can remove the columns with only null values. These are the last 2 columns in the dataframe, `source` and `dtmod`. Also we can drop the `total_non_base_water_volume` column since we may not have much need for it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "registry_df = registry_df.drop(\n",
    "    columns=[\"source\", \"dtmod\", \"total_base_non_water_volume\"]\n",
    ")\n",
    "registry_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will fix some of the dtypes of the columns.\n",
    "- Both the `job_start_date` and the `job_end_date` columns are object dtypes, so we will convert those to datetime dtypes and drop the timestamp.\n",
    "- We can also separate out the date components into its various components. This may come in handy for feature engineering later on.\n",
    "- The `projection` column is an object dtype. That can be converted to a string dtype and shorten to `crs` as it represents the Cooordinate Reference System used in the `latitude` and `longitude` columns values. We can dig into what CRS is later on.\n",
    "- The `federal_well` and `indian_well` columns are both boolean type columns. They may be more aptly named as `is_federal_well` and `is_indian_well` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function on 'job_start_date' and 'job_end_date'\n",
    "split_datetime(registry_df, \"job_start_date\")\n",
    "split_datetime(registry_df, \"job_end_date\")\n",
    "registry_df[[col for col in registry_df.columns if re.search(\"start|end\", col)]].info(\n",
    "    memory_usage=\"deep\"\n",
    ")\n",
    "# show the values which are null still\n",
    "registry_df[\n",
    "    registry_df[[col for col in registry_df.columns if re.search(\"start|end\", col)]]\n",
    "    .isna()\n",
    "    .any(axis=1)\n",
    "].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Convert 'job_start_date' to datetime format and format it as 'YYYY-MM-DD'\n",
    "registry_df[\"job_start_date\"] = pd.to_datetime(\n",
    "    registry_df[\"job_start_date\"], errors=\"coerce\"\n",
    ").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Convert 'job_end_date' to datetime format and format it as 'YYYY-MM-DD'\n",
    "registry_df[\"job_end_date\"] = pd.to_datetime(\n",
    "    registry_df[\"job_end_date\"], errors=\"coerce\"\n",
    ").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# drop rows with null values in 'job_start_date' and 'job_end_date'\n",
    "# registry_df = registry_df.dropna(subset=[\"job_start_date\", \"job_end_date\"])\n",
    "\n",
    "\n",
    "# Rename some columns for clarity\n",
    "registry_df.rename(\n",
    "    columns={\n",
    "        \"federal_well\": \"is_federal_well\",\n",
    "        \"indian_well\": \"is_indian_well\",\n",
    "        \"projection\": \"crs\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Display the information of the DataFrame\n",
    "registry_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will look at the `api_number` column.\n",
    "We learned from the read me that \n",
    "> APINumber - The American Petroleum Institute well identification number formatted as follows xx-xxx-xxxxx0000 Where: \n",
    "> - First two digits represent the state, \n",
    "> - second three digits represent the county, \n",
    "> - third 5 digits represent the well.\n",
    "\n",
    "Theoretically, we could just grab the first two characters of the `APINnumber` and use that as the state number according to the definition of the `APINumber` above. Actually, that would not be a good idea, and here is why.<br>\n",
    "Although the column was called `APINumber`, it is not actually a number, so if it starts with a leading `0` that first character `0`, cannot be omitted from the value. Let's look at some of the rows with a single digit state numbers.\n",
    "\n",
    "Right now, \n",
    "- the `api_number` column is an object dtype, but a better option would be a `string` dtype, as `object` dtype can be mixed . We can also shorten that column name to `api`.\n",
    "- the `state_number` column and the `county_number` column are both `int64` dtypes right now. `string` type may be a stronger option.\n",
    "- `state_code` and `county_code` may be better names for the `state_number` and `county_number` columns respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows where the state_number is a single digit\n",
    "registry_df[\n",
    "    (registry_df[\"state_number\"] == 3) | (registry_df[\"state_number\"] == 5)\n",
    "].sample(5, random_state=628)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rows' `api_number` values have leading `0`, which is correct, but some do not. The rows without the leading `0` though are 13 characters long instead of 14. Maybe we can just add a leading `0` where needed until all API number values are 14 characters long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of characters in the api_number column\n",
    "registry_df[\"api_number\"].astype(\"string\").str.len().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most are 14 characters long, but some are 13 characters long, like the ones we saw above without the leading `0`. Let's assume the ones with 13 characters are missing the leading `0` and not something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'api' to string and pad it with zeros to make it 14 characters long\n",
    "registry_df[\"api\"] = registry_df[\"api_number\"].astype(\"string\").str.zfill(14)\n",
    "\n",
    "# Convert 'state_number' to string and pad it with zeros to make it 2 characters long\n",
    "registry_df[\"state_code\"] = registry_df[\"state_number\"].astype(\"string\").str.zfill(2)\n",
    "\n",
    "# Convert 'county_number' to string and pad it with zeros to make it 3 characters long\n",
    "registry_df[\"county_code\"] = registry_df[\"county_number\"].astype(\"string\").str.zfill(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which rows may have the api with the first two digits not matching the state number\n",
    "api_state_mismatch_mask = registry_df[\"state_code\"] != registry_df[\"api\"].str[0:2]\n",
    "# api_state_mismatch_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which rows may have the api with the first two digits not matching the state number\n",
    "registry_df[api_state_mismatch_mask][\n",
    "    [\"api_number\", \"api\", \"state_code\", \"state_name\", \"county_code\", \"county_name\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expected to get 2 rows here, since we checked the length of the `api_number` column above we saw that 1 row had 10 and another row had 12 characters. It is only two rows, so this may be an easy fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Remove leading zeros and pad to 14 digits on mismatches\n",
    "registry_df.loc[api_state_mismatch_mask, \"api\"] = (\n",
    "    registry_df.loc[api_state_mismatch_mask, \"api\"].str.lstrip(\"0\").str.ljust(14, \"0\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which rows may have the api with the first two digits not matching the state number\n",
    "registry_df[api_state_mismatch_mask][\n",
    "    [\"api_number\", \"api\", \"state_code\", \"state_name\", \"county_code\", \"county_name\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# check which rows may have the api with the 3-5 digits not matching the county number\n",
    "api_county_mismatch_mask = registry_df[\"county_code\"] != registry_df[\"api\"].str[2:5]\n",
    "registry_df[api_county_mismatch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State name should not have more than 50 possible values, given that there are only 50 states in the US. If we were to check the number of unique values in the `state_name` column, we would see 95. This is due to the variation in the way the `state_name` value is entered. Although not as obvious, we can assume the same for the `county_name` column. Luckily, the `api` includes both the `state_number` and the `county_number`. With this we can do \n",
    "1. data validation ensuring that these corresponding columns match\n",
    "2. Ensure that the `state_name` and the `county_name` columns are correct. Important to note that \n",
    "> The state codes used in an API number are DIFFERENT from another standard which is the Federal Information Processing Standard (FIPS) state code established in 1987 by NIST. ([source](https://en.wikipedia.org/wiki/API_well_number#State_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'Number of different values in state_name column: {registry_df[\"state_name\"].nunique()}'\n",
    ")\n",
    "print(\n",
    "    f'Number of different values in state_number column: {registry_df[\"state_number\"].nunique()}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by state_code and find the mode of the state_name\n",
    "state_code_mode = (\n",
    "    registry_df.groupby(\"state_code\")[\"state_name\"]\n",
    "    .apply(lambda x: x.mode().iloc[0])\n",
    "    .reset_index()\n",
    ")\n",
    "state_code_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "registry_df = registry_df.merge(state_code_mode.rename(columns={\"state_name\": \"state\"}))\n",
    "registry_df.sample(3, random_state=628)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus our efforts in the most recent 10 years. Although more data is usually better, data too far in the past may distract whatever model we may build since unconventional drilling practices have really taken over the industry. We will also put our focus in one specific area, the Permian Basin. The Permian Basin has been instrumental in the shale boom transformation and is the most active area of exploration and production in the US presently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# create mask for from 2013 onwards\n",
    "post_2012_mask = registry_df[\"job_start_date\"] >= \"2013-01-01\"\n",
    "registry_df_post_2012 = registry_df[post_2012_mask].copy()\n",
    "\n",
    "# find all the rows with null values\n",
    "null_mask = registry_df_post_2012.isna().any(axis=1)\n",
    "registry_df_post_2012[null_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how many nans we still have in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_df_post_2012.info(memory_usage=\"deep\")\n",
    "registry_df_post_2012.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the `county_number` for the rows with a nan value in the `county_name` column, we can see why there is a nan for the `county_name`. Those numbers are most likely incorrect as small states like `North Dakota` and `Arkansas` do not have large `county_number` values. However we can still try to impute what the correct values by cross referencing with other sources or by using the `latitude` and `longitude` values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the rows with a null value for the county_name column\n",
    "registry_df_post_2012[registry_df_post_2012[\"county_name\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index of one of the rows with a null value for the county_name column (3rd one down)\n",
    "index_vanorsdale = registry_df_post_2012.query(\"api_number == '03729439000000'\").index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oklahoma Commission Corporation (OOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some search engine investigating, we can learn that WFD Oil Corporation is a PC in Oklahoma. We also learn that the well name is `VANORSDOL` ,the well number is `#1-29`, and the API number is `3503729439` `0000`. We can correct some of the data which was entered incorrectly in FracFocus.\n",
    "\n",
    "The data are looking for is in this markdown cell so we can manually input it in but we will do it through code instead. We will query the api number in the data we have on the wells in OK.\n",
    "\n",
    "| Column Name | Value |\n",
    "| --- | --- |\n",
    "API\t|3503729439\n",
    "WELL_NAME|\tVANORSDOL\n",
    "WELL_NUM|\t#1-29\n",
    "OPERATOR|\tWFD OIL CORPORATION\n",
    "WELLSTATUS|\tAC\n",
    "WELLTYPE|\tOIL\n",
    "SH_LAT\t|35.749381\n",
    "SH_LON\t|-96.370355\n",
    "COUNTY\t|CREEK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When reading the Parquet file\n",
    "occ_wells = pd.read_parquet(OCC_PARQUET_URL)\n",
    "# Convert the WKT column back to a geometry column\n",
    "occ_wells[\"geometry\"] = occ_wells[\"geometry\"].apply(lambda x: wkt.loads(x))\n",
    "\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame, specifying the CRS\n",
    "occ_wells = gpd.GeoDataFrame(\n",
    "    occ_wells, geometry=\"geometry\", crs=occ_wells[\"crs\"].iloc[0]\n",
    ")\n",
    "occ_wells.info()\n",
    "# look at 1 sample row of the dataframe\n",
    "occ_wells.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'api' column to string\n",
    "occ_wells[\"api\"] = occ_wells[\"api\"].astype(\"int64\").astype(\"string\")\n",
    "occ_wells.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the well_name column\n",
    "registry_df_post_2012[\"well\"] = registry_df_post_2012[\"well_name\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the well_name column for 'vanors\n",
    "# occ_wells[occ_wells[\"well_name\"].str.contains(\"vanors\", case=False, na=False)]\n",
    "vanorsdol_row = occ_wells.query(\n",
    "    'well_name.fillna(\"\").str.contains(\"vanors\", case=False) & (api == \"3503729439\")',\n",
    ")[\n",
    "    [\n",
    "        \"api\",\n",
    "        \"well_name\",\n",
    "        \"well_num\",\n",
    "        \"operator\",\n",
    "        \"sh_lat\",\n",
    "        \"sh_lon\",\n",
    "        \"county\",\n",
    "    ]\n",
    "].rename(\n",
    "    columns={\"sh_lat\": \"latitude\", \"sh_lon\": \"longitude\"}\n",
    ")\n",
    "vanorsdol_row[\"well\"] = (\n",
    "    vanorsdol_row[\"well_name\"].str.title()\n",
    "    + \" \"\n",
    "    + vanorsdol_row[\"well_num\"].astype(\"string\")\n",
    ")\n",
    "index_vanorsdol = vanorsdol_row.index\n",
    "\n",
    "columns_to_replace = [\"api\", \"well\", \"latitude\", \"longitude\"]\n",
    "for col in columns_to_replace:\n",
    "    registry_df_post_2012.loc[index_vanorsdale, col] = vanorsdol_row.loc[\n",
    "        index_vanorsdol, col\n",
    "    ].values\n",
    "\n",
    "# check that the values have been replaced\n",
    "registry_df_post_2012.loc[index_vanorsdale, columns_to_replace]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the api column to 14 characters again\n",
    "registry_df_post_2012[\"api\"] = registry_df_post_2012[\"api\"].str.ljust(14, \"0\")\n",
    "\n",
    "registry_df_post_2012[registry_df_post_2012[\"county_name\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with `latitude` and `longitude` coordinates for all 4 rows with missing `county_name`, let's find out which counties they belong to spatially.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geodataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the boundary coordinates for all the counties in the US from the [census.gov](https://www.census.gov/) website. We saved the URL for this as `CENSUS_COUNTY_MAP_URL` at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# read in the US counties map data using geopandas\n",
    "county = gpd.read_file(CENSUS_COUNTY_MAP_URL)[\n",
    "    [\"GEOID\", \"STATEFP\", \"COUNTYFP\", \"NAME\", \"geometry\"]\n",
    "]\n",
    "county.columns = county.columns.str.lower()\n",
    "county.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will scrape the FIPS table from wikipedia since the county dataframe does not have the state name and merge the 2 tables just for convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "fips_df = pd.read_html(FIPS_WIKI_URL)[1]\n",
    "fips_df.columns = [\"geoid\", \"county\", \"state\"]\n",
    "fips_df[\"geoid\"] = fips_df[\"geoid\"].astype(\"string\").str.zfill(5)\n",
    "fips_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "county_fips_gdf = county.merge(fips_df, on=\"geoid\")\n",
    "county_fips_gdf.sample(3, random_state=628)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick note about `GeoDataFrames`: they must have a column called `geometry` and this column contains the geometric objects. This call is what enables `geopandas` to perform spatial operations, and can also contain certain attributes like `.crs` which is the coordinate reference system.\n",
    "\n",
    "\n",
    "Commonly used datums in North America are NAD27, NAD83, and WGS84. More info [here](https://webhelp.esri.com/arcgisdesktop/9.3/index.cfm?TopicName=Projection_basics_the_GIS_professional_needs_to_know).<br>\n",
    "\n",
    "The county geodataframe uses `EPSG:4269` which is the EPSG code for the NAD83 coordinate system. Let's create a geodataframe with the `latitude` and `longitude` values that we have and put all of the points to the same CRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_fips_gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# ensures each row of the geodataframe is in the same CRS\n",
    "registry_gdf = unify_crs(registry_df_post_2012, crs_col=\"crs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now perform a spatial join on the 2 GeoDataFrame.\n",
    "# fitler for rows with null county_name\n",
    "\n",
    "joined_gdf = (\n",
    "    registry_gdf[registry_gdf[\"county_name\"].isna()]\n",
    "    .sjoin(county_fips_gdf.drop(columns=[\"county\"]), how=\"left\", predicate=\"intersects\")\n",
    "    .drop(columns=[\"index_right\"])\n",
    ")\n",
    "joined_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `North Dakota` `county_number` should be `061`, which is `Mountrail` county, not `610`. \n",
    "- The `Utah` `county_number` though was actually correct. The error was the state number which should have been `42`, not `43`. This error is somewhat significant as according to the data dictionary:\n",
    "> APINumber - The American Petroleum Institute well identification number formatted as follows xx-xxx-xxxxx0000 Where: First two digits \n",
    "represent the state, second three digits represent the county, third 5 digits represent the well.<br>\n",
    "\n",
    "All this means is the `api` number is also incorrect. It should be `42317428660000` (<u><b>42</b></u>-317-42866-0000) instead of `43317428660000` (<u><b>43</b></u>-317-42866-0000).<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's corect theos values putting the county_code and\n",
    "registry_gdf[\"county\"] = registry_gdf[\"county_name\"].copy()\n",
    "\n",
    "# replace the county_code and county columns with the values from the joined_gdf\n",
    "registry_gdf.loc[joined_gdf.index, \"county\"] = joined_gdf[\"name\"]\n",
    "registry_gdf.loc[joined_gdf.index, \"county_code\"] = joined_gdf[\"countyfp\"]\n",
    "\n",
    "# change the api column of the last row in joined_gdf to 42317428660000 instead of 43317428660000\n",
    "# registry_gdf.loc[joined_gdf.index[-1], \"api\"] = \"42317428660000\"\n",
    "registry_gdf[\"api\"] = registry_gdf[\"api\"].replace(\"43317428660000\", \"42317428660000\")\n",
    "# correct the state_code values for where the api was changed\n",
    "registry_gdf[\"state_code\"] = registry_gdf[\"api\"].str[0:2]\n",
    "# Create a mapping from 'state_code' to 'state'\n",
    "state_mapping = state_code_mode.set_index(\"state_code\")[\"state\"].to_dict()\n",
    "\n",
    "# Use the mapping to update the 'state' column in 'registry_gdf'\n",
    "registry_gdf[\"state\"] = registry_gdf[\"state_code\"].map(state_mapping)\n",
    "\n",
    "\n",
    "# check that the values have been replaced\n",
    "registry_gdf[registry_gdf[\"county_name\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_gdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to impute the missing `county_name` values would have been by using the `well_name` in the `registry_df_post_2012` dataframe. Assuming the other wells on the same pad has the correct `state_code` and `state` values. However, this may not have worked with `Vanorsdol` as there's only one well with that name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_df_post_2012[\n",
    "    registry_df_post_2012[\"well_name\"].str.contains(\"vanors\", case=False, na=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show other wells with a similar name to rhea 1-6\n",
    "registry_df_post_2012[\n",
    "    registry_df_post_2012[\"well\"].str.contains(\"Rhea 1-6\", case=False, na=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_df_post_2012[registry_df_post_2012[\"well\"].str.contains(\"Trulson\", case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# county_fips_gdf[county_fips_gdf[\"state\"].isin(permian_states)]\n",
    "registry_df_post_2012[registry_df_post_2012[\"well\"].str.contains(\"palermo\", case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the well column uppercase to lower variation among for wells on the same pad\n",
    "registry_gdf[\"well\"] = registry_gdf[\"well\"].str.upper()\n",
    "# make the operator column uppercase to lower the variation among entry for the same operator\n",
    "registry_gdf[\"operator\"] = registry_gdf[\"operator_name\"].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_gdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we did for those 4 wells to find the `county` and `state` for which they belong to, we can do that for all the wells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2 new columns, lat and lon in the registry_gdf for corrections\n",
    "registry_gdf[\"lat\"] = registry_gdf[\"latitude\"].copy()\n",
    "registry_gdf[\"lon\"] = registry_gdf[\"longitude\"].copy()\n",
    "\n",
    "# Check which other wells may have a state value which did not agree with the spatial join result\n",
    "registry_county_gdf = registry_gdf.sjoin(\n",
    "    county_fips_gdf, how=\"left\", predicate=\"intersects\"\n",
    ").drop(columns=[\"index_right\"])\n",
    "\n",
    "trimmed_column_set = [\n",
    "    \"api\",\n",
    "    \"well\",\n",
    "    \"state_left\",\n",
    "    \"state_right\",\n",
    "    \"county_left\",\n",
    "    \"county_right\",\n",
    "    \"county_code\",\n",
    "    \"operator\",\n",
    "    \"lon\",\n",
    "    \"lat\",\n",
    "    \"geometry\",\n",
    "]\n",
    "\n",
    "# rows whichthe spatial join did not match for the both dataframes\n",
    "# mismatch_geo = registry_county_gdf[registry_county_gdf[\"state_right\"].isna()]\n",
    "mismatch_geo = registry_county_gdf[\n",
    "    registry_county_gdf[\"state_left\"] != registry_county_gdf[\"state_right\"]\n",
    "]\n",
    "print(f\"Number of rows: {len(mismatch_geo)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ones with a nan in the `state_right` column are those that we could not find a match for based on the `geometry`. We can match those for OK using the `api` number and see if the coordinates match for the wells in FracFocus match those from the OCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the rows with Oklahoma in the state_left column\n",
    "mismatch_geo_ok = mismatch_geo.query('state_left.str.contains(\"Oklahoma\")')\n",
    "print(f\"Number of rows from OK state: {len(mismatch_geo_ok)}\")\n",
    "mismatch_geo_ok[trimmed_column_set].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the original index in a new column\n",
    "mismatch_geo_ok[\"original_index\"] = mismatch_geo_ok.index\n",
    "# alter api to match the format in the occ_wells dataframe\n",
    "mismatch_geo_ok[\"ok_api\"] = mismatch_geo_ok[\"api\"].str[:10]  # ok for Oklahoma\n",
    "# drop the api column\n",
    "mismatch_geo_ok.drop(columns=[\"api\"], inplace=True)\n",
    "# rename the api column in occ_wells to ok_api\n",
    "occ_wells.rename(columns={\"api\": \"ok_api\"}, inplace=True)\n",
    "# look for the api in the occ_wells dataframe and merge that row to mismatch_geo_ok\n",
    "joined_mismatch_ok = mismatch_geo_ok.merge(\n",
    "    occ_wells[\n",
    "        [\"ok_api\", \"well_name\", \"well_num\", \"operator\", \"sh_lat\", \"sh_lon\", \"county\"]\n",
    "    ],\n",
    "    how=\"left\",\n",
    "    on=\"ok_api\",\n",
    ")\n",
    "joined_mismatch_ok[[\"geometry\", \"latitude\", \"longitude\", \"sh_lat\", \"sh_lon\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the lat and lon values with the sh_lat and sh_lon values\n",
    "# Replace the 'lat' and 'lon' values\n",
    "joined_mismatch_ok.loc[\n",
    "    joined_mismatch_ok[\"sh_lat\"].notna(), \"lat\"\n",
    "] = joined_mismatch_ok[\"sh_lat\"]\n",
    "joined_mismatch_ok.loc[\n",
    "    joined_mismatch_ok[\"sh_lon\"].notna(), \"lon\"\n",
    "] = joined_mismatch_ok[\"sh_lon\"]\n",
    "\n",
    "joined_mismatch_ok.set_index(\"original_index\", inplace=True)\n",
    "\n",
    "# occ_wells[occ_wells[\"api\"].isin(mismatch_geo_ok[\"ok_api\"])][\n",
    "#     [\"api\", \"well_name\", \"well_num\", \"operator\", \"sh_lat\", \"sh_lon\", \"county\"]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update 'lat' and 'lon' in the original DataFrame\n",
    "registry_gdf.loc[joined_mismatch_ok.index, \"lat\"] = joined_mismatch_ok[\"lat\"]\n",
    "registry_gdf.loc[joined_mismatch_ok.index, \"lon\"] = joined_mismatch_ok[\"lon\"]\n",
    "# registry_gdf\n",
    "\n",
    "\n",
    "# Update 'geometry' in the original DataFrame\n",
    "registry_gdf.loc[joined_mismatch_ok.index, \"geometry\"] = [\n",
    "    Point(xy) for xy in zip(joined_mismatch_ok.lon, joined_mismatch_ok.lat)\n",
    "]\n",
    "# Check which other wells may have a state value which did not agree with the spatial join result again\n",
    "registry_county_gdf = registry_gdf.sjoin(\n",
    "    county_fips_gdf, how=\"left\", predicate=\"intersects\"\n",
    ").drop(columns=[\"index_right\"])\n",
    "\n",
    "# mismatch_geo = registry_county_gdf[registry_county_gdf[\"state_right\"].isna()]\n",
    "\n",
    "mismatch_geo = registry_county_gdf[\n",
    "    registry_county_gdf[\"state_left\"] != registry_county_gdf[\"state_right\"]\n",
    "]\n",
    "print(f\"Number of rows with mismatched state values: {mismatch_geo.shape[0]}\")\n",
    "# mismatch_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query for the wells in Texas\n",
    "mismatch_geo_tx = mismatch_geo[trimmed_column_set].query(\n",
    "    'state_left.str.contains(\"Texas\")'\n",
    ")\n",
    "# store the original index in a new column\n",
    "mismatch_geo_tx[\"original_index\"] = mismatch_geo_tx.index\n",
    "\n",
    "mismatch_geo_tx[\"tx_api\"] = mismatch_geo_tx[\"api\"].str[2:10]\n",
    "# mismatch_geo_tx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texas Railroad Commission (RRC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Land survey data, bottom well data, and surface well data were taken from the RRC website. The data was then uploaded to GCP for easier reliabiliity. They are 254 zipfiles, one for each county, in the state of Texas. Each of those zipfiles contained various file extensions, and spatial data format that is usually contained in shapefiles, and contained info for various categories ranging from Airport lines to Offshore survey polys.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex patterns to identify which shapefiles to extract\n",
    "# we are grabbing the survey lines, surface wells, and bottom well locations shp files\n",
    "patterns = [r\"surv\\d{3}p\", r\"well\\d{3}s\", r\"well\\d{3}b\"]\n",
    "\n",
    "# Look at the survey lines polygons and the surface wells points. Data saved from RRC website\n",
    "shp_dict = extract_matching_shp_files_from_zip_urls_concurrent(SHP_ZIP_URLS, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def deep_getsizeof(obj):\n",
    "    \"\"\"Recursively find size of object and its elements\"\"\"\n",
    "    size = sys.getsizeof(obj)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum(deep_getsizeof(k) + deep_getsizeof(v) for k, v in obj.items())\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        size += sum(deep_getsizeof(x) for x in obj)\n",
    "    return size\n",
    "\n",
    "\n",
    "# Assume 'my_dict' is your dictionary\n",
    "total_size_in_bytes = deep_getsizeof(shp_dict)\n",
    "total_size_in_mb = total_size_in_bytes / 1e6\n",
    "print(f\"The total size of the dictionary is {total_size_in_mb:.2f} MB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_in_000(total_size_in_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the patterns to separate the gdf in the dict based on the pattern\n",
    "surv_dict = {k: shp_dict[k] for k, v in shp_dict.items() if re.search(patterns[0], k)}\n",
    "swell_dict = {k: shp_dict[k] for k, v in shp_dict.items() if re.search(patterns[1], k)}\n",
    "bwell_dict = {k: shp_dict[k] for k, v in shp_dict.items() if re.search(patterns[2], k)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texas RRC land survey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O&G well symnum is a number that indicates the type of well simplified for fewer bins\n",
    "well_symnum_dict = {\n",
    "    2: \"Permitted Location\",\n",
    "    3: \"Dry Hole\",\n",
    "    4: \"Oil/Gas\",  # oil\n",
    "    5: \"Oil/Gas\",  # gas\n",
    "    6: \"Oil/Gas\",  # oil/ gas\n",
    "    7: \"Plugged/Shut-in\",  # oil\n",
    "    8: \"Plugged/Shut-in\",  # gas\n",
    "    9: \"Canceled Location\",\n",
    "    10: \"Plugged/Shut-in\",\n",
    "    11: \"Injection/Disposal\",\n",
    "    12: \"Core Test\",\n",
    "    17: \"Storage\",  # oil\n",
    "    18: \"Storage\",  # gas\n",
    "    19: \"Plugged/Shut-in\",  # oil\n",
    "    20: \"Plugged/Shut-in\",  # gas\n",
    "    21: \"Injection/Disposal\",  # oil\n",
    "    22: \"Injection/Disposal\",  # gas\n",
    "    23: \"Injection/Disposal\",  # oil/ gas\n",
    "    73: \"Brine Mining\",\n",
    "    74: \"Water Supply\",\n",
    "    75: \"Water Supply\",  # oil\n",
    "    76: \"Water Supply\",  # gas\n",
    "    77: \"Water Supply\",  # oil/ gas\n",
    "    86: \"Horizontal\",  # Horizontal Well Surface Location\",\n",
    "    87: \"Horizontal\",  # Directional/Sidetrack Well Surface Location,\n",
    "    88: \"Storage\",\n",
    "    103: \"Storage\",  # oil/gas\n",
    "}\n",
    "# well_symnum_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the GeoDataFrames in surv_dict into a single GeoDataFrame\n",
    "surv_data_gdf = concat_gdf_from_dict(surv_dict)\n",
    "\n",
    "# Convert the column names to snake case for consistency\n",
    "surv_data_gdf.columns = [pascal_to_snake(col) for col in surv_data_gdf.columns]\n",
    "# addd a coulmn for the county_code\n",
    "surv_data_gdf[\"county_code\"] = surv_data_gdf[\"source_file\"].str.extract(r\"(\\d{3})\")\n",
    "\n",
    "# Display a sample of 3 rows from the DataFrame\n",
    "display(surv_data_gdf.sample(3))\n",
    "\n",
    "# Display information about the DataFrame, including the number of non-null entries in each column\n",
    "surv_data_gdf.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texas RRC surface well data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the GeoDataFrames in well_dict into a single GeoDataFrame\n",
    "swell_data_gdf = concat_gdf_from_dict(swell_dict)\n",
    "\n",
    "# Convert the column names to snake case for consistency\n",
    "swell_data_gdf.columns = [pascal_to_snake(col) for col in swell_data_gdf.columns]\n",
    "# get the county code from the source_file column\n",
    "swell_data_gdf[\"county_code\"] = swell_data_gdf[\"source_file\"].str.extract(r\"(\\d{3})\")\n",
    "# map the dictionary to the SYMNUM column and fill the rare values with 'Other'\n",
    "swell_data_gdf[\"well_type\"] = (\n",
    "    swell_data_gdf[\"symnum\"].map(well_symnum_dict).fillna(\"Other\")\n",
    ")\n",
    "\n",
    "# Display a sample of 3 rows from the DataFrame\n",
    "display(swell_data_gdf.sample(3))\n",
    "\n",
    "# Display information about the DataFrame, including the number of non-null entries in each column\n",
    "swell_data_gdf.info(\n",
    "    memory_usage=\"deep\"\n",
    ")  # shp_dict = extract_specific_gdf_from_zip_url(shp_zip_urls, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swell_data_gdf[\"well_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the well types on a horizontal bar chart\n",
    "swell_data_gdf[\"well_type\"].value_counts().hvplot.barh(\n",
    "    height=400,\n",
    "    width=600,\n",
    "    title=\"Surface Well Types Count\",\n",
    "    xlabel=\"\",\n",
    "    ylabel=\"\",\n",
    "    xaxis=\"bare\",\n",
    "    hover_cols=\"all\",\n",
    ").opts(\n",
    "    active_tools=[\"box_zoom\"],\n",
    "    toolbar=\"above\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot on a map the dry holes\n",
    "dry_hole_gdf = swell_data_gdf[swell_data_gdf[\"well_type\"] == \"Dry Hole\"]\n",
    "\n",
    "# vertorize the coordinates\n",
    "dry_hole_mer = platecaree_to_mercator_vectorised(\n",
    "    dry_hole_gdf[\"geometry\"].x, dry_hole_gdf[\"geometry\"].y\n",
    ")\n",
    "dry_hole_coords = pd.DataFrame(dry_hole_mer, columns=[\"x\", \"y\"])\n",
    "\n",
    "bg_map = get_background_map()\n",
    "# plot the dry holes on a map\n",
    "bg_map * gv.Points(\n",
    "    dry_hole_coords.reset_index(), [\"x\", \"y\"], [\"index\"], crs=ccrs.GOOGLE_MERCATOR\n",
    ").opts(width=800, height=600, title=\"Dry Hole Locations\", size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to plot the well types on a map\n",
    "def plot_well_types(gdf, well_type):\n",
    "    \"\"\"Plots the well types on a map\"\"\"\n",
    "    # filter the swell_data_gdf for the well_type\n",
    "    well_type_gdf = gdf[gdf[\"well_type\"] == well_type]\n",
    "    # vertorize the coordinates\n",
    "    well_type_mer = platecaree_to_mercator_vectorised(\n",
    "        well_type_gdf[\"geometry\"].x, well_type_gdf[\"geometry\"].y\n",
    "    )\n",
    "    well_type_coords = pd.DataFrame(well_type_mer, columns=[\"x\", \"y\"])\n",
    "    # plot the well types on a map\n",
    "    return get_background_map() * gv.Points(\n",
    "        well_type_coords.reset_index(),\n",
    "        [\"x\", \"y\"],\n",
    "        [\"index\"],\n",
    "        crs=ccrs.GOOGLE_MERCATOR,\n",
    "    ).opts(width=800, height=600, title=f\"{well_type} Locations\", size=1).opts(\n",
    "        active_tools=[\"box_zoom\"],\n",
    "        toolbar=\"above\",\n",
    "        xaxis=\"bare\",\n",
    "        yaxis=\"bare\",\n",
    "        tools=[\"hover\"],\n",
    "    )\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# create a partial function for the plot_well_types function\n",
    "plot_well_types_partial = partial(plot_well_types, swell_data_gdf)\n",
    "\n",
    "plot_well_types_partial(\"Plugged/Shut-in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texas RRC bottom hole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the GeoDataFrames in well_dict into a single GeoDataFrame\n",
    "bwell_data_gdf = concat_gdf_from_dict(bwell_dict)\n",
    "\n",
    "# Convert the column names to snake case for consistency\n",
    "bwell_data_gdf.columns = [pascal_to_snake(col) for col in bwell_data_gdf.columns]\n",
    "# get the county code from the source_file column\n",
    "bwell_data_gdf[\"county_code\"] = bwell_data_gdf[\"source_file\"].str.extract(r\"(\\d{3})\")\n",
    "\n",
    "# map the dictionary to the SYMNUM column and fill the rare values with 'Other'\n",
    "bwell_data_gdf[\"well_type\"] = (\n",
    "    bwell_data_gdf[\"symnum\"].map(well_symnum_dict).fillna(\"Other\")\n",
    ")\n",
    "# Display a sample of 3 rows from the DataFrame\n",
    "display(bwell_data_gdf.sample(3))\n",
    "\n",
    "# Display information about the DataFrame, including the number of non-null entries in each column\n",
    "bwell_data_gdf.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### matching the mismatched wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swell_columns = [\"api\", \"lat83\", \"long83\", \"well_type\"]\n",
    "mismatch_columns = [\"tx_api\", \"lon\", \"lat\", \"original_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch_geo_tx[mismatch_columns].merge(\n",
    "    swell_data_gdf[swell_columns], how=\"left\", left_on=\"tx_api\", right_on=\"api\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_mismatch_tx = mismatch_geo_tx[mismatch_columns].merge(\n",
    "    swell_data_gdf[swell_columns], how=\"left\", left_on=\"tx_api\", right_on=\"api\"\n",
    ")\n",
    "\n",
    "# set the index to the original_index column\n",
    "joined_mismatch_tx.set_index(\"original_index\", inplace=True)\n",
    "\n",
    "# replace the lat and lon values with the lat83 and lon83 values if not nan\n",
    "joined_mismatch_tx.loc[joined_mismatch_tx[\"lat83\"].notna(), \"lat\"] = joined_mismatch_tx[\n",
    "    \"lat83\"\n",
    "]\n",
    "joined_mismatch_tx.loc[\n",
    "    joined_mismatch_tx[\"long83\"].notna(), \"lon\"\n",
    "] = joined_mismatch_tx[\"long83\"]\n",
    "\n",
    "# Update 'lat' and 'lon' in the original DataFrame\n",
    "\n",
    "\n",
    "registry_gdf.loc[joined_mismatch_tx.index, \"lat\"] = joined_mismatch_tx[\"lat\"]\n",
    "registry_gdf.loc[joined_mismatch_tx.index, \"lon\"] = joined_mismatch_tx[\"lon\"]\n",
    "\n",
    "\n",
    "# Update 'geometry' in the original DataFrame\n",
    "registry_gdf.loc[joined_mismatch_tx.index, \"geometry\"] = [\n",
    "    Point(xy) for xy in zip(joined_mismatch_tx.lon, joined_mismatch_tx.lat)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which other wells may have a state value which did not agree with the spatial join result again\n",
    "registry_county_gdf = registry_gdf.sjoin(\n",
    "    county_fips_gdf, how=\"left\", predicate=\"intersects\"\n",
    ").drop(columns=[\"index_right\"])\n",
    "mismatch_geo = registry_county_gdf[registry_county_gdf[\"state_right\"].isna()]\n",
    "mismatch_geo.shape\n",
    "print(f\"Number of rows with mismatched state values: {mismatch_geo.shape[0]}\")\n",
    "display(mismatch_geo.sample(3))\n",
    "\n",
    "# drop the rows with null values in the state_right column of mismatch_geo from the registry_gdf\n",
    "print(f\"Number of rows before dropping: {registry_gdf.shape[0]}\")\n",
    "registry_gdf.drop(mismatch_geo.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_gdf.sjoin(county_fips_gdf, how=\"left\", predicate=\"intersects\").drop(\n",
    "    columns=[\"index_right\"]\n",
    ")[trimmed_column_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the columns to keep\n",
    "columns_we_want = [\n",
    "    \"api\",\n",
    "    \"well\",\n",
    "    \"state_code\",\n",
    "    \"state_left\",\n",
    "    \"state_right\",\n",
    "    \"county_code\",\n",
    "    \"county_left\",\n",
    "    \"countyfp\",\n",
    "    \"name\",\n",
    "    \"operator\",\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"geoid\",\n",
    "    \"geometry\",\n",
    "]\n",
    "registry_county_gdf = registry_gdf.sjoin(\n",
    "    county_fips_gdf, how=\"left\", predicate=\"intersects\"\n",
    ").drop(columns=[\"index_right\"])[columns_we_want]\n",
    "\n",
    "print(f\"Number of rows: {registry_county_gdf.shape[0]}\")\n",
    "registry_county_gdf[\n",
    "    registry_county_gdf[\"state_left\"] != registry_county_gdf[\"state_right\"]\n",
    "][columns_we_want].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the rows where the state_left is California with the state_right being Texas\n",
    "# This would represent where the geometry said Texas but the api or original dataset had California\n",
    "# we check the api with\n",
    "cali_tx_query = registry_county_gdf.query(\n",
    "    'state_left == \"California\" & state_right == \"Texas\"'\n",
    ")\n",
    "# from the query, take the last 9 of the api (well_id) with 42 at beginning\n",
    "# with county_code from county that was matched with the geometry\n",
    "cali_tx_query[\"state_code\"] = \"42\"\n",
    "cali_tx_query[\"county_code\"] = cali_tx_query[\"countyfp\"]\n",
    "cali_tx_query[\"well_id\"] = cali_tx_query[\"api\"].str[-9:]\n",
    "cali_tx_query[\"api\"] = (\n",
    "    cali_tx_query[\"state_code\"]\n",
    "    + cali_tx_query[\"county_code\"]\n",
    "    + cali_tx_query[\"well_id\"]\n",
    ")\n",
    "\n",
    "# put api, county_code and state_code into the registry_gdf\n",
    "registry_gdf.loc[\n",
    "    cali_tx_query.index, [\"api\", \"county_code\", \"state_code\"]\n",
    "] = cali_tx_query[[\"api\", \"county_code\", \"state_code\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_county_gdf[\n",
    "    registry_county_gdf[columns_we_want][\"state_left\"]\n",
    "    != registry_county_gdf[columns_we_want][\"state_right\"]\n",
    "][columns_we_want]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swell_data_gdf[\"well_type\"].value_counts()\n",
    "# well_data_gdf[well_data_gdf[\"well_type\"] == \"Other\"][\"symnum\"].value_counts()\n",
    "# look for the mismatch_geo_tx api in the swell_data_gdf and get those rrows\n",
    "\n",
    "mismatch_geo_tx_sw = swell_data_gdf[\n",
    "    swell_data_gdf[\"api\"].isin(mismatch_geo_tx[\"tx_api\"])\n",
    "].copy()\n",
    "mismatch_geo_tx_sw\n",
    "# convert the long83 and lat83 columns to geomety Points\n",
    "mismatch_geo_tx_sw[\"geometry\"] = [\n",
    "    Point(xy) for xy in zip(mismatch_geo_tx_sw.long83, mismatch_geo_tx_sw.lat83)\n",
    "]\n",
    "# create a GeoDataFrame from the mismatch_geo_tx_sw dataframe\n",
    "mismatch_geo_tx_sw = gpd.GeoDataFrame(\n",
    "    mismatch_geo_tx_sw, geometry=\"geometry\", crs=\"EPSG:4269\"\n",
    ")\n",
    "# plot the mismatch_geo_tx_sw GeoDataFrame\n",
    "# bg_map * gv.Points(mismatch_geo_tx_sw).opts(height=500, width=800, tools=[\"hover\"])\n",
    "\n",
    "mismatch_geo_tx_sw.sjoin(county_fips_gdf, how=\"left\", predicate=\"intersects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the length of the tx_api values\n",
    "swell_data_gdf[\"api\"].astype(\"string\").str.len().value_counts()\n",
    "\n",
    "# drop the rows with the api number length of 3\n",
    "swell_data_gdf.drop(\n",
    "    swell_data_gdf[swell_data_gdf[\"api\"].astype(\"string\").str.len() == 3].index,\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_zip = SHP_ZIP_URLS[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the zip file at test_zip\n",
    "with urlopen(test_zip) as u:\n",
    "    zip_data = u.read()\n",
    "# extract the zip file\n",
    "with ZipMemoryFile(zip_data) as z:\n",
    "    zip_files = z.listdir()\n",
    "    print(zip_files)\n",
    "    # get the .shp files\n",
    "    shp_files = [f for f in zip_files if f.endswith(\".shp\")]\n",
    "    # read in the .shp files\n",
    "    for shp_file in shp_files:\n",
    "        with z.open(shp_file) as f:\n",
    "            gdf = gpd.GeoDataFrame.from_features(f, crs=f.crs)\n",
    "            # add column for the source file\n",
    "            gdf[\"source_file\"] = shp_file\n",
    "            print(f\"Shape of {shp_file}: {gdf.shape}\")\n",
    "            display(gdf.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_map = get_background_map()\n",
    "\n",
    "ok_counties = county_fips_gdf.query('state.str.contains(\"Oklahoma\")').copy()\n",
    "ok_counties.to_crs(\"EPSG:3857\", inplace=True)\n",
    "\n",
    "tx_counties = county_fips_gdf.query('state.str.contains(\"Texas\")').copy()\n",
    "tx_counties.to_crs(\"EPSG:3857\", inplace=True)\n",
    "\n",
    "ok_polys = gv.Polygons(ok_counties, crs=ccrs.GOOGLE_MERCATOR).opts(\n",
    "    projection=ccrs.GOOGLE_MERCATOR,\n",
    "    line_color=\"black\",\n",
    "    fill_alpha=0,\n",
    "    height=500,\n",
    "    width=500,\n",
    ")\n",
    "bg_map * ok_polys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of states that are in the Permian Basin.\n",
    "nm_tx = [\"New Mexico\", \"Texas\"]\n",
    "\n",
    "# Filter the county_fips_gdf DataFrame to include only the counties in the Permian states.\n",
    "counties_nm_tx_gdf = county_fips_gdf[county_fips_gdf[\"state\"].isin(nm_tx)]\n",
    "\n",
    "# Perform a spatial join between the registry_gdf and counties_nm_tx_gdf DataFrames.\n",
    "# This will add the data from counties_nm_tx_gdf to registry_gdf for matching locations.\n",
    "# After the join, drop the 'index_right' column as it's not needed.\n",
    "registry_nm_tx_gdf = registry_gdf.sjoin(counties_nm_tx_gdf).drop(\n",
    "    columns=[\"index_right\"]\n",
    ")\n",
    "registry_nm_tx_gdf.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_nm_tx_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_nm_tx_gdf[\"operator_name\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pivot table with the count of operators for each year\n",
    "operator_year_count = registry_nm_tx_gdf.pivot_table(\n",
    "    index=\"operator_name\", columns=\"job_start_year\", values=\"api\", aggfunc=\"count\"\n",
    ")\n",
    "# See which operators were active every year\n",
    "# operator_year_count[operator_year_count.count(axis=1) == 11]\n",
    "\n",
    "# See who has been active for the last 5 years\n",
    "operator_active_5y = operator_year_count.loc[\n",
    "    ~operator_year_count.iloc[:, -5:].isna().any(axis=1)\n",
    "].fillna(0)\n",
    "\n",
    "# do some styler table formatting from pandas\n",
    "style = operator_active_5y.style.background_gradient(\n",
    "    cmap=\"cet_CET_L2_r\", axis=1, vmin=0, vmax=operator_year_count.max().max()\n",
    ")\n",
    "# Format the numbers in the table as integers\n",
    "style = style.format(\"{:.0f}\")\n",
    "print(f\"Number of operators active for the last 5 years: {len(operator_active_5y)}\")\n",
    "style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask for rows where the first 2 characters of the api number are not 42 nor 30.\n",
    "# This is done to filter out rows that do not belong to the states we are interested in (Texas and New Mexico).\n",
    "api_mask = ~registry_nm_tx_gdf[\"api\"].str[0:2].isin([\"42\", \"30\"])\n",
    "\n",
    "# Apply the mask to the registry_nm_tx_gdf DataFrame to get the rows that match the condition.\n",
    "mismatch_state = registry_nm_tx_gdf[api_mask]\n",
    "\n",
    "# Display selected columns from the mismatch_state DataFrame.\n",
    "# These columns provide information about the well, its location, and the job start date.\n",
    "display(\n",
    "    mismatch_state[\n",
    "        [\n",
    "            \"api\",\n",
    "            \"api_number\",\n",
    "            \"state_right\",\n",
    "            \"state_name\",\n",
    "            \"state_number\",\n",
    "            \"well_name\",\n",
    "            \"operator_name\",\n",
    "            \"county_name\",\n",
    "            \"latitude\",\n",
    "            \"longitude\",\n",
    "            \"geometry\",\n",
    "            \"crs\",\n",
    "            \"job_start_date\",\n",
    "            \"county\",\n",
    "            \"countyfp\",\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# get a background map\n",
    "bg_map = get_background_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the coordinates to Mercator\n",
    "mercator_coords = platecaree_to_mercator_vectorised(\n",
    "    registry_nm_tx_gdf[\"geometry\"].x, registry_nm_tx_gdf[\"geometry\"].y\n",
    ")\n",
    "\n",
    "# Round the coordinates and create a DataFrame\n",
    "mer_points = pd.DataFrame(np.round(mercator_coords), columns=[\"x\", \"y\"])\n",
    "\n",
    "# Create a Points object for plotting\n",
    "gpoints = gv.Points(\n",
    "    mer_points.reset_index(), [\"x\", \"y\"], [\"index\"], crs=ccrs.GOOGLE_MERCATOR\n",
    ").opts(height=600, width=800, color=\"skyblue\", size=1, tools=[\"hover\"])\n",
    "\n",
    "# Create a layout with the background map and the points\n",
    "layout = bg_map * gpoints\n",
    "layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map files taken from the EIA website.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function 'extract_gdfs_from_zip_url_concurrent' to get GeoDataFrames from the URLs in 'basins_url_list'\n",
    "# This function concurrently downloads and extracts GeoDataFrames from the given URLs\n",
    "basins_dict = extract_gdfs_from_zip_url_concurrent(basins_url_list)\n",
    "\n",
    "# Display the keys of 'basins_dict' to see the names of the basins\n",
    "display(basins_dict.keys())\n",
    "\n",
    "# Concatenate the GeoDataFrames in 'basins_dict' into a single GeoDataFrame using the function 'concat_gdf_from_dict'\n",
    "basins_gdf = concat_gdf_from_dict(basins_dict)\n",
    "\n",
    "# Convert the column names of 'basins_gdf' to snake case for consistency\n",
    "# The function 'pascal_to_snake' is used to convert PascalCase or camelCase to snake_case\n",
    "basins_gdf.columns = [pascal_to_snake(col) for col in basins_gdf.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of sub basins/ geodataframes: {len(basins_dict)}\\n\")\n",
    "\n",
    "for k, gdf in basins_dict.items():\n",
    "    print(f\"{k}| Shape:{gdf.shape}| CRS:{gdf.crs.to_string()}\")\n",
    "    display(gdf.sample())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the shapefile of the basin boundaries\n",
    "basins_dict = extract_gdfs_from_zip_url_concurrent(basins_url_list)\n",
    "display(basins_dict.keys())\n",
    "basins_gdf = concat_gdf_from_dict(basins_dict)\n",
    "# scrub the column names\n",
    "basins_gdf.columns = [pascal_to_snake(col) for col in basins_gdf.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot shale plays and basin boundaries of the different formations in the Permian Basin\n",
    "bg_map * basins_gdf.hvplot(\n",
    "    geo=True,\n",
    "    alpha=0.5,\n",
    "    title=\"Shale Plays in the Permian Basin\",\n",
    "    legend=True,\n",
    "    by=\"shale_play\",\n",
    "    muted_alpha=0.01,\n",
    ").opts(\n",
    "    tools=[\"hover\", \"tap\"],\n",
    "    legend_position=\"right\",\n",
    "    height=600,\n",
    "    width=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holoviews import opts\n",
    "\n",
    "# Dissolve the geometries of the basins_gdf GeoDataFrame into a single geometry\n",
    "\n",
    "\n",
    "shale_plays_gdf = basins_gdf[[\"geometry\"]].dissolve()\n",
    "# get the counties in the Permian Basin\n",
    "permian_counties = county_fips_gdf.intersects(shale_plays_gdf)\n",
    "# plot the counties in the Permian Basin\n",
    "bg_map * gv.Polygons(county_fips_gdf[permian_counties]).opts(\n",
    "    title=\"Counties in the Permian Basin\",\n",
    "    tools=[\"hover\"],\n",
    "    height=600,\n",
    "    width=800,\n",
    "    alpha=0.5,\n",
    "    color=\"skyblue\",\n",
    ")\n",
    "# plot an outline of the Permian Basin over the counties\n",
    "overlay = (\n",
    "    bg_map\n",
    "    * gv.Polygons(county_fips_gdf[permian_counties])\n",
    "    * gv.Path(shale_plays_gdf)\n",
    "    # * gpoints\n",
    ")\n",
    "\n",
    "overlay.opts(\n",
    "    opts.Polygons(alpha=0.5, cmap=[\"#73d2ff\"], line_color=\"gray\"),\n",
    "    opts.Path(alpha=0.5, color=\"black\"),\n",
    "    opts.Overlay(tools=[\"hover\"], height=600, width=800),\n",
    "    opts.Points(color=\"crimson\"),\n",
    ")\n",
    "\n",
    "\n",
    "# plot to confirm that the geometries have been dissolved\n",
    "# bg_map * gv.Polygons(shale_plays_gdf).opts(\n",
    "#     # geo=True,\n",
    "#     title=\"Dissolved Shale play in the Permian Basin\",\n",
    "#     height=600,\n",
    "#     width=800,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State land leases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Mexico:\n",
    "> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the extract_gdfs_from_zip_url_concurrent function to download and extract GeoDataFrames\n",
    "# from the shapefile zip files at the URLs in the shp_url_list. The function returns a dictionary\n",
    "# where the keys are the names of the shapefiles and the values are the corresponding GeoDataFrames.\n",
    "nm_slo_dict = extract_gdfs_from_zip_url_concurrent(nm_slo_url_list)\n",
    "\n",
    "# Display the keys of the land_map_dict dictionary. These are the names of the shapefiles\n",
    "# that were downloaded and extracted.\n",
    "nm_slo_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# land_map_dict = extract_gdfs_from_zip_url_concurrent(shp_url_list)\n",
    "\n",
    "\n",
    "# land_map_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the gdfs in the dictionary\n",
    "for k, gdf in nm_slo_dict.items():\n",
    "    print(f\"{k}| Shape:{gdf.shape}| CRS:{gdf.crs.to_string()}\")\n",
    "    display(gdf.sample(3))\n",
    "    display(gdf.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 separate gdfs instead of concatenating them as they have distinct columns\n",
    "nm_slo_gdfs = list(nm_slo_dict.values())\n",
    "# first one is the geologic regions\n",
    "nm_slo_geo = nm_slo_gdfs[0]\n",
    "# scrub the columns\n",
    "nm_slo_geo.columns = [pascal_to_snake(col) for col in nm_slo_geo.columns]\n",
    "\n",
    "# Define  a dictionary for the opts to include in plot function\n",
    "poly_opts = dict(\n",
    "    alpha=0.8,\n",
    "    height=600,\n",
    "    width=800,\n",
    "    line_width=0,\n",
    "    line_color=\"lightgray\",\n",
    "    tools=[\"hover\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Adjust opts for this plot\n",
    "poly_opts_copy = poly_opts.copy()\n",
    "poly_opts_copy[\"line_width\"] = 1\n",
    "\n",
    "# plot the geologic regions gdf\n",
    "bg_map * gv.Polygons(nm_slo_geo.to_crs(\"EPSG:4269\"), vdims=[\"label\"]).opts(\n",
    "    **poly_opts_copy, cmap=[\"#73d2ff\"] * 256, title=\"New Mexico Geologic Regions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second one is the oil and gas leases on New Mexico State Trust Lands\n",
    "nm_slo_lease = nm_slo_gdfs[1]\n",
    "# scrub the columns\n",
    "nm_slo_lease.columns = [pascal_to_snake(col) for col in nm_slo_lease.columns]\n",
    "# create a new column for the area of the lease\n",
    "nm_slo_lease[\"area\"] = nm_slo_lease[\"geometry\"].area\n",
    "# groupby the ogrid_nam and sum the area\n",
    "# add the transformed area to the gdf\n",
    "nm_slo_lease[\"ogrid_area\"] = (\n",
    "    nm_slo_lease.groupby(\"ogrid_nam\")[\"area\"].transform(\"sum\") / 1e6\n",
    ")\n",
    "\n",
    "# plot the oil and gas leases gdf for New Mexico State Trust Lands\n",
    "bg_map * gv.Polygons(\n",
    "    nm_slo_lease.to_crs(\"EPSG:4269\"), vdims=[\"ogrid_nam\", \"ogrid_area\"]\n",
    ").opts(\n",
    "    **poly_opts,\n",
    "    cnorm=\"eq_hist\",\n",
    "    colorbar=True,\n",
    "    title=\"Oil and Gas Leases on New Mexico State Trust Lands\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(land_map_dict.values())\n",
    "# [gdf['geometry'] for gdf in land_map_dict.values()]\n",
    "random_color_list = [\n",
    "    \"#\" + \"\".join([random.choice(\"0123456789ABCDEF\") for j in range(6)])\n",
    "    for i in range(len(nm_slo_dict))\n",
    "]\n",
    "plots = []\n",
    "new_map = get_background_map()\n",
    "plots.append(new_map)\n",
    "for color, (name, gdf) in zip(random_color_list, nm_slo_dict.items()):\n",
    "    # Add new column with the name of the shapefile for the hover tool\n",
    "    gdf[\"label\"] = name\n",
    "    plot = gv.Polygons(gdf.to_crs(\"EPSG:4269\"), vdims=[\"label\"]).opts(\n",
    "        tools=[\"hover\"], height=600, width=800, alpha=0.5, title=\"\"\n",
    "    )\n",
    "    plots.append(plot)\n",
    "\n",
    "overlay = hv.Overlay(plots)\n",
    "overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm_slo_lease.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the GeoDataFrames in well_dict into a single GeoDataFrame\n",
    "swell_data_gdf = concat_gdf_from_dict(swell_dict)\n",
    "\n",
    "# Convert the column names to snake case for consistency\n",
    "swell_data_gdf.columns = [pascal_to_snake(col) for col in swell_data_gdf.columns]\n",
    "# get the county code from the source_file column\n",
    "swell_data_gdf[\"county_code\"] = swell_data_gdf[\"source_file\"].str.extract(r\"(\\d{3})\")\n",
    "\n",
    "# Display a sample of 3 rows from the DataFrame\n",
    "display(swell_data_gdf.sample(3))\n",
    "\n",
    "# Display information about the DataFrame, including the number of non-null entries in each column\n",
    "swell_data_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column to the surv_data_gdf with the county number\n",
    "# the county_number wil be the numbers in the source_file column\n",
    "surv_data_gdf[\"county_number\"] = surv_data_gdf[\"source_file\"].str.extract(r\"(\\d{3})\")\n",
    "\n",
    "# using just the geometry and the county_number columns, intersect with the permian basin gdf\n",
    "surv_permian_gdf = surv_data_gdf[[\"geometry\", \"county_number\"]].sjoin(\n",
    "    shale_plays_gdf[[\"geometry\"]], how=\"inner\", predicate=\"intersects\"\n",
    ")\n",
    "# see which counties are in the permian basin\n",
    "pb_county_numbers = surv_permian_gdf[\"county_number\"].unique().tolist()\n",
    "\n",
    "# plot the survey lines in the permian basin\n",
    "bg_map * gv.Polygons(\n",
    "    surv_permian_gdf.to_crs(\"EPSG:4269\"), vdims=[\"county_number\"]\n",
    ").opts(\n",
    "    tools=[\"hover\"],\n",
    "    height=600,\n",
    "    width=800,\n",
    "    alpha=0.5,\n",
    "    line_width=0,\n",
    "    title=\"Permian Basin Survey Lines\",\n",
    ")\n",
    "\n",
    "# surv_data_gdf.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how land survey polygon data looks on map\n",
    "pb_plot = shale_plays_gdf.hvplot(geo=True, color=\"red\", alpha=0.5, line_width=0).opts(\n",
    "    height=600, width=800\n",
    ")\n",
    "survey_plot = surv_data_gdf.hvplot(geo=True, color=\"blue\", alpha=0.5, line_width=0)\n",
    "\n",
    "# bg_map * survey_plot * pb_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the intersection of the survey polygons and the Permian Basin polygon\n",
    "survey_pb_gdf = gpd.overlay(surv_data_gdf, shale_plays_gdf, how=\"intersection\")\n",
    "survey_pb_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# survey_pb_gdf.explore()\n",
    "# surv_data_gdf.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join of the registry_gdf(from fracfocus) and surv_data_gdf\n",
    "registry_join_gdf = gpd.sjoin(\n",
    "    registry_gdf[\n",
    "        [\n",
    "            \"geometry\",\n",
    "            \"api\",\n",
    "            \"operator_name\",\n",
    "            \"well_name\",\n",
    "            \"state\",\n",
    "            \"county_name\",\n",
    "            \"county_number\",\n",
    "        ]\n",
    "    ],\n",
    "    surv_data_gdf,\n",
    ").drop(columns=[\"index_right\"])\n",
    "\n",
    "\n",
    "registry_join_gdf.sort_values(by=\"api\")\n",
    "\n",
    "registry_join_gdf.county_name.value_counts()\n",
    "\n",
    "# create a well_id column from the api column\n",
    "\n",
    "registry_join_gdf[\"tx_api\"] = registry_join_gdf[\"api\"].str[2:10]\n",
    "\n",
    "\n",
    "# merge the welltype column from well_data_gdf to registry_join_gdf on the api_short column\n",
    "\n",
    "registry_join_gdf = (\n",
    "    registry_join_gdf.merge(swell_data_gdf[[\"tx_api\", \"well_type\"]], on=\"tx_api\")\n",
    "    .drop(columns=[\"scrap_file\", \"level4_sur\"])\n",
    "    .rename(columns={\"level2_blo\": \"block\"})\n",
    ")\n",
    "# registry_join_gdf.explore()\n",
    "# plot polygons using geoviews\n",
    "bg_map * gv.Polygons(registry_join_gdf.to_crs(\"EPSG:4269\"), vdims=[\"well_type\"]).opts(\n",
    "    **poly_opts, color=\"well_type\", title=\"Well Types in the Permian Basin\"\n",
    ")\n",
    "\n",
    "bg_map * registry_join_gdf.hvplot(\n",
    "    geo=True,\n",
    "    by=\"well_type\",\n",
    "    alpha=0.8,\n",
    "    legend=\"right\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    size=1,\n",
    "    muted_alpha=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geodatabase files taken from the Texas GLO (General Land Office.)\n",
    "\n",
    "These files contained both the Oil and Gas Leases (active only), managed by the Texas GLO, and Oil & Gas units (active only) which is Oil and Gas pooling agreements managed by the Texas GLO. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the geodataframe of the active leases\n",
    "# active_gdb_dict = read_gdb_from_zip_url(gdb_zip_urls)\n",
    "\n",
    "\n",
    "# get the geodataframe of the active leases using concurrent futures\n",
    "active_gdb_dict = read_gdb_from_zip_url_concurrent(GDB_ZIP_URLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_gdb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_gdb_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the active lease geodatabase\n",
    "active_leases_gdf = active_gdb_dict[\"OAG_Leases_Active\"]\n",
    "# clean column names\n",
    "active_leases_gdf.columns = [pascal_to_snake(col) for col in active_leases_gdf.columns]\n",
    "active_leases_gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the columns with the date in it using regex\n",
    "date_cols = [col for col in active_leases_gdf.columns if re.search(r\"date\", col)]\n",
    "# add any other columns that should be dates\n",
    "date_cols.extend([\"lease_input\"])\n",
    "\n",
    "date_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the date columns to datetime\n",
    "active_leases_gdf[date_cols] = pd.concat(\n",
    "    [pd.to_datetime(active_leases_gdf[col]) for col in date_cols], axis=1\n",
    ")\n",
    "# active_leases_gdf[date_cols] = active_leases_gdf[date_cols].fillna(\n",
    "#     pd.Timestamp(\"1900-06-28\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the columns interested in seeing\n",
    "columns_of_interest = date_cols + [\n",
    "    \"county\",\n",
    "    \"geometry\",\n",
    "    \"land_type\",\n",
    "    \"primary_term_year\",\n",
    "    \"original_lessee\",\n",
    "    \"lessor\",\n",
    "    \"field_name\",\n",
    "    \"lease_type\",\n",
    "    \"lease_status\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_leases_gdf[columns_of_interest].info()\n",
    "active_leases_gdf[active_leases_gdf[columns_of_interest].isna().any(axis=1)][\n",
    "    columns_of_interest\n",
    "].sort_values(by=\"effective_date\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_date_cols = list(set(columns_of_interest) - set(date_cols))\n",
    "pd.concat(\n",
    "    [\n",
    "        active_leases_gdf[non_date_cols],\n",
    "        active_leases_gdf[date_cols].astype(\n",
    "            str\n",
    "        ),  # the .explore() does not work with NaT in datetime columns\n",
    "    ],\n",
    "    axis=1,\n",
    ").explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Production Data Query Dump from RRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:8788/status\n"
     ]
    }
   ],
   "source": [
    "client = Client(\n",
    "    n_workers=4, threads_per_worker=1, memory_limit=\"2GB\", dashboard_address=\":8788\"\n",
    ")\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general purpose table paths\n",
    "county_path = Path(\"../data/PDQ_DSV/GP_COUNTY_DATA_TABLE.dsv\")\n",
    "district_path = Path(\"../data/PDQ_DSV/GP_DISTRICT_DATA_TABLE.dsv\")\n",
    "\n",
    "# production report data paths\n",
    "county_cycle_path = Path(\"../data/PDQ_DSV/OG_COUNTY_CYCLE_DATA_TABLE.dsv\")\n",
    "county_lease_path = Path(\"../data/PDQ_DSV/OG_COUNTY_LEASE_CYCLE_DATA_TABLE.dsv\")\n",
    "lease_path = Path(\"../data/PDQ_DSV/OG_LEASE_CYCLE_DATA_TABLE.dsv\")\n",
    "well_comp_path = Path(\"../data/PDQ_DSV/OG_WELL_COMPLETION_DATA_TABLE.dsv\")\n",
    "field_cycle_path = Path(\"../data/PDQ_DSV/OG_FIELD_CYCLE_DATA_TABLE.dsv\")\n",
    "operator_cycle_path = Path(\"../data/PDQ_DSV/OG_OPERATOR_CYCLE_DATA_TABLE.dsv\")\n",
    "operator_info = Path(\"../data/PDQ_DSV/OG_OPERATOR_DW_DATA_TABLE.dsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create func which takes a path and prints the columns and displays the head\n",
    "def show_sample(path):\n",
    "    \"\"\"Shows the columns and a sample of the data in the file at 'path'\"\"\"\n",
    "    print(f\"File: {path.name}\\n\")\n",
    "    # Use Dask to compute the number of rows\n",
    "    ddf = dd.read_csv(path, sep=\"}\")\n",
    "    print(f\"Number of rows: {len(ddf)}\\n\")\n",
    "\n",
    "    print(f\"Columns in {path.name}:\\n\")\n",
    "    columns = pd.read_csv(path, sep=\"}\", nrows=1).columns\n",
    "    print(f\"Number of columns: {len(columns)}\")\n",
    "    print(f\"Columns: {columns}\")\n",
    "\n",
    "    print(f\"Sample of data in {path.name}:\")\n",
    "    display(pd.read_csv(path, sep=\"}\", nrows=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: GP_COUNTY_DATA_TABLE.dsv\n",
      "\n",
      "Number of rows: 277\n",
      "\n",
      "Columns in GP_COUNTY_DATA_TABLE.dsv:\n",
      "\n",
      "Number of columns: 7\n",
      "Columns: Index(['COUNTY_NO', 'COUNTY_FIPS_CODE', 'COUNTY_NAME', 'DISTRICT_NO',\n",
      "       'DISTRICT_NAME', 'ON_SHORE_FLAG', 'ONSHORE_ASSC_CNTY_FLAG'],\n",
      "      dtype='object')\n",
      "Sample of data in GP_COUNTY_DATA_TABLE.dsv:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNTY_NO</th>\n",
       "      <th>COUNTY_FIPS_CODE</th>\n",
       "      <th>COUNTY_NAME</th>\n",
       "      <th>DISTRICT_NO</th>\n",
       "      <th>DISTRICT_NAME</th>\n",
       "      <th>ON_SHORE_FLAG</th>\n",
       "      <th>ONSHORE_ASSC_CNTY_FLAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>363</td>\n",
       "      <td>363</td>\n",
       "      <td>PALO PINTO</td>\n",
       "      <td>8</td>\n",
       "      <td>7B</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>367</td>\n",
       "      <td>367</td>\n",
       "      <td>PARKER</td>\n",
       "      <td>8</td>\n",
       "      <td>7B</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>411</td>\n",
       "      <td>411</td>\n",
       "      <td>SAN SABA</td>\n",
       "      <td>8</td>\n",
       "      <td>7B</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>417</td>\n",
       "      <td>417</td>\n",
       "      <td>SHACKELFORD</td>\n",
       "      <td>8</td>\n",
       "      <td>7B</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>425</td>\n",
       "      <td>425</td>\n",
       "      <td>SOMERVELL</td>\n",
       "      <td>8</td>\n",
       "      <td>7B</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   COUNTY_NO  COUNTY_FIPS_CODE  COUNTY_NAME  DISTRICT_NO DISTRICT_NAME  \\\n",
       "0        363               363   PALO PINTO            8            7B   \n",
       "1        367               367       PARKER            8            7B   \n",
       "2        411               411     SAN SABA            8            7B   \n",
       "3        417               417  SHACKELFORD            8            7B   \n",
       "4        425               425    SOMERVELL            8            7B   \n",
       "\n",
       "  ON_SHORE_FLAG ONSHORE_ASSC_CNTY_FLAG  \n",
       "0             Y                      N  \n",
       "1             Y                      N  \n",
       "2             Y                      N  \n",
       "3             Y                      N  \n",
       "4             Y                      N  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_sample(county_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNTY_NO</th>\n",
       "      <th>DISTRICT_NO</th>\n",
       "      <th>CYCLE_YEAR</th>\n",
       "      <th>CYCLE_MONTH</th>\n",
       "      <th>CYCLE_YEAR_MONTH</th>\n",
       "      <th>CNTY_OIL_PROD_VOL</th>\n",
       "      <th>CNTY_OIL_ALLOW</th>\n",
       "      <th>CNTY_OIL_ENDING_BAL</th>\n",
       "      <th>CNTY_GAS_PROD_VOL</th>\n",
       "      <th>CNTY_GAS_ALLOW</th>\n",
       "      <th>CNTY_GAS_LIFT_INJ_VOL</th>\n",
       "      <th>CNTY_COND_PROD_VOL</th>\n",
       "      <th>CNTY_COND_LIMIT</th>\n",
       "      <th>CNTY_COND_ENDING_BAL</th>\n",
       "      <th>CNTY_CSGD_PROD_VOL</th>\n",
       "      <th>CNTY_CSGD_LIMIT</th>\n",
       "      <th>CNTY_CSGD_GAS_LIFT</th>\n",
       "      <th>CNTY_OIL_TOT_DISP</th>\n",
       "      <th>CNTY_GAS_TOT_DISP</th>\n",
       "      <th>CNTY_COND_TOT_DISP</th>\n",
       "      <th>CNTY_CSGD_TOT_DISP</th>\n",
       "      <th>COUNTY_NAME</th>\n",
       "      <th>DISTRICT_NAME</th>\n",
       "      <th>OIL_GAS_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1993</td>\n",
       "      <td>1</td>\n",
       "      <td>199301</td>\n",
       "      <td>7355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6347</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ANDERSON</td>\n",
       "      <td>5</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1993</td>\n",
       "      <td>2</td>\n",
       "      <td>199302</td>\n",
       "      <td>6312</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ANDERSON</td>\n",
       "      <td>5</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1993</td>\n",
       "      <td>3</td>\n",
       "      <td>199303</td>\n",
       "      <td>6222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4973</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ANDERSON</td>\n",
       "      <td>5</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1993</td>\n",
       "      <td>4</td>\n",
       "      <td>199304</td>\n",
       "      <td>6139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ANDERSON</td>\n",
       "      <td>5</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1993</td>\n",
       "      <td>5</td>\n",
       "      <td>199305</td>\n",
       "      <td>5785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ANDERSON</td>\n",
       "      <td>5</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   COUNTY_NO  DISTRICT_NO  CYCLE_YEAR  CYCLE_MONTH  CYCLE_YEAR_MONTH  \\\n",
       "0          1            5        1993            1            199301   \n",
       "1          1            5        1993            2            199302   \n",
       "2          1            5        1993            3            199303   \n",
       "3          1            5        1993            4            199304   \n",
       "4          1            5        1993            5            199305   \n",
       "\n",
       "   CNTY_OIL_PROD_VOL  CNTY_OIL_ALLOW  CNTY_OIL_ENDING_BAL  CNTY_GAS_PROD_VOL  \\\n",
       "0               7355             NaN                  NaN                  0   \n",
       "1               6312             NaN                  NaN                  0   \n",
       "2               6222             NaN                  NaN                  0   \n",
       "3               6139             NaN                  NaN                  0   \n",
       "4               5785             NaN                  NaN                  0   \n",
       "\n",
       "   CNTY_GAS_ALLOW  CNTY_GAS_LIFT_INJ_VOL  CNTY_COND_PROD_VOL  CNTY_COND_LIMIT  \\\n",
       "0             NaN                    NaN                   0              NaN   \n",
       "1             NaN                    NaN                   0              NaN   \n",
       "2             NaN                    NaN                   0              NaN   \n",
       "3             NaN                    NaN                   0              NaN   \n",
       "4             NaN                    NaN                   0              NaN   \n",
       "\n",
       "   CNTY_COND_ENDING_BAL  CNTY_CSGD_PROD_VOL  CNTY_CSGD_LIMIT  \\\n",
       "0                   NaN                6347              NaN   \n",
       "1                   NaN                4919              NaN   \n",
       "2                   NaN                4973              NaN   \n",
       "3                   NaN                4410              NaN   \n",
       "4                   NaN                5961              NaN   \n",
       "\n",
       "   CNTY_CSGD_GAS_LIFT  CNTY_OIL_TOT_DISP  CNTY_GAS_TOT_DISP  \\\n",
       "0                 NaN                NaN                NaN   \n",
       "1                 NaN                NaN                NaN   \n",
       "2                 NaN                NaN                NaN   \n",
       "3                 NaN                NaN                NaN   \n",
       "4                 NaN                NaN                NaN   \n",
       "\n",
       "   CNTY_COND_TOT_DISP  CNTY_CSGD_TOT_DISP COUNTY_NAME  DISTRICT_NAME  \\\n",
       "0                 NaN                 NaN    ANDERSON              5   \n",
       "1                 NaN                 NaN    ANDERSON              5   \n",
       "2                 NaN                 NaN    ANDERSON              5   \n",
       "3                 NaN                 NaN    ANDERSON              5   \n",
       "4                 NaN                 NaN    ANDERSON              5   \n",
       "\n",
       "  OIL_GAS_CODE  \n",
       "0            O  \n",
       "1            O  \n",
       "2            O  \n",
       "3            O  \n",
       "4            O  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['COUNTY_NO', 'DISTRICT_NO', 'CYCLE_YEAR', 'CYCLE_MONTH',\n",
       "       'CYCLE_YEAR_MONTH', 'CNTY_OIL_PROD_VOL', 'CNTY_OIL_ALLOW',\n",
       "       'CNTY_OIL_ENDING_BAL', 'CNTY_GAS_PROD_VOL', 'CNTY_GAS_ALLOW',\n",
       "       'CNTY_GAS_LIFT_INJ_VOL', 'CNTY_COND_PROD_VOL', 'CNTY_COND_LIMIT',\n",
       "       'CNTY_COND_ENDING_BAL', 'CNTY_CSGD_PROD_VOL', 'CNTY_CSGD_LIMIT',\n",
       "       'CNTY_CSGD_GAS_LIFT', 'CNTY_OIL_TOT_DISP', 'CNTY_GAS_TOT_DISP',\n",
       "       'CNTY_COND_TOT_DISP', 'CNTY_CSGD_TOT_DISP', 'COUNTY_NAME',\n",
       "       'DISTRICT_NAME', 'OIL_GAS_CODE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(county_cycle_path, sep=\"}\", nrows=5)\n",
    "display(df)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oil_gas_code</th>\n",
       "      <th>district_no</th>\n",
       "      <th>lease_no</th>\n",
       "      <th>cycle_year</th>\n",
       "      <th>cycle_month</th>\n",
       "      <th>cycle_year_month</th>\n",
       "      <th>lease_no_district_no</th>\n",
       "      <th>operator_no</th>\n",
       "      <th>field_no</th>\n",
       "      <th>field_type</th>\n",
       "      <th>gas_well_no</th>\n",
       "      <th>prod_report_filed_flag</th>\n",
       "      <th>lease_oil_prod_vol</th>\n",
       "      <th>lease_oil_allow</th>\n",
       "      <th>lease_oil_ending_bal</th>\n",
       "      <th>lease_gas_prod_vol</th>\n",
       "      <th>lease_gas_allow</th>\n",
       "      <th>lease_gas_lift_inj_vol</th>\n",
       "      <th>lease_cond_prod_vol</th>\n",
       "      <th>lease_cond_limit</th>\n",
       "      <th>lease_cond_ending_bal</th>\n",
       "      <th>lease_csgd_prod_vol</th>\n",
       "      <th>lease_csgd_limit</th>\n",
       "      <th>lease_csgd_gas_lift</th>\n",
       "      <th>lease_oil_tot_disp</th>\n",
       "      <th>lease_gas_tot_disp</th>\n",
       "      <th>lease_cond_tot_disp</th>\n",
       "      <th>lease_csgd_tot_disp</th>\n",
       "      <th>district_name</th>\n",
       "      <th>lease_name</th>\n",
       "      <th>operator_name</th>\n",
       "      <th>field_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G</td>\n",
       "      <td>1</td>\n",
       "      <td>498</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>202301</td>\n",
       "      <td>49801</td>\n",
       "      <td>100222</td>\n",
       "      <td>93874100</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>01</td>\n",
       "      <td>CAUTHORN -A-</td>\n",
       "      <td>CONTANGO RESOURCES, LLC</td>\n",
       "      <td>VINEGARONE (STRAWN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G</td>\n",
       "      <td>1</td>\n",
       "      <td>498</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>202302</td>\n",
       "      <td>49801</td>\n",
       "      <td>100222</td>\n",
       "      <td>93874100</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>01</td>\n",
       "      <td>CAUTHORN -A-</td>\n",
       "      <td>CONTANGO RESOURCES, LLC</td>\n",
       "      <td>VINEGARONE (STRAWN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G</td>\n",
       "      <td>1</td>\n",
       "      <td>498</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>202303</td>\n",
       "      <td>49801</td>\n",
       "      <td>100222</td>\n",
       "      <td>93874100</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>01</td>\n",
       "      <td>CAUTHORN -A-</td>\n",
       "      <td>CONTANGO RESOURCES, LLC</td>\n",
       "      <td>VINEGARONE (STRAWN)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  oil_gas_code  district_no  lease_no  cycle_year  cycle_month  \\\n",
       "0            G            1       498        2023            1   \n",
       "1            G            1       498        2023            2   \n",
       "2            G            1       498        2023            3   \n",
       "\n",
       "   cycle_year_month  lease_no_district_no  operator_no  field_no field_type  \\\n",
       "0            202301                 49801       100222  93874100         CA   \n",
       "1            202302                 49801       100222  93874100         CA   \n",
       "2            202303                 49801       100222  93874100         CA   \n",
       "\n",
       "  gas_well_no prod_report_filed_flag  lease_oil_prod_vol  lease_oil_allow  \\\n",
       "0           1                      Y                   0                0   \n",
       "1           1                      Y                   0                0   \n",
       "2           1                      Y                   0                0   \n",
       "\n",
       "   lease_oil_ending_bal  lease_gas_prod_vol  lease_gas_allow  \\\n",
       "0                     0                   0                0   \n",
       "1                     0                   0                0   \n",
       "2                     0                   0                0   \n",
       "\n",
       "   lease_gas_lift_inj_vol  lease_cond_prod_vol  lease_cond_limit  \\\n",
       "0                       0                    0                 0   \n",
       "1                       0                    0                 0   \n",
       "2                       0                    0                 0   \n",
       "\n",
       "   lease_cond_ending_bal  lease_csgd_prod_vol  lease_csgd_limit  \\\n",
       "0                      0                    0                 0   \n",
       "1                      0                    0                 0   \n",
       "2                      0                    0                 0   \n",
       "\n",
       "   lease_csgd_gas_lift  lease_oil_tot_disp  lease_gas_tot_disp  \\\n",
       "0                    0                   0                   0   \n",
       "1                    0                   0                   0   \n",
       "2                    0                   0                   0   \n",
       "\n",
       "   lease_cond_tot_disp  lease_csgd_tot_disp district_name    lease_name  \\\n",
       "0                    0                    0            01  CAUTHORN -A-   \n",
       "1                    0                    0            01  CAUTHORN -A-   \n",
       "2                    0                    0            01  CAUTHORN -A-   \n",
       "\n",
       "             operator_name           field_name  \n",
       "0  CONTANGO RESOURCES, LLC  VINEGARONE (STRAWN)  \n",
       "1  CONTANGO RESOURCES, LLC  VINEGARONE (STRAWN)  \n",
       "2  CONTANGO RESOURCES, LLC  VINEGARONE (STRAWN)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oil_gas_code</th>\n",
       "      <th>district_no</th>\n",
       "      <th>lease_no</th>\n",
       "      <th>cycle_year</th>\n",
       "      <th>cycle_month</th>\n",
       "      <th>cycle_year_month</th>\n",
       "      <th>lease_no_district_no</th>\n",
       "      <th>operator_no</th>\n",
       "      <th>field_no</th>\n",
       "      <th>field_type</th>\n",
       "      <th>gas_well_no</th>\n",
       "      <th>prod_report_filed_flag</th>\n",
       "      <th>lease_oil_prod_vol</th>\n",
       "      <th>lease_oil_allow</th>\n",
       "      <th>lease_oil_ending_bal</th>\n",
       "      <th>lease_gas_prod_vol</th>\n",
       "      <th>lease_gas_allow</th>\n",
       "      <th>lease_gas_lift_inj_vol</th>\n",
       "      <th>lease_cond_prod_vol</th>\n",
       "      <th>lease_cond_limit</th>\n",
       "      <th>lease_cond_ending_bal</th>\n",
       "      <th>lease_csgd_prod_vol</th>\n",
       "      <th>lease_csgd_limit</th>\n",
       "      <th>lease_csgd_gas_lift</th>\n",
       "      <th>lease_oil_tot_disp</th>\n",
       "      <th>lease_gas_tot_disp</th>\n",
       "      <th>lease_cond_tot_disp</th>\n",
       "      <th>lease_csgd_tot_disp</th>\n",
       "      <th>district_name</th>\n",
       "      <th>lease_name</th>\n",
       "      <th>operator_name</th>\n",
       "      <th>field_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>623719</th>\n",
       "      <td>O</td>\n",
       "      <td>14</td>\n",
       "      <td>6871</td>\n",
       "      <td>1993</td>\n",
       "      <td>10</td>\n",
       "      <td>199310</td>\n",
       "      <td>687114</td>\n",
       "      <td>534237</td>\n",
       "      <td>15217250</td>\n",
       "      <td>R</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Y</td>\n",
       "      <td>151</td>\n",
       "      <td>155</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7502</td>\n",
       "      <td>0</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>ISAACS, JOHN C. JR. \"G\"</td>\n",
       "      <td>MAXUS EXPLORATION COMPANY</td>\n",
       "      <td>CANADIAN, SE (DOUGLAS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623720</th>\n",
       "      <td>O</td>\n",
       "      <td>14</td>\n",
       "      <td>6871</td>\n",
       "      <td>1993</td>\n",
       "      <td>11</td>\n",
       "      <td>199311</td>\n",
       "      <td>687114</td>\n",
       "      <td>534237</td>\n",
       "      <td>15217250</td>\n",
       "      <td>R</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Y</td>\n",
       "      <td>138</td>\n",
       "      <td>150</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>7260</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>ISAACS, JOHN C. JR. \"G\"</td>\n",
       "      <td>MAXUS EXPLORATION COMPANY</td>\n",
       "      <td>CANADIAN, SE (DOUGLAS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623721</th>\n",
       "      <td>O</td>\n",
       "      <td>14</td>\n",
       "      <td>6871</td>\n",
       "      <td>1993</td>\n",
       "      <td>12</td>\n",
       "      <td>199312</td>\n",
       "      <td>687114</td>\n",
       "      <td>534237</td>\n",
       "      <td>15217250</td>\n",
       "      <td>R</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Y</td>\n",
       "      <td>97</td>\n",
       "      <td>155</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>7502</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>10</td>\n",
       "      <td>ISAACS, JOHN C. JR. \"G\"</td>\n",
       "      <td>MAXUS EXPLORATION COMPANY</td>\n",
       "      <td>CANADIAN, SE (DOUGLAS)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       oil_gas_code  district_no  lease_no  cycle_year  cycle_month  \\\n",
       "623719            O           14      6871        1993           10   \n",
       "623720            O           14      6871        1993           11   \n",
       "623721            O           14      6871        1993           12   \n",
       "\n",
       "        cycle_year_month  lease_no_district_no  operator_no  field_no  \\\n",
       "623719            199310                687114       534237  15217250   \n",
       "623720            199311                687114       534237  15217250   \n",
       "623721            199312                687114       534237  15217250   \n",
       "\n",
       "       field_type gas_well_no prod_report_filed_flag  lease_oil_prod_vol  \\\n",
       "623719         R         <NA>                      Y                 151   \n",
       "623720         R         <NA>                      Y                 138   \n",
       "623721         R         <NA>                      Y                  97   \n",
       "\n",
       "        lease_oil_allow  lease_oil_ending_bal  lease_gas_prod_vol  \\\n",
       "623719              155                   123                   0   \n",
       "623720              150                    93                   0   \n",
       "623721              155                   184                   0   \n",
       "\n",
       "        lease_gas_allow  lease_gas_lift_inj_vol  lease_cond_prod_vol  \\\n",
       "623719                0                       0                    0   \n",
       "623720                0                       0                    0   \n",
       "623721                0                       0                    0   \n",
       "\n",
       "        lease_cond_limit  lease_cond_ending_bal  lease_csgd_prod_vol  \\\n",
       "623719                 0                      0                    6   \n",
       "623720                 0                      0                   42   \n",
       "623721                 0                      0                   62   \n",
       "\n",
       "        lease_csgd_limit  lease_csgd_gas_lift  lease_oil_tot_disp  \\\n",
       "623719              7502                    0                 178   \n",
       "623720              7260                    0                 168   \n",
       "623721              7502                    0                   6   \n",
       "\n",
       "        lease_gas_tot_disp  lease_cond_tot_disp  lease_csgd_tot_disp  \\\n",
       "623719                   0                    0                    6   \n",
       "623720                   0                    0                   42   \n",
       "623721                   0                    0                   62   \n",
       "\n",
       "       district_name               lease_name              operator_name  \\\n",
       "623719            10  ISAACS, JOHN C. JR. \"G\"  MAXUS EXPLORATION COMPANY   \n",
       "623720            10  ISAACS, JOHN C. JR. \"G\"  MAXUS EXPLORATION COMPANY   \n",
       "623721            10  ISAACS, JOHN C. JR. \"G\"  MAXUS EXPLORATION COMPANY   \n",
       "\n",
       "                    field_name  \n",
       "623719  CANADIAN, SE (DOUGLAS)  \n",
       "623720  CANADIAN, SE (DOUGLAS)  \n",
       "623721  CANADIAN, SE (DOUGLAS)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oil_gas_code</th>\n",
       "      <th>district_no</th>\n",
       "      <th>lease_no</th>\n",
       "      <th>cycle_year</th>\n",
       "      <th>cycle_month</th>\n",
       "      <th>cycle_year_month</th>\n",
       "      <th>lease_no_district_no</th>\n",
       "      <th>operator_no</th>\n",
       "      <th>field_no</th>\n",
       "      <th>field_type</th>\n",
       "      <th>gas_well_no</th>\n",
       "      <th>prod_report_filed_flag</th>\n",
       "      <th>lease_oil_prod_vol</th>\n",
       "      <th>lease_oil_allow</th>\n",
       "      <th>lease_oil_ending_bal</th>\n",
       "      <th>lease_gas_prod_vol</th>\n",
       "      <th>lease_gas_allow</th>\n",
       "      <th>lease_gas_lift_inj_vol</th>\n",
       "      <th>lease_cond_prod_vol</th>\n",
       "      <th>lease_cond_limit</th>\n",
       "      <th>lease_cond_ending_bal</th>\n",
       "      <th>lease_csgd_prod_vol</th>\n",
       "      <th>lease_csgd_limit</th>\n",
       "      <th>lease_csgd_gas_lift</th>\n",
       "      <th>lease_oil_tot_disp</th>\n",
       "      <th>lease_gas_tot_disp</th>\n",
       "      <th>lease_cond_tot_disp</th>\n",
       "      <th>lease_csgd_tot_disp</th>\n",
       "      <th>district_name</th>\n",
       "      <th>lease_name</th>\n",
       "      <th>operator_name</th>\n",
       "      <th>field_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=115</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>string</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: rename, 3 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                oil_gas_code district_no lease_no cycle_year cycle_month cycle_year_month lease_no_district_no operator_no field_no field_type gas_well_no prod_report_filed_flag lease_oil_prod_vol lease_oil_allow lease_oil_ending_bal lease_gas_prod_vol lease_gas_allow lease_gas_lift_inj_vol lease_cond_prod_vol lease_cond_limit lease_cond_ending_bal lease_csgd_prod_vol lease_csgd_limit lease_csgd_gas_lift lease_oil_tot_disp lease_gas_tot_disp lease_cond_tot_disp lease_csgd_tot_disp district_name lease_name operator_name field_name\n",
       "npartitions=115                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "                      string       int64    int64      int64       int64            int64                int64       int64    int64     string      string                 string              int64           int64                int64              int64           int64                  int64               int64            int64                 int64               int64            int64               int64              int64              int64               int64               int64        string     string        string     string\n",
       "                         ...         ...      ...        ...         ...              ...                  ...         ...      ...        ...         ...                    ...                ...             ...                  ...                ...             ...                    ...                 ...              ...                   ...                 ...              ...                 ...                ...                ...                 ...                 ...           ...        ...           ...        ...\n",
       "...                      ...         ...      ...        ...         ...              ...                  ...         ...      ...        ...         ...                    ...                ...             ...                  ...                ...             ...                    ...                 ...              ...                   ...                 ...              ...                 ...                ...                ...                 ...                 ...           ...        ...           ...        ...\n",
       "                         ...         ...      ...        ...         ...              ...                  ...         ...      ...        ...         ...                    ...                ...             ...                  ...                ...             ...                    ...                 ...              ...                   ...                 ...              ...                 ...                ...                ...                 ...                 ...           ...        ...           ...        ...\n",
       "                         ...         ...      ...        ...         ...              ...                  ...         ...      ...        ...         ...                    ...                ...             ...                  ...                ...             ...                    ...                 ...              ...                   ...                 ...              ...                 ...                ...                ...                 ...                 ...           ...        ...           ...        ...\n",
       "Dask Name: rename, 3 graph layers"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_lease = dd.read_csv(\n",
    "    lease_path,\n",
    "    sep=\"}\",\n",
    "    blocksize=1e8,\n",
    "    dtype={\"DISTRICT_NAME\": str, \"GAS_WELL_NO\": str},\n",
    ")\n",
    "ddf_lease.columns = [pascal_to_snake(col) for col in ddf_lease.columns]\n",
    "\n",
    "n_rows = len(ddf_lease)\n",
    "\n",
    "\n",
    "display(ddf_lease.head(3))\n",
    "display(ddf_lease.tail(3))\n",
    "ddf_lease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['oil_gas_code', 'district_no', 'lease_no', 'cycle_year', 'cycle_month',\n",
       "       'cycle_year_month', 'lease_no_district_no', 'operator_no', 'field_no',\n",
       "       'field_type', 'gas_well_no', 'prod_report_filed_flag',\n",
       "       'lease_oil_prod_vol', 'lease_oil_allow', 'lease_oil_ending_bal',\n",
       "       'lease_gas_prod_vol', 'lease_gas_allow', 'lease_gas_lift_inj_vol',\n",
       "       'lease_cond_prod_vol', 'lease_cond_limit', 'lease_cond_ending_bal',\n",
       "       'lease_csgd_prod_vol', 'lease_csgd_limit', 'lease_csgd_gas_lift',\n",
       "       'lease_oil_tot_disp', 'lease_gas_tot_disp', 'lease_cond_tot_disp',\n",
       "       'lease_csgd_tot_disp', 'district_name', 'lease_name', 'operator_name',\n",
       "       'field_name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_lease.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to a certain yearse\n",
    "ddf_lease_filtered = ddf_lease[ddf_lease[\"cycle_year\"] > 2012]\n",
    "\n",
    "# repartition the dataframe\n",
    "# ddf_lease_filtered = ddf_lease_filtered.repartition(npartitions=1000)\n",
    "\n",
    "operators_by_lease = ddf_lease_filtered[\"operator_name\"].value_counts()\n",
    "\n",
    "top_operators_by_lease = operators_by_lease.nlargest(10)\n",
    "\n",
    "# top_operators_by_lease = ddf_lease_filtered[\"operator_name\"].value_counts().nlargest(10)\n",
    "\n",
    "desc = ddf_lease_filtered.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='p488544'>\n",
       "  <div id=\"ebc6193a-5843-4927-b26b-bdd9fbb672f3\" data-root-id=\"p488544\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"624c0348-6b7e-44c9-b4d8-8fc3c9241f14\":{\"version\":\"3.3.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Row\",\"id\":\"p488544\",\"attributes\":{\"name\":\"Row03042\",\"tags\":[\"embedded\"],\"stylesheets\":[\"\\n:host(.pn-loading.pn-arc):before, .pn-loading.pn-arc:before {\\n  background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n  background-size: auto calc(min(50%, 400px));\\n}\",{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"p488547\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.3.1/dist/css/loading.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"p488609\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.3.1/dist/css/listpanel.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"p488545\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.3.1/dist/bundled/theme/default.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"p488546\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.3.1/dist/bundled/theme/native.css\"}}],\"min_width\":600,\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"Spacer\",\"id\":\"p488548\",\"attributes\":{\"name\":\"HSpacer03053\",\"stylesheets\":[\"\\n:host(.pn-loading.pn-arc):before, .pn-loading.pn-arc:before {\\n  background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n  background-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"p488547\"},{\"id\":\"p488545\"},{\"id\":\"p488546\"}],\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\"}},{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p488561\",\"attributes\":{\"height\":400,\"margin\":[5,10],\"sizing_mode\":\"fixed\",\"align\":\"start\",\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p488554\",\"attributes\":{\"tags\":[[[\"count\",\"count\",null]],[]],\"end\":1385337.6,\"reset_start\":0.0,\"reset_end\":1385337.6}},\"y_range\":{\"type\":\"object\",\"name\":\"FactorRange\",\"id\":\"p488555\",\"attributes\":{\"tags\":[[[\"operator_name\",\"operator_name\",null]],{\"type\":\"map\",\"entries\":[[\"invert_yaxis\",false],[\"autorange\",false]]}],\"factors\":[\"XTO ENERGY INC.\",\"DEVON ENERGY PRODUCTION CO, L.P.\",\"HILCORP ENERGY COMPANY\",\"ENERVEST OPERATING, L.L.C.\",\"EOG RESOURCES, INC.\",\"UPP OPERATING, LLC\",\"PIONEER NATURAL RES. USA, INC.\",\"SCOUT ENERGY MANAGEMENT LLC\",\"ANADARKO E&P ONSHORE LLC\",\"APACHE CORPORATION\"]}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p488571\"},\"y_scale\":{\"type\":\"object\",\"name\":\"CategoricalScale\",\"id\":\"p488572\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p488564\",\"attributes\":{\"text\":\"Top 10 Operators by Lease Count\",\"text_color\":\"black\",\"text_font_size\":\"12pt\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p488597\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p488588\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p488589\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p488590\"},\"data\":{\"type\":\"map\",\"entries\":[[\"operator_name\",[\"XTO ENERGY INC.\",\"DEVON ENERGY PRODUCTION CO, L.P.\",\"HILCORP ENERGY COMPANY\",\"ENERVEST OPERATING, L.L.C.\",\"EOG RESOURCES, INC.\",\"UPP OPERATING, LLC\",\"PIONEER NATURAL RES. USA, INC.\",\"SCOUT ENERGY MANAGEMENT LLC\",\"ANADARKO E&P ONSHORE LLC\",\"APACHE CORPORATION\"]],[\"count\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"66kTAPGICwA/XQsAWDoKAL3oBwDYMQcAivUGAPX7BQDmmgUAWeoEAA==\"},\"shape\":[10],\"dtype\":\"int32\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p488598\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p488599\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p488594\",\"attributes\":{\"tags\":[\"apply_ranges\"],\"y\":{\"type\":\"field\",\"field\":\"operator_name\"},\"height\":{\"type\":\"value\",\"value\":0.8},\"right\":{\"type\":\"field\",\"field\":\"count\"},\"fill_color\":{\"type\":\"value\",\"value\":\"#30a2da\"},\"hatch_color\":{\"type\":\"value\",\"value\":\"#30a2da\"}}},\"selection_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p488600\",\"attributes\":{\"tags\":[\"apply_ranges\"],\"y\":{\"type\":\"field\",\"field\":\"operator_name\"},\"height\":{\"type\":\"value\",\"value\":0.8},\"left\":{\"type\":\"value\",\"value\":0},\"right\":{\"type\":\"field\",\"field\":\"count\"},\"line_color\":{\"type\":\"value\",\"value\":\"black\"},\"line_alpha\":{\"type\":\"value\",\"value\":1.0},\"line_width\":{\"type\":\"value\",\"value\":1},\"line_join\":{\"type\":\"value\",\"value\":\"bevel\"},\"line_cap\":{\"type\":\"value\",\"value\":\"butt\"},\"line_dash\":{\"type\":\"value\",\"value\":[]},\"line_dash_offset\":{\"type\":\"value\",\"value\":0},\"fill_color\":{\"type\":\"value\",\"value\":\"#30a2da\"},\"fill_alpha\":{\"type\":\"value\",\"value\":1.0},\"hatch_color\":{\"type\":\"value\",\"value\":\"#30a2da\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":1.0},\"hatch_scale\":{\"type\":\"value\",\"value\":12.0},\"hatch_pattern\":{\"type\":\"value\",\"value\":null},\"hatch_weight\":{\"type\":\"value\",\"value\":1.0}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p488595\",\"attributes\":{\"tags\":[\"apply_ranges\"],\"y\":{\"type\":\"field\",\"field\":\"operator_name\"},\"height\":{\"type\":\"value\",\"value\":0.8},\"right\":{\"type\":\"field\",\"field\":\"count\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"#30a2da\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"value\",\"value\":\"#30a2da\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p488596\",\"attributes\":{\"tags\":[\"apply_ranges\"],\"y\":{\"type\":\"field\",\"field\":\"operator_name\"},\"height\":{\"type\":\"value\",\"value\":0.8},\"right\":{\"type\":\"field\",\"field\":\"count\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"#30a2da\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"value\",\"value\":\"#30a2da\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p488570\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p488559\",\"attributes\":{\"tags\":[\"hv_created\"],\"renderers\":\"auto\",\"zoom_together\":\"none\"}},{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p488560\",\"attributes\":{\"tags\":[\"hv_created\"],\"renderers\":[{\"id\":\"p488597\"}],\"tooltips\":[[\"operator_name\",\"@{operator_name}\"],[\"count\",\"@{count}\"]]}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p488583\"},{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p488584\"},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p488585\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p488586\",\"attributes\":{\"syncable\":false,\"level\":\"overlay\",\"visible\":false,\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5}}}},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p488587\"}],\"active_drag\":{\"id\":\"p488585\"}}},\"toolbar_location\":\"above\",\"left\":[{\"type\":\"object\",\"name\":\"CategoricalAxis\",\"id\":\"p488578\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"CategoricalTicker\",\"id\":\"p488579\"},\"formatter\":{\"type\":\"object\",\"name\":\"CategoricalTickFormatter\",\"id\":\"p488580\"},\"axis_label\":\"\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p488581\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p488573\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p488574\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p488575\"},\"axis_label\":\"\",\"axis_label_text_font_size\":\"0pt\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p488576\"},\"major_label_text_font_size\":\"0pt\",\"major_tick_line_color\":null,\"minor_tick_line_color\":null}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p488577\",\"attributes\":{\"axis\":{\"id\":\"p488573\"},\"grid_line_color\":null}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p488582\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p488578\"},\"grid_line_color\":null}}],\"min_border_top\":10,\"min_border_bottom\":10,\"min_border_left\":10,\"min_border_right\":10,\"output_backend\":\"webgl\"}},{\"type\":\"object\",\"name\":\"Spacer\",\"id\":\"p488607\",\"attributes\":{\"name\":\"HSpacer03056\",\"stylesheets\":[\"\\n:host(.pn-loading.pn-arc):before, .pn-loading.pn-arc:before {\\n  background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n  background-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"p488547\"},{\"id\":\"p488545\"},{\"id\":\"p488546\"}],\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\"}}]}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"toggle_value1\",\"properties\":[{\"name\":\"active_icons\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"options\",\"kind\":\"Any\",\"default\":{\"type\":\"map\",\"entries\":[[\"favorite\",\"heart\"]]}},{\"name\":\"value\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"_reactions\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"_base_url\",\"kind\":\"Any\",\"default\":\"https://tabler-icons.io/static/tabler-icons/icons/\"}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"value\",\"kind\":\"Any\",\"default\":null},{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]}]}};\n",
       "  var render_items = [{\"docid\":\"624c0348-6b7e-44c9-b4d8-8fc3c9241f14\",\"roots\":{\"p488544\":\"ebc6193a-5843-4927-b26b-bdd9fbb672f3\"},\"root_ids\":[\"p488544\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  const is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1\n",
       "  function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && (id_el.children[0].className === 'bk-root')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version && !is_dev) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       ":Bars   [operator_name]   (count)"
      ]
     },
     "execution_count": 149,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "p488544"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_operators_computed, operators_computed = dask.compute(\n",
    "    top_operators_by_lease, operators_by_lease\n",
    ")\n",
    "\n",
    "top_operators_computed.hvplot.barh(\n",
    "    height=400,\n",
    "    width=600,\n",
    "    title=\"Top 10 Operators by Lease Count\",\n",
    "    xlabel=\"\",\n",
    "    ylabel=\"\",\n",
    "    xaxis=\"bare\",\n",
    "    hover_cols=\"all\",\n",
    ").opts(\n",
    "    active_tools=[\"box_zoom\"],\n",
    "    toolbar=\"above\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "operator_name\n",
       "XTO ENERGY INC.                     1288683\n",
       "DEVON ENERGY PRODUCTION CO, L.P.     755953\n",
       "HILCORP ENERGY COMPANY               744767\n",
       "ENERVEST OPERATING, L.L.C.           670296\n",
       "EOG RESOURCES, INC.                  518333\n",
       "                                     ...   \n",
       "SPI OPERATIONS LLC                        1\n",
       "STONEBRIAR ENERGY                         1\n",
       "SUNRAY OIL CO., INC.                      1\n",
       "TGB EQUIPMENT LEASING, LLC                1\n",
       "WES PRO, INC.                             1\n",
       "Name: count, Length: 11531, dtype: int64[pyarrow]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "operators_computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = ddf_lease_filtered.groupby([\"cycle_year\", \"operator_name\", \"district_no\"])[\n",
    "    \"lease_oil_prod_vol\",\n",
    "    \"lease_gas_prod_vol\",\n",
    "    \"lease_cond_prod_vol\",\n",
    "    \"lease_csgd_prod_vol\",\n",
    "].sum()\n",
    "\n",
    "# grouped.visualize(optimize_graph=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_computed = grouped.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_prod_vol(grouped_computed, product):\n",
    "    \n",
    "    max_index = (\n",
    "        grouped_computed.reset_index()\n",
    "        .groupby([\"cycle_year\", \"district_no\"])[f\"lease_{product}_prod_vol\"]\n",
    "        .idxmax()\n",
    "    )\n",
    "    result = grouped_computed.reset_index().loc[max_index]\n",
    "    result[\"product\"] = product\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function for different products\n",
    "oil_result = get_max_prod_vol(grouped_computed, \"oil\")\n",
    "gas_result = get_max_prod_vol(grouped_computed, \"gas\")\n",
    "cond_result = get_max_prod_vol(grouped_computed, \"cond\")\n",
    "csgd_result = get_max_prod_vol(grouped_computed, \"csgd\")\n",
    "\n",
    "# Concatenate the results\n",
    "final_result = pd.concat([oil_result, gas_result, cond_result, csgd_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 572 entries, 135179 to 4871\n",
      "Data columns (total 8 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   cycle_year           572 non-null    int64 \n",
      " 1   operator_name        572 non-null    string\n",
      " 2   district_no          572 non-null    int64 \n",
      " 3   lease_oil_prod_vol   572 non-null    int64 \n",
      " 4   lease_gas_prod_vol   572 non-null    int64 \n",
      " 5   lease_cond_prod_vol  572 non-null    int64 \n",
      " 6   lease_csgd_prod_vol  572 non-null    int64 \n",
      " 7   product              572 non-null    object\n",
      "dtypes: int64(6), object(1), string(1)\n",
      "memory usage: 79.8 KB\n"
     ]
    }
   ],
   "source": [
    "final_result.sort_values(by=[\"cycle_year\", \"district_no\", \"product\"]).info(\n",
    "    memory_usage=\"deep\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Dask client with a higher memory limit\n",
    "client = Client(memory_limit=\"10GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the CSV file\n",
    "# lease_path = \"../data/raw/lease_table.csv\"\n",
    "\n",
    "# Define the column types\n",
    "column_types = {\"DISTRICT_NAME\": str, \"GAS_WELL_NO\": str}\n",
    "\n",
    "# Read the CSV file with Dask\n",
    "lease_table_df = dd.read_csv(lease_path, sep=\"}\", blocksize=1e8, dtype=column_types)\n",
    "\n",
    "# Convert column names to snake case\n",
    "lease_table_df.columns = [pascal_to_snake(col) for col in lease_table_df.columns]\n",
    "\n",
    "# Repartition the DataFrame into 10 partitions\n",
    "lease_table_df = lease_table_df.repartition(npartitions=10)\n",
    "\n",
    "# Save the DataFrame as a Parquet file\n",
    "lease_table_df.to_parquet(\"../data/processed/og_lease_cycle_data_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of workers: 4\n"
     ]
    }
   ],
   "source": [
    "info = client.scheduler_info()  # get scheduler info\n",
    "\n",
    "workers = info[\"workers\"]  # get the workers\n",
    "\n",
    "print(f\"Number of workers: {len(workers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
