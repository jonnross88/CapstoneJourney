{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This project is part of a Capstone project for Springboard Data Science Career Track.\n",
    "\n",
    "The goal of this project is to develop a machine learning model to rank and predict the likelihood that an oil company will initiate a frac job in a county within the Permian Basin in the first quarter of 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import concurrent.futures as cf\n",
    "import random\n",
    "import re\n",
    "import tempfile\n",
    "import warnings\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "import missingno as msno\n",
    "import cartopy.crs as ccrs\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import geopandas as gpd\n",
    "import geoviews as gv\n",
    "import geoviews.tile_sources as gts\n",
    "import colorcet as cc\n",
    "import holoviews as hv\n",
    "import hvplot.pandas  # noqa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyproj\n",
    "from fiona.io import ZipMemoryFile\n",
    "from pyvis.network import Network\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "gv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Test initial print statement\n",
    "print(\"CapstoneJourney begins!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants\n",
    "Let's start by defining some constants that will be used throughout this notebook.\n",
    "\n",
    "Most of the data was first downloaded from external websites and then uploaded onto a cloud storage bucket. This was done to ensure consistency and availability during the project. A brief description of the data and its original source link is referenced below.\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "The following table provides an overview of the data sources used in this project:\n",
    "\n",
    "| Dataset Name | Source URL | Original Source | Description | Date Downloaded |\n",
    "|--------------|------------|-----------------|-------------|-----------------|\n",
    "|RegistryUpload Table | [link](https://fracfocus.org/data-download) | FracFocus | This table contains each disclosureâ€™s header information such as the job date, API number, location, base water volume, and total vertical depth. | 2023-11-11 |\n",
    "RBDMSWells | [link](https://gisdata-occokc.opendata.arcgis.com/datasets/OCCOKC::rbdms-wells/about) | Oklahoma Corporation Commission | This table contains Oklahoma RBDMS statewide well data | 2023-11-23 |\n",
    "| Wolfcamp Delaware Play Boundary | [link]((https://www.eia.gov/maps/maps.htm))| EIA | Permian Basin, Delaware Sub-Basin: Wolfcamp play boundary (9/4/2018) | 2023-11-19 |\n",
    "| Wolfcamp Midland Play Boundaries | [link]((https://www.eia.gov/maps/maps.htm))| EIA | Wolfcamp A, B, C, and D play boundaries, Midland Basin (6/4/2020) | 2023-11-21 |\n",
    "| ShalePlay Delaware | [link]((https://www.eia.gov/maps/maps.htm))| EIA |Delaware play boundary (10/8/2019)  | 2023-11-21 |\n",
    "| AboYeso GlorietaYeso Spraberry | [link]((https://www.eia.gov/maps/maps.htm))| EIA | Abo-Yeso, Glorieta-Yeso, and Spraberry play boundaries (3/11/2016) | 2023-11-21 |\n",
    "| NM SLO OilGas Leases | [link](https://www.nmstatelands.org/maps-gis/gis-data-download/)| New Mexico State Land Office | Active Oil and Gas Leases (11/07/2023) | 2023-11-21 |\n",
    "| NM SLO Geologic Regions | [link]((https://www.nmstatelands.org/maps-gis/gis-data-download/))| New Mexico State Land Office | Geologic Regions (01/04/2010) | 2023-11-21 |\n",
    "| NM SLO STL Status Combined | [link]((https://www.nmstatelands.org/maps-gis/gis-data-download/))| New Mexico State Land Office | New Mexico State Trust Lands By Subdivision (04/14/2022) | 2023-11-21 |\n",
    "| All Layers By County | [link](https://rrc.texas.gov/resource-center/research/data-sets-available-for-download/)  | Railroad Commission of Texas | Map & Associated Data: Base Map, Wells, Surveys & Pipelines layers | 2023-11-17 |\n",
    "| Oil & Gas Leases | [link](https://www.glo.texas.gov/land/land-management/gis/index.html) | Texas General Land Office | Active Leases (11/17/2023) | 2023-11-17 |\n",
    "| Oil & Gas Units | [link](https://www.glo.texas.gov/land/land-management/gis/index.html) | Texas General Land Office | Active Units (11/17/2023) | 2023-11-17 |\n",
    "| U.S. County Boundaries | [link](https://www2.census.gov/geo/tiger/TIGER2022/COUNTY/tl_2022_us_county.zip) | United States Census Bureau | County (2022-10-31). Data is downloaded directly in the code. | N/A |\n",
    "| U.S. County FIPS Codes | [link](https://en.wikipedia.org/wiki/List_of_United_States_FIPS_codes_by_county) | Wikipedia | List of United States FIPS codes by county. Data is downloaded directly in the code. | N/A |\n",
    "\n",
    "Each row in the table represents a different dataset. The columns are:\n",
    "\n",
    "- **Dataset Name**: The name of the dataset.\n",
    "- **Source URL**: The URL where the dataset can be downloaded. Click on \"link\" to access the webpage.\n",
    "- **Original Source**: The original source of the data.\n",
    "- **Description**: A brief description of the dataset.\n",
    "- **Date Downloaded**: The date when the dataset was downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "# This cell generates lists of URLs to CSV files stored in a Google Cloud Storage bucket.\n",
    "# The CSV files contain data from the FracFocus Chemical Disclosure Registry.\n",
    "\n",
    "# Generate a list of URLs to the FracFocusRegistry CSV files.\n",
    "# There are 24 files in total, named FracFocusRegistry_i.csv where i ranges from 1 to 24.\n",
    "DATA_URLS1 = [\n",
    "    f\"https://storage.googleapis.com/mrprime_dataset/fracfocus/FracFocusRegistry_{i}.csv\"\n",
    "    for i in range(1, 25)\n",
    "]\n",
    "\n",
    "# Generate a list of URLs to the registryupload CSV files.\n",
    "# There are 3 files in total, named registryupload_i.csv where i ranges from 1 to 3.\n",
    "DATA_URLS2 = [\n",
    "    f\"https://storage.googleapis.com/mrprime_dataset/fracfocus/registryupload_{j}.csv\"\n",
    "    for j in range(1, 4)\n",
    "]\n",
    "\n",
    "# URL to the readme.txt file in the bucket.\n",
    "DATA_README_URL = [\n",
    "    \"https://storage.googleapis.com/mrprime_dataset/fracfocus/readme.txt\"\n",
    "]\n",
    "\n",
    "# url to the OCC (Oklahoma) well data in th bucket\n",
    "OCC_PARQUET_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/occ/rbdms_wells.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Url for the shapefile for US counties from the Census Bureau's website.\n",
    "CENSUS_COUNTY_MAP_URL = (\n",
    "    \"https://www2.census.gov/geo/tiger/TIGER2022/COUNTY/tl_2022_us_county.zip\"\n",
    ")\n",
    "# Url for a Wikipedia page containing a table of FIPS codes for US counties.\n",
    "FIPS_WIKI_URL = (\n",
    "    \"https://en.wikipedia.org/wiki/List_of_United_States_FIPS_codes_by_county\"\n",
    ")\n",
    "# Bounds of the continental US in longitude and latitude.\n",
    "USA_BOUNDS = (-124.77, 24.52, -66.95, 49.38)\n",
    "# bounds of the continental US in Web Mercator coordinates.\n",
    "USA_BOUNDS_MERCATOR = (-13874905.0, 2870341.0, -7453304.0, 6338219.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# url for the shapefiles of Permian Basin, Delaware Sub-Basin: Wolfcamp play boundary\n",
    "WOLFCAMP_ZIP_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/eia/Wolfcamp_Delaware_Play_Boundary.zip\"\n",
    "MIDLAND_ZIP_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/eia/Wolfcamp_Midland_Play_Boundaries_EIA.zip\"\n",
    "DELAWARE_ZIP_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/eia/ShalePlay_Delaware_EIA.zip\"\n",
    "ABOYESO_ZIP_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/eia/ShalePlays_AboYeso_GlorietaYeso_Spraberry_EIA.zip\"\n",
    "# PB_ZIP_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/eia/PermianBasin_Boundary_Structural_Tectonic.zip\"\n",
    "\n",
    "basins_url_list = [\n",
    "    WOLFCAMP_ZIP_URL,\n",
    "    MIDLAND_ZIP_URL,\n",
    "    DELAWARE_ZIP_URL,\n",
    "    ABOYESO_ZIP_URL,\n",
    "    # PB_ZIP_URL,\n",
    "]\n",
    "\n",
    "\n",
    "# url for shapefiles of Polygon data set intended to delineate active oil and gas leases on New Mexico State Trust Lands.\n",
    "NM_SLO_OIL_LEASE_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/nm_slo/OilGas_Leases.zip\"\n",
    "\n",
    "# url for shapefiles of Polygon layer created to highlight general boundaries of subsurface geologic basins and uplifts of New Mexico\n",
    "NM_SLO_GEO_REGION_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/nm_slo/slo_GeologicRegions.zip\"\n",
    "# url for shapefiles of Polygons of New Mexico State Trust Lands by PLSS subdivision (quarter-quarter, lot, tract, or partial).\n",
    "NM_SLO_STL_PLSS_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/nm_slo/slo_STLStatusCombined.zip\"\n",
    "\n",
    "nm_slo_url_list = [\n",
    "    NM_SLO_OIL_LEASE_URL,\n",
    "    NM_SLO_GEO_REGION_URL,\n",
    "]  # , NM_SLO_STL_PLSS_URL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Define a list of county numbers that we want to test. These numbers correspond to counties\n",
    "# that we did not include in the data folder, but they do not cover all 254 counties.\n",
    "\n",
    "# county numbers are only odd numbers\n",
    "county_nums = [str(i).zfill(3) for i in range(1, 508) if i % 2]\n",
    "\n",
    "# Generate a list of URLs to the shapefile zip files stored in a Google Cloud Storage bucket.\n",
    "# The zip files are named Shp{num}.zip, where {num} is a county number from the county_nums list.\n",
    "SHP_ZIP_URLS = [\n",
    "    f\"https://storage.googleapis.com/mrprime_dataset/capstone_journey/rrc/all_layers_rrc_20231117/Shp{num}.zip\"\n",
    "    for num in county_nums\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# url for the active leases in Texas on State land gdb\n",
    "GDB_ZIP_URLS = [\n",
    "    \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/glo/GDB_ActiveLeases.zip\",\n",
    "    \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/glo/GDB_ActiveUnits.zip\",\n",
    "    # \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/glo/GDB_InactiveLeases.zip\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function definations\n",
    "Next, let's define some functions that will be used throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def read_csv_concurrent(urls_list):\n",
    "    \"\"\"Reads a list of CSV files concurrently\"\"\"\n",
    "    # Create a thread pool\n",
    "    with cf.ThreadPoolExecutor() as executor:\n",
    "        # Use map to apply pd.read_csv to each URL\n",
    "        results = list(tqdm(executor.map(pd.read_csv, urls_list), total=len(urls_list)))\n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def extract_specific_gdf_from_local_zip(\n",
    "    zip_paths: list[str], regex_patterns: list[str]\n",
    ") -> dict[str, gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Reads shapefiles from a list of zip files and returns a dictionary\n",
    "    where the keys are the names of the shapefiles and the values are GeoDataFrames.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store the GeoDataFrames\n",
    "    shp_dict = {}\n",
    "    # compile the regex patterns\n",
    "    patterns = [re.compile(pattern) for pattern in regex_patterns]\n",
    "\n",
    "    # Loop over the list of zip file paths\n",
    "    for zip_path in zip_paths:\n",
    "        # Open the zip file\n",
    "        with ZipFile(zip_path) as z:\n",
    "            # Get the list of files in the zip file\n",
    "            zip_contents = z.namelist()\n",
    "            # Filter the list to get only the shapefiles that match any of the patterns\n",
    "            shp_files = [\n",
    "                f\n",
    "                for f in zip_contents\n",
    "                for pattern in patterns\n",
    "                if pattern.search(f) and f.endswith(\".shp\")\n",
    "            ]\n",
    "            # read the shapefiles into GeoDataFrames\n",
    "            for shp_file in shp_files:\n",
    "                # Get the name of the shapefile\n",
    "                shp_name = Path(shp_file).stem\n",
    "                # Read the shapefile into a GeoDataFrame and add it to the dictionary\n",
    "                shp_dict[shp_name] = gpd.read_file(f\"zip://{zip_path}!{shp_file}\")\n",
    "    # Return the dictionary of GeoDataFrames\n",
    "    return shp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def extract_matching_shp_files_from_zip_urls(\n",
    "    zip_urls: list[str], regex_patterns: list[str]\n",
    ") -> dict[str, gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Reads shapefiles from a list of zip file urls and returns a dictionary\n",
    "    where the keys are the names of the shapefiles and the values are GeoDataFrames.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store the GeoDataFrames\n",
    "    shp_dict = {}\n",
    "    # compile the regex patterns\n",
    "    patterns = [re.compile(pattern) for pattern in regex_patterns]\n",
    "\n",
    "    # Loop over the list of zip file urls\n",
    "    for zip_url in tqdm(zip_urls, desc=\"Processing zip files\"):\n",
    "        # download the zip file\n",
    "        with urlopen(zip_url) as u:\n",
    "            zip_data = u.read()\n",
    "        # create a ZipMemoryFile from the zip data\n",
    "        with ZipMemoryFile(zip_data) as z:\n",
    "            # get the list of files in the zip file\n",
    "            zip_files = z.listdir()\n",
    "            # filter for shapefiles that match any of the patterns\n",
    "            shp_files = [\n",
    "                f\n",
    "                for f in zip_files\n",
    "                for pattern in patterns\n",
    "                if pattern.search(f) and f.endswith(\".shp\")\n",
    "            ]\n",
    "            # read the shapefiles into GeoDataFrames\n",
    "            for shp_file in shp_files:\n",
    "                with z.open(shp_file) as f:\n",
    "                    shp_dict[Path(shp_file).stem] = gpd.GeoDataFrame.from_features(\n",
    "                        f, crs=f.crs\n",
    "                    )\n",
    "    # Return the dictionary of GeoDataFrames\n",
    "    return shp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def process_zip_url(\n",
    "    zip_url: str, patterns: list[re.Pattern]\n",
    ") -> dict[str, gpd.GeoDataFrame]:\n",
    "    \"\"\"Downloads a zip file url and returns a dictionary of GeoDataFrames for shapefiles that match the patterns\"\"\"\n",
    "    shp_dict = {}\n",
    "    with urlopen(zip_url) as u:\n",
    "        zip_data = u.read()\n",
    "    with ZipMemoryFile(zip_data) as z:\n",
    "        zip_files = z.listdir()\n",
    "        shp_files = [\n",
    "            f\n",
    "            for f in zip_files\n",
    "            for pattern in patterns\n",
    "            if pattern.search(f) and f.endswith(\".shp\")\n",
    "        ]\n",
    "        for shp_file in shp_files:\n",
    "            with z.open(shp_file) as f:\n",
    "                shp_dict[Path(shp_file).stem] = gpd.GeoDataFrame.from_features(\n",
    "                    f, crs=f.crs\n",
    "                )\n",
    "    return shp_dict\n",
    "\n",
    "\n",
    "def extract_matching_shp_files_from_zip_urls_concurrent(\n",
    "    zip_urls: list[str], regex_patterns: list[str]\n",
    ") -> dict[str, gpd.GeoDataFrame]:\n",
    "    \"\"\"Reads shapefiles from a list of zip file urls and returns a dictionary\n",
    "    where the keys are the names of the shapefiles and the values are GeoDataFrames.\"\"\"\n",
    "    shp_dict = {}\n",
    "    patterns = [re.compile(pattern) for pattern in regex_patterns]\n",
    "    with cf.ThreadPoolExecutor() as executor:\n",
    "        future_to_url = {\n",
    "            executor.submit(process_zip_url, url, patterns): url for url in zip_urls\n",
    "        }\n",
    "        futures = tqdm(\n",
    "            cf.as_completed(future_to_url),\n",
    "            total=len(future_to_url),\n",
    "            desc=\"Processing URLs\",\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "        for future in futures:\n",
    "            shp_dict.update(future.result())\n",
    "    return shp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def concat_gdf_from_dict(gdf_dict: dict[str, gpd.GeoDataFrame]) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Given a dictionary of GeoDataFrames, returns a single GeoDataFrame\n",
    "    with a new column indicating the source of the data.\n",
    "    \"\"\"\n",
    "    # use a dictionary comprehension to create a new dictionary\n",
    "    gdf_data = {k: gdf.assign(source_file=k) for k, gdf in gdf_dict.items()}\n",
    "    # return the concatenated GeoDataFrame\n",
    "    return pd.concat(gdf_data.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def extract_gdfs_from_zip(zip_path: str) -> Optional[dict[str, gpd.GeoDataFrame]]:\n",
    "    \"\"\"\n",
    "    Reads shapefiles from a zip file and returns a dictionary of GeoDataFrames.\n",
    "    \"\"\"\n",
    "    gdfs = {}\n",
    "    # Open the zip file\n",
    "    with ZipFile(zip_path) as z:\n",
    "        # Get the list of files in the zip file\n",
    "        zip_contents = z.namelist()\n",
    "        # Find the shapefiles\n",
    "        shp_files = [f for f in zip_contents if f.endswith(\".shp\")]\n",
    "        for shp_file in shp_files:\n",
    "            # Read the shapefile into a GeoDataFrame\n",
    "            gdf = gpd.read_file(f\"zip://{zip_path}!{shp_file}\")\n",
    "            gdfs[shp_file] = gdf\n",
    "\n",
    "    # If no shapefile was found, return None\n",
    "    return gdfs if gdfs else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def extract_gdfs_from_zip_url(zip_url: str) -> Optional[dict[str, gpd.GeoDataFrame]]:\n",
    "    \"\"\"\n",
    "    Downloads a ZIP file from a URL, reads shapefiles from the ZIP file, and returns a dictionary of GeoDataFrames.\n",
    "    \"\"\"\n",
    "    gdfs = {}\n",
    "    # Open the URL\n",
    "    with urlopen(zip_url) as u:\n",
    "        # Read the content of the response into a byte stream\n",
    "        zip_data = u.read()\n",
    "        # Open the ZIP file from the byte stream\n",
    "        with ZipMemoryFile(zip_data) as z:\n",
    "            # Get the list of files in the ZIP file\n",
    "            zip_contents = z.listdir()\n",
    "            # Find the shapefiles\n",
    "            shp_files = [f for f in zip_contents if f.endswith(\".shp\")]\n",
    "            for shp_file in shp_files:\n",
    "                # Read the shapefile into a GeoDataFrame\n",
    "                with z.open(shp_file) as f:\n",
    "                    gdf = gpd.GeoDataFrame.from_features(f, crs=f.crs)\n",
    "                gdfs[Path(shp_file).stem] = gdf\n",
    "\n",
    "    # If no shapefile was found, return None\n",
    "    return gdfs if gdfs else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def process_shp_url(zip_url: str):\n",
    "    \"\"\"Downloads a zip file url and returns a dictionary of GeoDataFrames for shapefiles that match the patterns\"\"\"\n",
    "    shp_dict = {}\n",
    "    with urlopen(zip_url) as u:\n",
    "        zip_data = u.read()\n",
    "    with ZipMemoryFile(zip_data) as z:\n",
    "        zip_files = z.listdir()\n",
    "        shp_files = [f for f in zip_files if f.endswith(\".shp\")]\n",
    "        for shp_file in shp_files:\n",
    "            with z.open(shp_file) as f:\n",
    "                shp_dict[Path(shp_file).stem] = gpd.GeoDataFrame.from_features(\n",
    "                    f, crs=f.crs\n",
    "                )\n",
    "    return shp_dict\n",
    "\n",
    "\n",
    "def extract_gdfs_from_zip_url_concurrent(\n",
    "    zip_urls: list[str],\n",
    ") -> dict[str, gpd.GeoDataFrame]:\n",
    "    \"\"\"Reads shapefiles from a list of zip file urls and returns a dictionary\n",
    "    where the keys are the names of the shapefiles and the values are GeoDataFrames.\"\"\"\n",
    "    shp_dict = {}\n",
    "    with cf.ThreadPoolExecutor() as executor:\n",
    "        future_to_url = {executor.submit(process_shp_url, url): url for url in zip_urls}\n",
    "        futures = tqdm(\n",
    "            cf.as_completed(future_to_url),\n",
    "            total=len(future_to_url),\n",
    "            desc=\"Processing URLs\",\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "        for future in futures:\n",
    "            shp_dict.update(future.result())\n",
    "    return shp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def read_gdb_from_zip(gdb_zips_list: list[str]):\n",
    "    \"\"\"Reads a list of zip files containing geodatabases and returns a dictionary of GeoDataFrames\"\"\"\n",
    "    # initialize an empty dictionary\n",
    "    gdb_dict = {}\n",
    "    # loop through each zip file\n",
    "    for gdb_zip in gdb_zips_list:\n",
    "        with ZipFile(gdb_zip, \"r\") as z:\n",
    "            # get list of files in zip\n",
    "            files = z.namelist()\n",
    "            # filter for gdb folders\n",
    "            gdb_folders = [f for f in files if f.endswith(\".gdb/\")]\n",
    "            # if there is a gdb folder in the zip file\n",
    "            if gdb_folders:\n",
    "                # get it and read it into a GeoDataFrame\n",
    "                gdb_folder = gdb_folders[0]\n",
    "                gdb_dict[Path(gdb_folder).stem] = gpd.read_file(\n",
    "                    f\"zip://{gdb_zip}!{gdb_folder}\"\n",
    "                ).to_crs(\"EPSG:4269\")\n",
    "    # return the dictionary of GeoDataFrames\n",
    "    return gdb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def read_gdb_from_zip_url(gdb_urls_list: list[str]):\n",
    "    \"\"\"Reads a list of zip file urls containing geodatabases and returns a dictionary of GeoDataFrames\"\"\"\n",
    "    # initialize an empty dictionary\n",
    "    gdb_dict = {}\n",
    "    # loop through each zip file\n",
    "    for gdb_url in gdb_urls_list:\n",
    "        # create a temporary directory\n",
    "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "            # download the zip file\n",
    "            with urlopen(gdb_url) as u, open(f\"{tmp_dir}/data.zip\", \"wb\") as f_out:\n",
    "                f_out.write(u.read())\n",
    "            # extract the zip file\n",
    "            with ZipFile(f\"{tmp_dir}/data.zip\", \"r\") as zip_ref:\n",
    "                zip_ref.extractall(tmp_dir)\n",
    "            # get the list of extracted files\n",
    "            extracted_files = list(Path(tmp_dir).iterdir())\n",
    "            # filter for gdb folders\n",
    "            gdb_folders = [f for f in extracted_files if f.suffix == \".gdb\"]\n",
    "            # if there is a gdb folder in the extracted files\n",
    "            if gdb_folders:\n",
    "                # get it and read it into a GeoDataFrame\n",
    "                gdb_folder = gdb_folders[0]\n",
    "                gdb_dict[Path(gdb_folder).stem] = gpd.read_file(gdb_folder).to_crs(\n",
    "                    \"EPSG:4269\"\n",
    "                )\n",
    "    # return the dictionary of GeoDataFrames\n",
    "    return gdb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def process_gdb_url(gdb_url):\n",
    "    \"\"\"Downloads a zip file url containing a geodatabase and returns a GeoDataFrame\"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        # download the zip file\n",
    "        with urlopen(gdb_url) as u, open(f\"{tmp_dir}/data.zip\", \"wb\") as f_out:\n",
    "            f_out.write(u.read())\n",
    "        # extract the zip file\n",
    "        with ZipFile(f\"{tmp_dir}/data.zip\", \"r\") as zip_ref:\n",
    "            zip_ref.extractall(tmp_dir)\n",
    "        # get the list of extracted files\n",
    "        extracted_files = list(Path(tmp_dir).iterdir())\n",
    "        # filter for gdb folders\n",
    "        gdb_folders = [f for f in extracted_files if f.suffix == \".gdb\"]\n",
    "        # if there is a gdb folder in the extracted files\n",
    "        if gdb_folders:\n",
    "            # get it and read it into a GeoDataFrame\n",
    "            gdb_folder = gdb_folders[0]\n",
    "            return Path(gdb_folder).stem, gpd.read_file(gdb_folder)\n",
    "\n",
    "\n",
    "def read_gdb_from_zip_url_concurrent(gdb_urls_list: list[str]):\n",
    "    \"\"\"Reads a list of zip file urls containing geodatabases and returns a dictionary of GeoDataFrames\"\"\"\n",
    "    # initialize an empty dictionary\n",
    "    gdb_dict = {}\n",
    "    # create a ThreadPoolExecutor\n",
    "    with cf.ThreadPoolExecutor() as executor:\n",
    "        # submit the process_gdb_url function for each url and gather the results\n",
    "        future_to_url = {\n",
    "            executor.submit(process_gdb_url, url): url for url in gdb_urls_list\n",
    "        }\n",
    "        for future in cf.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                key, data = future.result()\n",
    "                gdb_dict[key] = data\n",
    "            except Exception as exc:\n",
    "                print(f\"{url} generated an exception: {exc}\")\n",
    "    # return the dictionary of GeoDataFrames\n",
    "    return gdb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "def pascal_to_snake(name: str):\n",
    "    \"\"\"Converts a string from PascalCase to snake_case\"\"\"\n",
    "    # (?<=[A-Za-z0-9]) - positive lookbehind for any alphanumeric character\n",
    "    # (?=[A-Z][a-z]) - positive lookahead for any uppercase followed by lowercase\n",
    "    pattern = re.compile(r\"(?<=[A-Za-z0-9])(?=[A-Z][a-z])\")\n",
    "    name = pattern.sub(\"_\", name).lower()\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def unify_crs(\n",
    "    dataframe: pd.DataFrame,\n",
    "    lon_col: str = \"longitude\",\n",
    "    lat_col: str = \"latitude\",\n",
    "    crs_col: str = \"crs\",\n",
    "    final_crs: str = \"EPSG:4269\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a DataFrame with lon/lat or x/y coordinates,\n",
    "    converts the coordinates to a unified crs and combines\n",
    "    into a single GeoDataframe with a geometry column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the main columns that will be used for the conversion\n",
    "    main_cols = [lon_col, lat_col, crs_col]\n",
    "\n",
    "    # Get the other columns in the dataframe\n",
    "    other_cols = list(set(dataframe.columns) - set(main_cols))\n",
    "\n",
    "    # Create a subframe with only the main columns\n",
    "    subframe = dataframe[main_cols]\n",
    "\n",
    "    # Create a list of GeoDataFrames, each with a different CRS\n",
    "    geo_dfs = [\n",
    "        gpd.GeoDataFrame(\n",
    "            # Use the data for this CRS\n",
    "            data=data,\n",
    "            # Create a geometry column from the lon/lat columns\n",
    "            geometry=gpd.points_from_xy(x=data[lon_col].values, y=data[lat_col].values),\n",
    "            # Set the CRS for this GeoDataFrame\n",
    "            crs=pyproj.CRS(crs_val),\n",
    "            # Convert the GeoDataFrame to the final CRS\n",
    "        ).to_crs(final_crs)\n",
    "        # Do this for each unique CRS in the subframe\n",
    "        for crs_val, data in subframe.groupby(crs_col)\n",
    "    ]\n",
    "\n",
    "    # Merge the GeoDataFrames back together and return the result\n",
    "    return pd.merge(\n",
    "        # Concatenate the GeoDataFrames\n",
    "        pd.concat(geo_dfs, sort=True),\n",
    "        # Add the other columns back in\n",
    "        dataframe[other_cols],\n",
    "        # Merge on the index\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# @lru_cache(maxsize=3)\n",
    "def get_background_map(bgcolor=\"black\", alpha=0.5):\n",
    "    \"\"\"Returns a GeoViews background map\"\"\"\n",
    "    return gts.CartoLight().opts(bgcolor=bgcolor, alpha=alpha)\n",
    "\n",
    "\n",
    "def platecaree_to_mercator_vectorised(x, y):\n",
    "    \"\"\"Use Cartopy to convert PlateCarree coordinates to Mercator\"\"\"\n",
    "    return ccrs.GOOGLE_MERCATOR.transform_points(ccrs.PlateCarree(), x, y)[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_in_000(num):\n",
    "    \"\"\"Formats a number in thousands\"\"\"\n",
    "    for unit in [\"\", \"thousand\", \"million\", \"billion\", \"trillion\"]:\n",
    "        if abs(num) < 1000.0:\n",
    "            return f\"{num:3.2f} {unit}\"\n",
    "        num /= 1000.0\n",
    "    return f\"{num:.2f} quadrillion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_datetime(df, column):\n",
    "    \"\"\"Splits a datetime column into year, month, and day columns\"\"\"\n",
    "    # remove '_date' from the column name\n",
    "    column_stem = column.replace(\"_date\", \"\") if \"_date\" in column else column\n",
    "    try:\n",
    "        datetime_series = pd.to_datetime(df[column], errors=\"coerce\")\n",
    "        if datetime_series.isna().any():\n",
    "            print(f\"Errors occurred during conversion of column {column}.\")\n",
    "        df[column_stem + \"_year\"] = datetime_series.dt.year\n",
    "        df[column_stem + \"_month\"] = datetime_series.dt.month\n",
    "        df[column_stem + \"_day\"] = datetime_series.dt.day\n",
    "    except KeyError:\n",
    "        print(f\"Column {column} not found in the DataFrame.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First dataset is from FracFocus. There is also a readme file which contains the data dictionary for the dataset. Let's have a look at both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readme file with data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# get readme data\n",
    "readme = urlopen(DATA_README_URL[0]).read().decode(\"windows-1252\")\n",
    "display(readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print function goes beyond 'hello world' and takes care of the escape characters\n",
    "print(readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# We can collect all the dataframe into a list and then concatenate them\n",
    "df_list = read_csv_concurrent(DATA_URLS2)\n",
    "\n",
    "\n",
    "dfs = pd.concat(df_list).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "registry_df = pd.DataFrame()\n",
    "registry_df = dfs.copy()\n",
    "registry_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the missing values it is interesting to see that most missing values are from the `TVD`, `TotalBaseWaterVolume` and `TotalBaseNonWaterVolume`. One reason for this may be found in the data limitations on terms of use on the FracFocus website. It states:\n",
    "-  Disclosures submitted using the FracFocus 1.0 format (January, 2011 to May 31, 2013) will contain only header data. \n",
    "-  Disclosures submitted using the FracFocus 2.0 format (November 2012 to present) will contain both header and chemical data. NOTE: Between November, 2012 and May 31, 2013 disclosures in both 1.0 and 2.0 formats were submitted to the system. \n",
    "-  After May 31, 2013 only disclosures submitted in the 2.0 format were accepted.\n",
    "-  Data submitted appears as it was submitted by the operator or operatorâ€™s authorized agent. FracFocus does not warrant the data in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of non-missing values in each column\n",
    "missing_data_percent = (registry_df.notna().mean() * 100).rename(\"Percent\")\n",
    "\n",
    "# Create a DataFrame of the counts of non-missing values\n",
    "non_missing_count = registry_df.notna().sum().rename(\"Count\")\n",
    "\n",
    "# Concatenate the two DataFrames along the columns\n",
    "non_missing_data = pd.concat([missing_data_percent, non_missing_count], axis=1)\n",
    "\n",
    "# Create a horizontal bar plot of the percentage of non-missing data\n",
    "hbar_plot = non_missing_data.hvplot.barh(\n",
    "    y=\"Percent\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    title=\"Percentage of Non-Missing Data in Each Column\",\n",
    "    ylabel=\"\",\n",
    "    xlabel=\"\",\n",
    "    xaxis=\"bare\",\n",
    "    hover_cols=\"all\",\n",
    ").opts(\n",
    "    active_tools=[\"box_zoom\"],\n",
    "    toolbar=\"above\",\n",
    ")\n",
    "\n",
    "hbar_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some of the rows of the dataframe\n",
    "display(registry_df.head(3))\n",
    "display(registry_df.sample(5, random_state=628))\n",
    "display(registry_df.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our first look at a few sample rows some things stick out immediately.\n",
    "1. The dataset may be in chronological order and the values of the `JobStartDate`/`JobEndDate` at both of the extremes may be incorrect.\n",
    "2. There may be an abundance for `StateNumber` `42` if 4 out of the 5 draws of the 200k+ rows drawn at random had a `StateNumber` of `42`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we jump into cleaning the data in the columns, let's make the columns look more pythonic by changing the column names to snake_case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "registry_df.columns = [pascal_to_snake(col) for col in registry_df.columns]\n",
    "registry_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can remove the columns with only null values. These are the last 2 columns in the dataframe, `source` and `dtmod`. Also we can drop the `total_non_base_water_volume` column since we may not have much need for it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "registry_df = registry_df.drop(\n",
    "    columns=[\"source\", \"dtmod\", \"total_base_non_water_volume\"]\n",
    ")\n",
    "registry_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will fix some of the dtypes of the columns.\n",
    "- Both the `job_start_date` and the `job_end_date` columns are object dtypes, so we will convert those to datetime dtypes and drop the timestamp.\n",
    "- We can also separate out the date components into its various components. This may come in handy for feature engineering later on.\n",
    "- The `projection` column is an object dtype. That can be converted to a string dtype and shorten to `crs` as it represents the Cooordinate Reference System used in the `latitude` and `longitude` columns values. We can dig into what CRS is later on.\n",
    "- The `federal_well` and `indian_well` columns are both boolean type columns. They may be more aptly named as `is_federal_well` and `is_indian_well` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function on 'job_start_date' and 'job_end_date'\n",
    "split_datetime(registry_df, \"job_start_date\")\n",
    "split_datetime(registry_df, \"job_end_date\")\n",
    "registry_df[[col for col in registry_df.columns if re.search(\"start|end\", col)]].info(\n",
    "    memory_usage=\"deep\"\n",
    ")\n",
    "# show the values which are null still\n",
    "registry_df[\n",
    "    registry_df[[col for col in registry_df.columns if re.search(\"start|end\", col)]]\n",
    "    .isna()\n",
    "    .any(axis=1)\n",
    "].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Convert 'job_start_date' to datetime format and format it as 'YYYY-MM-DD'\n",
    "registry_df[\"job_start_date\"] = pd.to_datetime(\n",
    "    registry_df[\"job_start_date\"], errors=\"coerce\"\n",
    ").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Convert 'job_end_date' to datetime format and format it as 'YYYY-MM-DD'\n",
    "registry_df[\"job_end_date\"] = pd.to_datetime(\n",
    "    registry_df[\"job_end_date\"], errors=\"coerce\"\n",
    ").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# drop rows with null values in 'job_start_date' and 'job_end_date'\n",
    "# registry_df = registry_df.dropna(subset=[\"job_start_date\", \"job_end_date\"])\n",
    "\n",
    "\n",
    "# Rename some columns for clarity\n",
    "registry_df.rename(\n",
    "    columns={\n",
    "        \"federal_well\": \"is_federal_well\",\n",
    "        \"indian_well\": \"is_indian_well\",\n",
    "        \"projection\": \"crs\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Display the information of the DataFrame\n",
    "registry_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will look at the `api_number` column.\n",
    "We learned from the read me that \n",
    "> APINumber - The American Petroleum Institute well identification number formatted as follows xx-xxx-xxxxx0000 Where: \n",
    "> - First two digits represent the state, \n",
    "> - second three digits represent the county, \n",
    "> - third 5 digits represent the well.\n",
    "\n",
    "Theoretically, we could just grab the first two characters of the `APINnumber` and use that as the state number according to the definition of the `APINumber` above. Actually, that would not be a good idea, and here is why.<br>\n",
    "Although the column was called `APINumber`, it is not actually a number, so if it starts with a leading `0` that first character `0`, cannot be omitted from the value. Let's look at some of the rows with a single digit state numbers.\n",
    "\n",
    "Right now, \n",
    "- the `api_number` column is an object dtype, but a better option would be a `string` dtype, as `object` dtype can be mixed . We can also shorten that column name to `api`.\n",
    "- the `state_number` column and the `county_number` column are both `int64` dtypes right now. `string` type may be a stronger option.\n",
    "- `state_code` and `county_code` may be better names for the `state_number` and `county_number` columns respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows where the state_number is a single digit\n",
    "registry_df[\n",
    "    (registry_df[\"state_number\"] == 3) | (registry_df[\"state_number\"] == 5)\n",
    "].sample(5, random_state=628)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rows' `api_number` values have leading `0`, which is correct, but some do not. The rows without the leading `0` though are 13 characters long instead of 14. Maybe we can just add a leading `0` where needed until all API number values are 14 characters long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of characters in the api_number column\n",
    "registry_df[\"api_number\"].astype(\"string\").str.len().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most are 14 characters long, but some are 13 characters long, like the ones we saw above without the leading `0`. Let's assume the ones with 13 characters are missing the leading `0` and not something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'api' to string and pad it with zeros to make it 14 characters long\n",
    "registry_df[\"api\"] = registry_df[\"api_number\"].astype(\"string\").str.zfill(14)\n",
    "\n",
    "# Convert 'state_number' to string and pad it with zeros to make it 2 characters long\n",
    "registry_df[\"state_code\"] = registry_df[\"state_number\"].astype(\"string\").str.zfill(2)\n",
    "\n",
    "# Convert 'county_number' to string and pad it with zeros to make it 3 characters long\n",
    "registry_df[\"county_code\"] = registry_df[\"county_number\"].astype(\"string\").str.zfill(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which rows may have the api with the first two digits not matching the state number\n",
    "api_state_mismatch_mask = registry_df[\"state_code\"] != registry_df[\"api\"].str[0:2]\n",
    "# api_state_mismatch_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which rows may have the api with the first two digits not matching the state number\n",
    "registry_df[api_state_mismatch_mask][\n",
    "    [\"api_number\", \"api\", \"state_code\", \"state_name\", \"county_code\", \"county_name\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expected to get 2 rows here, since we checked the length of the `api_number` column above we saw that 1 row had 10 and another row had 12 characters. It is only two rows, so this may be an easy fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Remove leading zeros and pad to 14 digits on mismatches\n",
    "registry_df.loc[api_state_mismatch_mask, \"api\"] = (\n",
    "    registry_df.loc[api_state_mismatch_mask, \"api\"].str.lstrip(\"0\").str.ljust(14, \"0\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which rows may have the api with the first two digits not matching the state number\n",
    "registry_df[api_state_mismatch_mask][\n",
    "    [\"api_number\", \"api\", \"state_code\", \"state_name\", \"county_code\", \"county_name\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# check which rows may have the api with the 3-5 digits not matching the county number\n",
    "api_county_mismatch_mask = registry_df[\"county_code\"] != registry_df[\"api\"].str[2:5]\n",
    "registry_df[api_county_mismatch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State name should not have more than 50 possible values, given that there are only 50 states in the US. If we were to check the number of unique values in the `state_name` column, we would see 95. This is due to the variation in the way the `state_name` value is entered. Although not as obvious, we can assume the same for the `county_name` column. Luckily, the `api` includes both the `state_number` and the `county_number`. With this we can do \n",
    "1. data validation ensuring that these corresponding columns match\n",
    "2. Ensure that the `state_name` and the `county_name` columns are correct. Important to note that \n",
    "> The state codes used in an API number are DIFFERENT from another standard which is the Federal Information Processing Standard (FIPS) state code established in 1987 by NIST. ([source](https://en.wikipedia.org/wiki/API_well_number#State_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'Number of different values in state_name column: {registry_df[\"state_name\"].nunique()}'\n",
    ")\n",
    "print(\n",
    "    f'Number of different values in state_number column: {registry_df[\"state_number\"].nunique()}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by state_code and find the mode of the state_name\n",
    "state_code_mode = (\n",
    "    registry_df.groupby(\"state_code\")[\"state_name\"]\n",
    "    .apply(lambda x: x.mode().iloc[0])\n",
    "    .reset_index()\n",
    ")\n",
    "state_code_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "registry_df = registry_df.merge(state_code_mode.rename(columns={\"state_name\": \"state\"}))\n",
    "registry_df.sample(3, random_state=628)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus our efforts in the most recent 10 years. Although more data is usually better, data too far in the past may distract whatever model we may build since unconventional drilling practices have really taken over the industry. We will also put our focus in one specific area, the Permian Basin. The Permian Basin has been instrumental in the shale boom transformation and is the most active area of exploration and production in the US presently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# create mask for from 2013 onwards\n",
    "post_2012_mask = registry_df[\"job_start_date\"] >= \"2013-01-01\"\n",
    "registry_df_post_2012 = registry_df[post_2012_mask].copy()\n",
    "\n",
    "# find all the rows with null values\n",
    "null_mask = registry_df_post_2012.isna().any(axis=1)\n",
    "registry_df_post_2012[null_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how many nans we still have in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_df_post_2012.info(memory_usage=\"deep\")\n",
    "registry_df_post_2012.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the `county_number` for the rows with a nan value in the `county_name` column, we can see why there is a nan for the `county_name`. Those numbers are most likely incorrect as small states like `North Dakota` and `Arkansas` do not have large `county_number` values. However we can still try to impute what the correct values by cross referencing with other sources or by using the `latitude` and `longitude` values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the rows with a null value for the county_name column\n",
    "registry_df_post_2012[registry_df_post_2012[\"county_name\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index of one of the rows with a null value for the county_name column (3rd one down)\n",
    "index_vanorsdale = registry_df_post_2012.query(\"api_number == '03729439000000'\").index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oklahoma Commission Corporation (OOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some search engine investigating, we can learn that WFD Oil Corporation is a PC in Oklahoma. We also learn that the well name is `VANORSDOL` ,the well number is `#1-29`, and the API number is `3503729439` `0000`. We can correct some of the data which was entered incorrectly in FracFocus.\n",
    "\n",
    "The data are looking for is in this markdown cell so we can manually input it in but we will do it through code instead. We will query the api number in the data we have on the wells in OK.\n",
    "\n",
    "| Column Name | Value |\n",
    "| --- | --- |\n",
    "API\t|3503729439\n",
    "WELL_NAME|\tVANORSDOL\n",
    "WELL_NUM|\t#1-29\n",
    "OPERATOR|\tWFD OIL CORPORATION\n",
    "WELLSTATUS|\tAC\n",
    "WELLTYPE|\tOIL\n",
    "SH_LAT\t|35.749381\n",
    "SH_LON\t|-96.370355\n",
    "COUNTY\t|CREEK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When reading the Parquet file\n",
    "occ_wells = pd.read_parquet(OCC_PARQUET_URL)\n",
    "# Convert the WKT column back to a geometry column\n",
    "occ_wells[\"geometry\"] = occ_wells[\"geometry\"].apply(lambda x: wkt.loads(x))\n",
    "\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame, specifying the CRS\n",
    "occ_wells = gpd.GeoDataFrame(\n",
    "    occ_wells, geometry=\"geometry\", crs=occ_wells[\"crs\"].iloc[0]\n",
    ")\n",
    "occ_wells.info()\n",
    "# look at 1 sample row of the dataframe\n",
    "occ_wells.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'api' column to string\n",
    "occ_wells[\"api\"] = occ_wells[\"api\"].astype(\"int64\").astype(\"string\")\n",
    "occ_wells.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the well_name column\n",
    "registry_df_post_2012[\"well\"] = registry_df_post_2012[\"well_name\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_df_post_2012[registry_df_post_2012[\"api\"].str.contains(\"11533124\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the well_name column for 'vanors\n",
    "# occ_wells[occ_wells[\"well_name\"].str.contains(\"vanors\", case=False, na=False)]\n",
    "vanorsdol_row = occ_wells.query(\n",
    "    'well_name.fillna(\"\").str.contains(\"vanors\", case=False) & (api == \"3503729439\")',\n",
    ")[\n",
    "    [\n",
    "        \"api\",\n",
    "        \"well_name\",\n",
    "        \"well_num\",\n",
    "        \"operator\",\n",
    "        \"sh_lat\",\n",
    "        \"sh_lon\",\n",
    "        \"county\",\n",
    "    ]\n",
    "].rename(\n",
    "    columns={\"sh_lat\": \"latitude\", \"sh_lon\": \"longitude\"}\n",
    ")\n",
    "vanorsdol_row[\"well\"] = (\n",
    "    vanorsdol_row[\"well_name\"].str.title()\n",
    "    + \" \"\n",
    "    + vanorsdol_row[\"well_num\"].astype(\"string\")\n",
    ")\n",
    "index_vanorsdol = vanorsdol_row.index\n",
    "\n",
    "columns_to_replace = [\"api\", \"well\", \"latitude\", \"longitude\"]\n",
    "for col in columns_to_replace:\n",
    "    registry_df_post_2012.loc[index_vanorsdale, col] = vanorsdol_row.loc[\n",
    "        index_vanorsdol, col\n",
    "    ].values\n",
    "\n",
    "# check that the values have been replaced\n",
    "registry_df_post_2012.loc[index_vanorsdale, columns_to_replace]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the api column to 14 characters again\n",
    "registry_df_post_2012[\"api\"] = registry_df_post_2012[\"api\"].str.ljust(14, \"0\")\n",
    "\n",
    "registry_df_post_2012[registry_df_post_2012[\"county_name\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with `latitude` and `longitude` coordinates for all 4 rows with missing `county_name`, let's find out which counties they belong to spatially.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geodataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the boundary coordinates for all the counties in the US from the [census.gov](https://www.census.gov/) website. We saved the URL for this as `CENSUS_COUNTY_MAP_URL` at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# read in the US counties map data using geopandas\n",
    "county = gpd.read_file(CENSUS_COUNTY_MAP_URL)[\n",
    "    [\"GEOID\", \"STATEFP\", \"COUNTYFP\", \"NAME\", \"geometry\"]\n",
    "]\n",
    "county.columns = county.columns.str.lower()\n",
    "county.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will scrape the FIPS table from wikipedia since the county dataframe does not have the state name and merge the 2 tables just for convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "fips_df = pd.read_html(FIPS_WIKI_URL)[1]\n",
    "fips_df.columns = [\"geoid\", \"county\", \"state\"]\n",
    "fips_df[\"geoid\"] = fips_df[\"geoid\"].astype(\"string\").str.zfill(5)\n",
    "fips_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "county_fips_gdf = county.merge(fips_df, on=\"geoid\")\n",
    "county_fips_gdf.sample(3, random_state=628)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick note about `GeoDataFrames`: they must have a column called `geometry` and this column contains the geometric objects. This call is what enables `geopandas` to perform spatial operations, and can also contain certain attributes like `.crs` which is the coordinate reference system.\n",
    "\n",
    "\n",
    "Commonly used datums in North America are NAD27, NAD83, and WGS84. More info [here](https://webhelp.esri.com/arcgisdesktop/9.3/index.cfm?TopicName=Projection_basics_the_GIS_professional_needs_to_know).<br>\n",
    "\n",
    "The county geodataframe uses `EPSG:4269` which is the EPSG code for the NAD83 coordinate system. Let's create a geodataframe with the `latitude` and `longitude` values that we have and put all of the points to the same CRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_fips_gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# ensures each row of the geodataframe is in the same CRS\n",
    "registry_gdf = unify_crs(registry_df_post_2012, crs_col=\"crs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now perform a spatial join on the 2 GeoDataFrame.\n",
    "# fitler for rows with null county_name\n",
    "\n",
    "joined_gdf = (\n",
    "    registry_gdf[registry_gdf[\"county_name\"].isna()]\n",
    "    .sjoin(county_fips_gdf.drop(columns=[\"county\"]), how=\"left\", predicate=\"intersects\")\n",
    "    .drop(columns=[\"index_right\"])\n",
    ")\n",
    "joined_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `North Dakota` `county_number` should be `061`, which is `Mountrail` county, not `610`. \n",
    "- The `Utah` `county_number` though was actually correct. The error was the state number which should have been `42`, not `43`. This error is somewhat significant as according to the data dictionary:\n",
    "> APINumber - The American Petroleum Institute well identification number formatted as follows xx-xxx-xxxxx0000 Where: First two digits \n",
    "represent the state, second three digits represent the county, third 5 digits represent the well.<br>\n",
    "\n",
    "All this means is the `api` number is also incorrect. It should be `42317428660000` (<u><b>42</b></u>-317-42866-0000) instead of `43317428660000` (<u><b>43</b></u>-317-42866-0000).<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's corect theos values putting the county_code and\n",
    "registry_gdf[\"county\"] = registry_gdf[\"county_name\"].copy()\n",
    "\n",
    "# replace the county_code and county columns with the values from the joined_gdf\n",
    "registry_gdf.loc[joined_gdf.index, \"county\"] = joined_gdf[\"name\"]\n",
    "registry_gdf.loc[joined_gdf.index, \"county_code\"] = joined_gdf[\"countyfp\"]\n",
    "\n",
    "# change the api column of the last row in joined_gdf to 42317428660000 instead of 43317428660000\n",
    "# registry_gdf.loc[joined_gdf.index[-1], \"api\"] = \"42317428660000\"\n",
    "registry_gdf[\"api\"] = registry_gdf[\"api\"].replace(\"43317428660000\", \"42317428660000\")\n",
    "# correct the state_code values for where the api was changed\n",
    "registry_gdf[\"state_code\"] = registry_gdf[\"api\"].str[0:2]\n",
    "# Create a mapping from 'state_code' to 'state'\n",
    "state_mapping = state_code_mode.set_index(\"state_code\")[\"state\"].to_dict()\n",
    "\n",
    "# Use the mapping to update the 'state' column in 'registry_gdf'\n",
    "registry_gdf[\"state\"] = registry_gdf[\"state_code\"].map(state_mapping)\n",
    "\n",
    "\n",
    "# check that the values have been replaced\n",
    "registry_gdf[registry_gdf[\"county_name\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_gdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to impute the missing `county_name` values would have been by using the `well_name` in the `registry_df_post_2012` dataframe. Assuming the other wells on the same pad has the correct `state_code` and `state` values. However, this may not have worked with `Vanorsdol` as there's only one well with that name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_df_post_2012[\n",
    "    registry_df_post_2012[\"well_name\"].str.contains(\"vanors\", case=False, na=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show other wells with a similar name to rhea 1-6\n",
    "registry_df_post_2012[\n",
    "    registry_df_post_2012[\"well\"].str.contains(\"Rhea 1-6\", case=False, na=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_df_post_2012[registry_df_post_2012[\"well\"].str.contains(\"Trulson\", case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# county_fips_gdf[county_fips_gdf[\"state\"].isin(permian_states)]\n",
    "registry_df_post_2012[registry_df_post_2012[\"well\"].str.contains(\"palermo\", case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the well column uppercase to lower variation among for wells on the same pad\n",
    "registry_gdf[\"well\"] = registry_gdf[\"well\"].str.upper()\n",
    "# make the operator column uppercase to lower the variation among entry for the same operator\n",
    "registry_gdf[\"operator\"] = registry_gdf[\"operator_name\"].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_gdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we did for those 4 wells to find the `county` and `state` for which they belong to, we can do that for all the wells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2 new columns, lat and lon in the registry_gdf for corrections\n",
    "registry_gdf[\"lat\"] = registry_gdf[\"latitude\"].copy()\n",
    "registry_gdf[\"lon\"] = registry_gdf[\"longitude\"].copy()\n",
    "\n",
    "# Check which other wells may have a state value which did not agree with the spatial join result\n",
    "registry_county_gdf = registry_gdf.sjoin(\n",
    "    county_fips_gdf, how=\"left\", predicate=\"intersects\"\n",
    ").drop(columns=[\"index_right\"])\n",
    "\n",
    "trimmed_column_set = [\n",
    "    \"api\",\n",
    "    \"well\",\n",
    "    \"state_left\",\n",
    "    \"state_right\",\n",
    "    \"county_left\",\n",
    "    \"county_right\",\n",
    "    \"county_code\",\n",
    "    \"operator\",\n",
    "    \"lon\",\n",
    "    \"lat\",\n",
    "    \"geometry\",\n",
    "]\n",
    "\n",
    "# rows whichthe spatial join did not match for the both dataframes\n",
    "# mismatch_geo = registry_county_gdf[registry_county_gdf[\"state_right\"].isna()]\n",
    "mismatch_geo = registry_county_gdf[\n",
    "    registry_county_gdf[\"state_left\"] != registry_county_gdf[\"state_right\"]\n",
    "]\n",
    "print(f\"Number of rows: {len(mismatch_geo)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ones with a nan in the `state_right` column are those that we could not find a match for based on the `geometry`. We can match those for OK using the `api` number and see if the coordinates match for the wells in FracFocus match those from the OCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the rows with Oklahoma in the state_left column\n",
    "mismatch_geo_ok = mismatch_geo.query('state_left.str.contains(\"Oklahoma\")')\n",
    "print(f\"Number of rows from OK state: {len(mismatch_geo_ok)}\")\n",
    "mismatch_geo_ok[trimmed_column_set].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the original index in a new column\n",
    "mismatch_geo_ok[\"original_index\"] = mismatch_geo_ok.index\n",
    "# alter api to match the format in the occ_wells dataframe\n",
    "mismatch_geo_ok[\"ok_api\"] = mismatch_geo_ok[\"api\"].str[:10]  # ok for Oklahoma\n",
    "# drop the api column\n",
    "mismatch_geo_ok.drop(columns=[\"api\"], inplace=True)\n",
    "# rename the api column in occ_wells to ok_api\n",
    "occ_wells.rename(columns={\"api\": \"ok_api\"}, inplace=True)\n",
    "# look for the api in the occ_wells dataframe and merge that row to mismatch_geo_ok\n",
    "joined_mismatch_ok = mismatch_geo_ok.merge(\n",
    "    occ_wells[\n",
    "        [\"ok_api\", \"well_name\", \"well_num\", \"operator\", \"sh_lat\", \"sh_lon\", \"county\"]\n",
    "    ],\n",
    "    how=\"left\",\n",
    "    on=\"ok_api\",\n",
    ")\n",
    "joined_mismatch_ok[[\"geometry\", \"latitude\", \"longitude\", \"sh_lat\", \"sh_lon\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the lat and lon values with the sh_lat and sh_lon values\n",
    "# Replace the 'lat' and 'lon' values\n",
    "joined_mismatch_ok.loc[\n",
    "    joined_mismatch_ok[\"sh_lat\"].notna(), \"lat\"\n",
    "] = joined_mismatch_ok[\"sh_lat\"]\n",
    "joined_mismatch_ok.loc[\n",
    "    joined_mismatch_ok[\"sh_lon\"].notna(), \"lon\"\n",
    "] = joined_mismatch_ok[\"sh_lon\"]\n",
    "\n",
    "joined_mismatch_ok.set_index(\"original_index\", inplace=True)\n",
    "\n",
    "# occ_wells[occ_wells[\"api\"].isin(mismatch_geo_ok[\"ok_api\"])][\n",
    "#     [\"api\", \"well_name\", \"well_num\", \"operator\", \"sh_lat\", \"sh_lon\", \"county\"]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update 'lat' and 'lon' in the original DataFrame\n",
    "registry_gdf.loc[joined_mismatch_ok.index, \"lat\"] = joined_mismatch_ok[\"lat\"]\n",
    "registry_gdf.loc[joined_mismatch_ok.index, \"lon\"] = joined_mismatch_ok[\"lon\"]\n",
    "# registry_gdf\n",
    "\n",
    "\n",
    "# Update 'geometry' in the original DataFrame\n",
    "registry_gdf.loc[joined_mismatch_ok.index, \"geometry\"] = [\n",
    "    Point(xy) for xy in zip(joined_mismatch_ok.lon, joined_mismatch_ok.lat)\n",
    "]\n",
    "# Check which other wells may have a state value which did not agree with the spatial join result again\n",
    "registry_county_gdf = registry_gdf.sjoin(\n",
    "    county_fips_gdf, how=\"left\", predicate=\"intersects\"\n",
    ").drop(columns=[\"index_right\"])\n",
    "\n",
    "# mismatch_geo = registry_county_gdf[registry_county_gdf[\"state_right\"].isna()]\n",
    "\n",
    "mismatch_geo = registry_county_gdf[\n",
    "    registry_county_gdf[\"state_left\"] != registry_county_gdf[\"state_right\"]\n",
    "]\n",
    "print(f\"Number of rows with mismatched state values: {mismatch_geo.shape[0]}\")\n",
    "# mismatch_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query for the wells in Texas\n",
    "mismatch_geo_tx = mismatch_geo[trimmed_column_set].query(\n",
    "    'state_left.str.contains(\"Texas\")'\n",
    ")\n",
    "# store the original index in a new column\n",
    "mismatch_geo_tx[\"original_index\"] = mismatch_geo_tx.index\n",
    "\n",
    "mismatch_geo_tx[\"tx_api\"] = mismatch_geo_tx[\"api\"].str[2:10]\n",
    "# mismatch_geo_tx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texas Railroad Commission (RRC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Land survey data, bottom well data, and surface well data were taken from the RRC website. The data was then uploaded to GCP for easier reliabiliity. They are 254 zipfiles, one for each county, in the state of Texas. Each of those zipfiles contained various file extensions, and spatial data format that is usually contained in shapefiles, and contained info for various categories ranging from Airport lines to Offshore survey polys.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex patterns to identify which shapefiles to extract\n",
    "# we are grabbing the survey lines, surface wells, and bottom well locations shp files\n",
    "patterns = [r\"surv\\d{3}p\", r\"well\\d{3}s\", r\"well\\d{3}b\"]\n",
    "\n",
    "# Look at the survey lines polygons and the surface wells points. Data saved from RRC website\n",
    "shp_dict = extract_matching_shp_files_from_zip_urls_concurrent(SHP_ZIP_URLS, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def deep_getsizeof(obj):\n",
    "    \"\"\"Recursively find size of object and its elements\"\"\"\n",
    "    size = sys.getsizeof(obj)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum(deep_getsizeof(k) + deep_getsizeof(v) for k, v in obj.items())\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        size += sum(deep_getsizeof(x) for x in obj)\n",
    "    return size\n",
    "\n",
    "\n",
    "# Assume 'my_dict' is your dictionary\n",
    "total_size_in_bytes = deep_getsizeof(shp_dict)\n",
    "total_size_in_mb = total_size_in_bytes / 1e6\n",
    "print(f\"The total size of the dictionary is {total_size_in_mb:.2f} MB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_in_000(total_size_in_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the patterns to separate the gdf in the dict based on the pattern\n",
    "surv_dict = {k: shp_dict[k] for k, v in shp_dict.items() if re.search(patterns[0], k)}\n",
    "swell_dict = {k: shp_dict[k] for k, v in shp_dict.items() if re.search(patterns[1], k)}\n",
    "bwell_dict = {k: shp_dict[k] for k, v in shp_dict.items() if re.search(patterns[2], k)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texas RRC land survey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O&G well symnum is a number that indicates the type of well simplified for fewer bins\n",
    "well_symnum_dict = {\n",
    "    2: \"Permitted Location\",\n",
    "    3: \"Dry Hole\",\n",
    "    4: \"Oil/Gas\",  # oil\n",
    "    5: \"Oil/Gas\",  # gas\n",
    "    6: \"Oil/Gas\",  # oil/ gas\n",
    "    7: \"Plugged/Shut-in\",  # oil\n",
    "    8: \"Plugged/Shut-in\",  # gas\n",
    "    9: \"Canceled Location\",\n",
    "    10: \"Plugged/Shut-in\",\n",
    "    11: \"Injection/Disposal\",\n",
    "    12: \"Core Test\",\n",
    "    17: \"Storage\",  # oil\n",
    "    18: \"Storage\",  # gas\n",
    "    19: \"Plugged/Shut-in\",  # oil\n",
    "    20: \"Plugged/Shut-in\",  # gas\n",
    "    21: \"Injection/Disposal\",  # oil\n",
    "    22: \"Injection/Disposal\",  # gas\n",
    "    23: \"Injection/Disposal\",  # oil/ gas\n",
    "    73: \"Brine Mining\",\n",
    "    74: \"Water Supply\",\n",
    "    75: \"Water Supply\",  # oil\n",
    "    76: \"Water Supply\",  # gas\n",
    "    77: \"Water Supply\",  # oil/ gas\n",
    "    86: \"Horizontal\",  # Horizontal Well Surface Location\",\n",
    "    87: \"Horizontal\",  # Directional/Sidetrack Well Surface Location,\n",
    "    88: \"Storage\",\n",
    "    103: \"Storage\",  # oil/gas\n",
    "}\n",
    "# well_symnum_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the GeoDataFrames in surv_dict into a single GeoDataFrame\n",
    "surv_data_gdf = concat_gdf_from_dict(surv_dict)\n",
    "\n",
    "# Convert the column names to snake case for consistency\n",
    "surv_data_gdf.columns = [pascal_to_snake(col) for col in surv_data_gdf.columns]\n",
    "# addd a coulmn for the county_code\n",
    "surv_data_gdf[\"county_code\"] = surv_data_gdf[\"source_file\"].str.extract(r\"(\\d{3})\")\n",
    "\n",
    "# Display a sample of 3 rows from the DataFrame\n",
    "display(surv_data_gdf.sample(3))\n",
    "\n",
    "# Display information about the DataFrame, including the number of non-null entries in each column\n",
    "surv_data_gdf.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texas RRC surface well data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the GeoDataFrames in well_dict into a single GeoDataFrame\n",
    "swell_data_gdf = concat_gdf_from_dict(swell_dict)\n",
    "\n",
    "# Convert the column names to snake case for consistency\n",
    "swell_data_gdf.columns = [pascal_to_snake(col) for col in swell_data_gdf.columns]\n",
    "# get the county code from the source_file column\n",
    "swell_data_gdf[\"county_code\"] = swell_data_gdf[\"source_file\"].str.extract(r\"(\\d{3})\")\n",
    "# map the dictionary to the SYMNUM column and fill the rare values with 'Other'\n",
    "swell_data_gdf[\"well_type\"] = (\n",
    "    swell_data_gdf[\"symnum\"].map(well_symnum_dict).fillna(\"Other\")\n",
    ")\n",
    "\n",
    "# Display a sample of 3 rows from the DataFrame\n",
    "display(swell_data_gdf.sample(3))\n",
    "\n",
    "# Display information about the DataFrame, including the number of non-null entries in each column\n",
    "swell_data_gdf.info(\n",
    "    memory_usage=\"deep\"\n",
    ")  # shp_dict = extract_specific_gdf_from_zip_url(shp_zip_urls, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swell_data_gdf[\"well_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the well types on a horizontal bar chart\n",
    "swell_data_gdf[\"well_type\"].value_counts().hvplot.barh(\n",
    "    height=400,\n",
    "    width=600,\n",
    "    title=\"Surface Well Types Count\",\n",
    "    xlabel=\"\",\n",
    "    ylabel=\"\",\n",
    "    xaxis=\"bare\",\n",
    "    hover_cols=\"all\",\n",
    ").opts(\n",
    "    active_tools=[\"box_zoom\"],\n",
    "    toolbar=\"above\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot on a map the dry holes\n",
    "dry_hole_gdf = swell_data_gdf[swell_data_gdf[\"well_type\"] == \"Dry Hole\"]\n",
    "\n",
    "# vertorize the coordinates\n",
    "dry_hole_mer = platecaree_to_mercator_vectorised(\n",
    "    dry_hole_gdf[\"geometry\"].x, dry_hole_gdf[\"geometry\"].y\n",
    ")\n",
    "dry_hole_coords = pd.DataFrame(dry_hole_mer, columns=[\"x\", \"y\"])\n",
    "\n",
    "bg_map = get_background_map()\n",
    "# plot the dry holes on a map\n",
    "bg_map * gv.Points(\n",
    "    dry_hole_coords.reset_index(), [\"x\", \"y\"], [\"index\"], crs=ccrs.GOOGLE_MERCATOR\n",
    ").opts(width=800, height=600, title=\"Dry Hole Locations\", size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to plot the well types on a map\n",
    "def plot_well_types(gdf, well_type):\n",
    "    \"\"\"Plots the well types on a map\"\"\"\n",
    "    # filter the swell_data_gdf for the well_type\n",
    "    well_type_gdf = gdf[gdf[\"well_type\"] == well_type]\n",
    "    # vertorize the coordinates\n",
    "    well_type_mer = platecaree_to_mercator_vectorised(\n",
    "        well_type_gdf[\"geometry\"].x, well_type_gdf[\"geometry\"].y\n",
    "    )\n",
    "    well_type_coords = pd.DataFrame(well_type_mer, columns=[\"x\", \"y\"])\n",
    "    # plot the well types on a map\n",
    "    return get_background_map() * gv.Points(\n",
    "        well_type_coords.reset_index(),\n",
    "        [\"x\", \"y\"],\n",
    "        [\"index\"],\n",
    "        crs=ccrs.GOOGLE_MERCATOR,\n",
    "    ).opts(width=800, height=600, title=f\"{well_type} Locations\", size=1).opts(\n",
    "        active_tools=[\"box_zoom\"],\n",
    "        toolbar=\"above\",\n",
    "        xaxis=\"bare\",\n",
    "        yaxis=\"bare\",\n",
    "        tools=[\"hover\"],\n",
    "    )\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# create a partial function for the plot_well_types function\n",
    "plot_well_types_partial = partial(plot_well_types, swell_data_gdf)\n",
    "\n",
    "plot_well_types_partial(\"Plugged/Shut-in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texas RRC bottom hole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the GeoDataFrames in well_dict into a single GeoDataFrame\n",
    "bwell_data_gdf = concat_gdf_from_dict(bwell_dict)\n",
    "\n",
    "# Convert the column names to snake case for consistency\n",
    "bwell_data_gdf.columns = [pascal_to_snake(col) for col in bwell_data_gdf.columns]\n",
    "# get the county code from the source_file column\n",
    "bwell_data_gdf[\"county_code\"] = bwell_data_gdf[\"source_file\"].str.extract(r\"(\\d{3})\")\n",
    "\n",
    "# map the dictionary to the SYMNUM column and fill the rare values with 'Other'\n",
    "bwell_data_gdf[\"well_type\"] = (\n",
    "    bwell_data_gdf[\"symnum\"].map(well_symnum_dict).fillna(\"Other\")\n",
    ")\n",
    "# Display a sample of 3 rows from the DataFrame\n",
    "display(bwell_data_gdf.sample(3))\n",
    "\n",
    "# Display information about the DataFrame, including the number of non-null entries in each column\n",
    "bwell_data_gdf.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### matching the mismatched wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swell_columns = [\"api\", \"lat83\", \"long83\", \"well_type\"]\n",
    "mismatch_columns = [\"tx_api\", \"lon\", \"lat\", \"original_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch_geo_tx[mismatch_columns].merge(\n",
    "    swell_data_gdf[swell_columns], how=\"left\", left_on=\"tx_api\", right_on=\"api\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_mismatch_tx = mismatch_geo_tx[mismatch_columns].merge(\n",
    "    swell_data_gdf[swell_columns], how=\"left\", left_on=\"tx_api\", right_on=\"api\"\n",
    ")\n",
    "\n",
    "# set the index to the original_index column\n",
    "joined_mismatch_tx.set_index(\"original_index\", inplace=True)\n",
    "\n",
    "# replace the lat and lon values with the lat83 and lon83 values if not nan\n",
    "joined_mismatch_tx.loc[joined_mismatch_tx[\"lat83\"].notna(), \"lat\"] = joined_mismatch_tx[\n",
    "    \"lat83\"\n",
    "]\n",
    "joined_mismatch_tx.loc[\n",
    "    joined_mismatch_tx[\"long83\"].notna(), \"lon\"\n",
    "] = joined_mismatch_tx[\"long83\"]\n",
    "\n",
    "# Update 'lat' and 'lon' in the original DataFrame\n",
    "\n",
    "\n",
    "registry_gdf.loc[joined_mismatch_tx.index, \"lat\"] = joined_mismatch_tx[\"lat\"]\n",
    "registry_gdf.loc[joined_mismatch_tx.index, \"lon\"] = joined_mismatch_tx[\"lon\"]\n",
    "\n",
    "\n",
    "# Update 'geometry' in the original DataFrame\n",
    "registry_gdf.loc[joined_mismatch_tx.index, \"geometry\"] = [\n",
    "    Point(xy) for xy in zip(joined_mismatch_tx.lon, joined_mismatch_tx.lat)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which other wells may have a state value which did not agree with the spatial join result again\n",
    "registry_county_gdf = registry_gdf.sjoin(\n",
    "    county_fips_gdf, how=\"left\", predicate=\"intersects\"\n",
    ").drop(columns=[\"index_right\"])\n",
    "mismatch_geo = registry_county_gdf[registry_county_gdf[\"state_right\"].isna()]\n",
    "mismatch_geo.shape\n",
    "print(f\"Number of rows with mismatched state values: {mismatch_geo.shape[0]}\")\n",
    "display(mismatch_geo.sample(3))\n",
    "\n",
    "# drop the rows with null values in the state_right column of mismatch_geo from the registry_gdf\n",
    "print(f\"Number of rows before dropping: {registry_gdf.shape[0]}\")\n",
    "registry_gdf.drop(mismatch_geo.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_gdf.sjoin(county_fips_gdf, how=\"left\", predicate=\"intersects\").drop(\n",
    "    columns=[\"index_right\"]\n",
    ")[trimmed_column_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the columns to keep\n",
    "columns_we_want = [\n",
    "    \"api\",\n",
    "    \"well\",\n",
    "    \"state_code\",\n",
    "    \"state_left\",\n",
    "    \"state_right\",\n",
    "    \"county_code\",\n",
    "    \"county_left\",\n",
    "    \"countyfp\",\n",
    "    \"name\",\n",
    "    \"operator\",\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"geoid\",\n",
    "    \"geometry\",\n",
    "]\n",
    "registry_county_gdf = registry_gdf.sjoin(\n",
    "    county_fips_gdf, how=\"left\", predicate=\"intersects\"\n",
    ").drop(columns=[\"index_right\"])[columns_we_want]\n",
    "\n",
    "print(f\"Number of rows: {registry_county_gdf.shape[0]}\")\n",
    "registry_county_gdf[\n",
    "    registry_county_gdf[\"state_left\"] != registry_county_gdf[\"state_right\"]\n",
    "][columns_we_want].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the rows where the state_left is California with the state_right being Texas\n",
    "# This would represent where the geometry said Texas but the api or original dataset had California\n",
    "# we check the api with\n",
    "cali_tx_query = registry_county_gdf.query(\n",
    "    'state_left == \"California\" & state_right == \"Texas\"'\n",
    ")\n",
    "# from the query, take the last 9 of the api (well_id) with 42 at beginning\n",
    "# with county_code from county that was matched with the geometry\n",
    "cali_tx_query[\"state_code\"] = \"42\"\n",
    "cali_tx_query[\"county_code\"] = cali_tx_query[\"countyfp\"]\n",
    "cali_tx_query[\"well_id\"] = cali_tx_query[\"api\"].str[-9:]\n",
    "cali_tx_query[\"api\"] = (\n",
    "    cali_tx_query[\"state_code\"]\n",
    "    + cali_tx_query[\"county_code\"]\n",
    "    + cali_tx_query[\"well_id\"]\n",
    ")\n",
    "\n",
    "# put api, county_code and state_code into the registry_gdf\n",
    "registry_gdf.loc[\n",
    "    cali_tx_query.index, [\"api\", \"county_code\", \"state_code\"]\n",
    "] = cali_tx_query[[\"api\", \"county_code\", \"state_code\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_county_gdf[\n",
    "    registry_county_gdf[columns_we_want][\"state_left\"]\n",
    "    != registry_county_gdf[columns_we_want][\"state_right\"]\n",
    "][columns_we_want]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swell_data_gdf[\"well_type\"].value_counts()\n",
    "# well_data_gdf[well_data_gdf[\"well_type\"] == \"Other\"][\"symnum\"].value_counts()\n",
    "# look for the mismatch_geo_tx api in the swell_data_gdf and get those rrows\n",
    "\n",
    "mismatch_geo_tx_sw = swell_data_gdf[\n",
    "    swell_data_gdf[\"api\"].isin(mismatch_geo_tx[\"tx_api\"])\n",
    "].copy()\n",
    "mismatch_geo_tx_sw\n",
    "# convert the long83 and lat83 columns to geomety Points\n",
    "mismatch_geo_tx_sw[\"geometry\"] = [\n",
    "    Point(xy) for xy in zip(mismatch_geo_tx_sw.long83, mismatch_geo_tx_sw.lat83)\n",
    "]\n",
    "# create a GeoDataFrame from the mismatch_geo_tx_sw dataframe\n",
    "mismatch_geo_tx_sw = gpd.GeoDataFrame(\n",
    "    mismatch_geo_tx_sw, geometry=\"geometry\", crs=\"EPSG:4269\"\n",
    ")\n",
    "# plot the mismatch_geo_tx_sw GeoDataFrame\n",
    "# bg_map * gv.Points(mismatch_geo_tx_sw).opts(height=500, width=800, tools=[\"hover\"])\n",
    "\n",
    "mismatch_geo_tx_sw.sjoin(county_fips_gdf, how=\"left\", predicate=\"intersects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the length of the tx_api values\n",
    "swell_data_gdf[\"api\"].astype(\"string\").str.len().value_counts()\n",
    "\n",
    "# drop the rows with the api number length of 3\n",
    "swell_data_gdf.drop(\n",
    "    swell_data_gdf[swell_data_gdf[\"api\"].astype(\"string\").str.len() == 3].index,\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_zip = SHP_ZIP_URLS[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the zip file at test_zip\n",
    "with urlopen(test_zip) as u:\n",
    "    zip_data = u.read()\n",
    "# extract the zip file\n",
    "with ZipMemoryFile(zip_data) as z:\n",
    "    zip_files = z.listdir()\n",
    "    print(zip_files)\n",
    "    # get the .shp files\n",
    "    shp_files = [f for f in zip_files if f.endswith(\".shp\")]\n",
    "    # read in the .shp files\n",
    "    for shp_file in shp_files:\n",
    "        with z.open(shp_file) as f:\n",
    "            gdf = gpd.GeoDataFrame.from_features(f, crs=f.crs)\n",
    "            # add column for the source file\n",
    "            gdf[\"source_file\"] = shp_file\n",
    "            print(f\"Shape of {shp_file}: {gdf.shape}\")\n",
    "            display(gdf.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_map = get_background_map()\n",
    "\n",
    "ok_counties = county_fips_gdf.query('state.str.contains(\"Oklahoma\")').copy()\n",
    "ok_counties.to_crs(\"EPSG:3857\", inplace=True)\n",
    "\n",
    "tx_counties = county_fips_gdf.query('state.str.contains(\"Texas\")').copy()\n",
    "tx_counties.to_crs(\"EPSG:3857\", inplace=True)\n",
    "\n",
    "ok_polys = gv.Polygons(ok_counties, crs=ccrs.GOOGLE_MERCATOR).opts(\n",
    "    projection=ccrs.GOOGLE_MERCATOR,\n",
    "    line_color=\"black\",\n",
    "    fill_alpha=0,\n",
    "    height=500,\n",
    "    width=500,\n",
    ")\n",
    "bg_map * ok_polys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of states that are in the Permian Basin.\n",
    "nm_tx = [\"New Mexico\", \"Texas\"]\n",
    "\n",
    "# Filter the county_fips_gdf DataFrame to include only the counties in the Permian states.\n",
    "counties_nm_tx_gdf = county_fips_gdf[county_fips_gdf[\"state\"].isin(nm_tx)]\n",
    "\n",
    "# Perform a spatial join between the registry_gdf and counties_nm_tx_gdf DataFrames.\n",
    "# This will add the data from counties_nm_tx_gdf to registry_gdf for matching locations.\n",
    "# After the join, drop the 'index_right' column as it's not needed.\n",
    "registry_nm_tx_gdf = registry_gdf.sjoin(counties_nm_tx_gdf).drop(\n",
    "    columns=[\"index_right\"]\n",
    ")\n",
    "registry_nm_tx_gdf.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_nm_tx_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_nm_tx_gdf[\"operator_name\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pivot table with the count of operators for each year\n",
    "operator_year_count = registry_nm_tx_gdf.pivot_table(\n",
    "    index=\"operator_name\", columns=\"job_start_year\", values=\"api\", aggfunc=\"count\"\n",
    ")\n",
    "# See which operators were active every year\n",
    "# operator_year_count[operator_year_count.count(axis=1) == 11]\n",
    "\n",
    "# See who has been active for the last 5 years\n",
    "operator_active_5y = operator_year_count.loc[\n",
    "    ~operator_year_count.iloc[:, -5:].isna().any(axis=1)\n",
    "].fillna(0)\n",
    "\n",
    "# do some styler table formatting from pandas\n",
    "style = operator_active_5y.style.background_gradient(\n",
    "    cmap=\"cet_CET_L2_r\", axis=1, vmin=0, vmax=operator_year_count.max().max()\n",
    ")\n",
    "# Format the numbers in the table as integers\n",
    "style = style.format(\"{:.0f}\")\n",
    "print(f\"Number of operators active for the last 5 years: {len(operator_active_5y)}\")\n",
    "style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask for rows where the first 2 characters of the api number are not 42 nor 30.\n",
    "# This is done to filter out rows that do not belong to the states we are interested in (Texas and New Mexico).\n",
    "api_mask = ~registry_nm_tx_gdf[\"api\"].str[0:2].isin([\"42\", \"30\"])\n",
    "\n",
    "# Apply the mask to the registry_nm_tx_gdf DataFrame to get the rows that match the condition.\n",
    "mismatch_state = registry_nm_tx_gdf[api_mask]\n",
    "\n",
    "# Display selected columns from the mismatch_state DataFrame.\n",
    "# These columns provide information about the well, its location, and the job start date.\n",
    "display(\n",
    "    mismatch_state[\n",
    "        [\n",
    "            \"api\",\n",
    "            \"api_number\",\n",
    "            \"state_right\",\n",
    "            \"state_name\",\n",
    "            \"state_number\",\n",
    "            \"well_name\",\n",
    "            \"operator_name\",\n",
    "            \"county_name\",\n",
    "            \"latitude\",\n",
    "            \"longitude\",\n",
    "            \"geometry\",\n",
    "            \"crs\",\n",
    "            \"job_start_date\",\n",
    "            \"county\",\n",
    "            \"countyfp\",\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# get a background map\n",
    "bg_map = get_background_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the coordinates to Mercator\n",
    "mercator_coords = platecaree_to_mercator_vectorised(\n",
    "    registry_nm_tx_gdf[\"geometry\"].x, registry_nm_tx_gdf[\"geometry\"].y\n",
    ")\n",
    "\n",
    "# Round the coordinates and create a DataFrame\n",
    "mer_points = pd.DataFrame(np.round(mercator_coords), columns=[\"x\", \"y\"])\n",
    "\n",
    "# Create a Points object for plotting\n",
    "gpoints = gv.Points(\n",
    "    mer_points.reset_index(), [\"x\", \"y\"], [\"index\"], crs=ccrs.GOOGLE_MERCATOR\n",
    ").opts(height=600, width=800, color=\"skyblue\", size=1, tools=[\"hover\"])\n",
    "\n",
    "# Create a layout with the background map and the points\n",
    "layout = bg_map * gpoints\n",
    "layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map files taken from the EIA website.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function 'extract_gdfs_from_zip_url_concurrent' to get GeoDataFrames from the URLs in 'basins_url_list'\n",
    "# This function concurrently downloads and extracts GeoDataFrames from the given URLs\n",
    "basins_dict = extract_gdfs_from_zip_url_concurrent(basins_url_list)\n",
    "\n",
    "# Display the keys of 'basins_dict' to see the names of the basins\n",
    "display(basins_dict.keys())\n",
    "\n",
    "# Concatenate the GeoDataFrames in 'basins_dict' into a single GeoDataFrame using the function 'concat_gdf_from_dict'\n",
    "basins_gdf = concat_gdf_from_dict(basins_dict)\n",
    "\n",
    "# Convert the column names of 'basins_gdf' to snake case for consistency\n",
    "# The function 'pascal_to_snake' is used to convert PascalCase or camelCase to snake_case\n",
    "basins_gdf.columns = [pascal_to_snake(col) for col in basins_gdf.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of sub basins/ geodataframes: {len(basins_dict)}\\n\")\n",
    "\n",
    "for k, gdf in basins_dict.items():\n",
    "    print(f\"{k}| Shape:{gdf.shape}| CRS:{gdf.crs.to_string()}\")\n",
    "    display(gdf.sample())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the shapefile of the basin boundaries\n",
    "basins_dict = extract_gdfs_from_zip_url_concurrent(basins_url_list)\n",
    "display(basins_dict.keys())\n",
    "basins_gdf = concat_gdf_from_dict(basins_dict)\n",
    "# scrub the column names\n",
    "basins_gdf.columns = [pascal_to_snake(col) for col in basins_gdf.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot shale plays and basin boundaries of the different formations in the Permian Basin\n",
    "bg_map * basins_gdf.hvplot(\n",
    "    geo=True,\n",
    "    alpha=0.5,\n",
    "    title=\"Shale Plays in the Permian Basin\",\n",
    "    legend=True,\n",
    "    by=\"shale_play\",\n",
    "    muted_alpha=0.01,\n",
    ").opts(\n",
    "    tools=[\"hover\", \"tap\"],\n",
    "    legend_position=\"right\",\n",
    "    height=600,\n",
    "    width=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holoviews import opts\n",
    "\n",
    "# Dissolve the geometries of the basins_gdf GeoDataFrame into a single geometry\n",
    "\n",
    "\n",
    "shale_plays_gdf = basins_gdf[[\"geometry\"]].dissolve()\n",
    "# get the counties in the Permian Basin\n",
    "permian_counties = county_fips_gdf.intersects(shale_plays_gdf)\n",
    "# plot the counties in the Permian Basin\n",
    "bg_map * gv.Polygons(county_fips_gdf[permian_counties]).opts(\n",
    "    title=\"Counties in the Permian Basin\",\n",
    "    tools=[\"hover\"],\n",
    "    height=600,\n",
    "    width=800,\n",
    "    alpha=0.5,\n",
    "    color=\"skyblue\",\n",
    ")\n",
    "# plot an outline of the Permian Basin over the counties\n",
    "overlay = (\n",
    "    bg_map\n",
    "    * gv.Polygons(county_fips_gdf[permian_counties])\n",
    "    * gv.Path(shale_plays_gdf)\n",
    "    # * gpoints\n",
    ")\n",
    "\n",
    "overlay.opts(\n",
    "    opts.Polygons(alpha=0.5, cmap=[\"#73d2ff\"], line_color=\"gray\"),\n",
    "    opts.Path(alpha=0.5, color=\"black\"),\n",
    "    opts.Overlay(tools=[\"hover\"], height=600, width=800),\n",
    "    opts.Points(color=\"crimson\"),\n",
    ")\n",
    "\n",
    "\n",
    "# plot to confirm that the geometries have been dissolved\n",
    "# bg_map * gv.Polygons(shale_plays_gdf).opts(\n",
    "#     # geo=True,\n",
    "#     title=\"Dissolved Shale play in the Permian Basin\",\n",
    "#     height=600,\n",
    "#     width=800,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State land leases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Mexico:\n",
    "> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the extract_gdfs_from_zip_url_concurrent function to download and extract GeoDataFrames\n",
    "# from the shapefile zip files at the URLs in the shp_url_list. The function returns a dictionary\n",
    "# where the keys are the names of the shapefiles and the values are the corresponding GeoDataFrames.\n",
    "nm_slo_dict = extract_gdfs_from_zip_url_concurrent(nm_slo_url_list)\n",
    "\n",
    "# Display the keys of the land_map_dict dictionary. These are the names of the shapefiles\n",
    "# that were downloaded and extracted.\n",
    "nm_slo_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# land_map_dict = extract_gdfs_from_zip_url_concurrent(shp_url_list)\n",
    "\n",
    "\n",
    "# land_map_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the gdfs in the dictionary\n",
    "for k, gdf in nm_slo_dict.items():\n",
    "    print(f\"{k}| Shape:{gdf.shape}| CRS:{gdf.crs.to_string()}\")\n",
    "    display(gdf.sample(3))\n",
    "    display(gdf.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 separate gdfs instead of concatenating them as they have distinct columns\n",
    "nm_slo_gdfs = list(nm_slo_dict.values())\n",
    "# first one is the geologic regions\n",
    "nm_slo_geo = nm_slo_gdfs[0]\n",
    "# scrub the columns\n",
    "nm_slo_geo.columns = [pascal_to_snake(col) for col in nm_slo_geo.columns]\n",
    "\n",
    "# Define  a dictionary for the opts to include in plot function\n",
    "poly_opts = dict(\n",
    "    alpha=0.8,\n",
    "    height=600,\n",
    "    width=800,\n",
    "    line_width=0,\n",
    "    line_color=\"lightgray\",\n",
    "    tools=[\"hover\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Adjust opts for this plot\n",
    "poly_opts_copy = poly_opts.copy()\n",
    "poly_opts_copy[\"line_width\"] = 1\n",
    "\n",
    "# plot the geologic regions gdf\n",
    "bg_map * gv.Polygons(nm_slo_geo.to_crs(\"EPSG:4269\"), vdims=[\"label\"]).opts(\n",
    "    **poly_opts_copy, cmap=[\"#73d2ff\"] * 256, title=\"New Mexico Geologic Regions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second one is the oil and gas leases on New Mexico State Trust Lands\n",
    "nm_slo_lease = nm_slo_gdfs[1]\n",
    "# scrub the columns\n",
    "nm_slo_lease.columns = [pascal_to_snake(col) for col in nm_slo_lease.columns]\n",
    "# create a new column for the area of the lease\n",
    "nm_slo_lease[\"area\"] = nm_slo_lease[\"geometry\"].area\n",
    "# groupby the ogrid_nam and sum the area\n",
    "# add the transformed area to the gdf\n",
    "nm_slo_lease[\"ogrid_area\"] = (\n",
    "    nm_slo_lease.groupby(\"ogrid_nam\")[\"area\"].transform(\"sum\") / 1e6\n",
    ")\n",
    "\n",
    "# plot the oil and gas leases gdf for New Mexico State Trust Lands\n",
    "bg_map * gv.Polygons(\n",
    "    nm_slo_lease.to_crs(\"EPSG:4269\"), vdims=[\"ogrid_nam\", \"ogrid_area\"]\n",
    ").opts(\n",
    "    **poly_opts,\n",
    "    cnorm=\"eq_hist\",\n",
    "    colorbar=True,\n",
    "    title=\"Oil and Gas Leases on New Mexico State Trust Lands\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(land_map_dict.values())\n",
    "# [gdf['geometry'] for gdf in land_map_dict.values()]\n",
    "random_color_list = [\n",
    "    \"#\" + \"\".join([random.choice(\"0123456789ABCDEF\") for j in range(6)])\n",
    "    for i in range(len(nm_slo_dict))\n",
    "]\n",
    "plots = []\n",
    "new_map = get_background_map()\n",
    "plots.append(new_map)\n",
    "for color, (name, gdf) in zip(random_color_list, nm_slo_dict.items()):\n",
    "    # Add new column with the name of the shapefile for the hover tool\n",
    "    gdf[\"label\"] = name\n",
    "    plot = gv.Polygons(gdf.to_crs(\"EPSG:4269\"), vdims=[\"label\"]).opts(\n",
    "        tools=[\"hover\"], height=600, width=800, alpha=0.5, title=\"\"\n",
    "    )\n",
    "    plots.append(plot)\n",
    "\n",
    "overlay = hv.Overlay(plots)\n",
    "overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm_slo_lease.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the GeoDataFrames in well_dict into a single GeoDataFrame\n",
    "swell_data_gdf = concat_gdf_from_dict(swell_dict)\n",
    "\n",
    "# Convert the column names to snake case for consistency\n",
    "swell_data_gdf.columns = [pascal_to_snake(col) for col in swell_data_gdf.columns]\n",
    "# get the county code from the source_file column\n",
    "swell_data_gdf[\"county_code\"] = swell_data_gdf[\"source_file\"].str.extract(r\"(\\d{3})\")\n",
    "\n",
    "# Display a sample of 3 rows from the DataFrame\n",
    "display(swell_data_gdf.sample(3))\n",
    "\n",
    "# Display information about the DataFrame, including the number of non-null entries in each column\n",
    "swell_data_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column to the surv_data_gdf with the county number\n",
    "# the county_number wil be the numbers in the source_file column\n",
    "surv_data_gdf[\"county_number\"] = surv_data_gdf[\"source_file\"].str.extract(r\"(\\d{3})\")\n",
    "\n",
    "# using just the geometry and the county_number columns, intersect with the permian basin gdf\n",
    "surv_permian_gdf = surv_data_gdf[[\"geometry\", \"county_number\"]].sjoin(\n",
    "    shale_plays_gdf[[\"geometry\"]], how=\"inner\", predicate=\"intersects\"\n",
    ")\n",
    "# see which counties are in the permian basin\n",
    "pb_county_numbers = surv_permian_gdf[\"county_number\"].unique().tolist()\n",
    "\n",
    "# plot the survey lines in the permian basin\n",
    "bg_map * gv.Polygons(\n",
    "    surv_permian_gdf.to_crs(\"EPSG:4269\"), vdims=[\"county_number\"]\n",
    ").opts(\n",
    "    tools=[\"hover\"],\n",
    "    height=600,\n",
    "    width=800,\n",
    "    alpha=0.5,\n",
    "    line_width=0,\n",
    "    title=\"Permian Basin Survey Lines\",\n",
    ")\n",
    "\n",
    "# surv_data_gdf.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how land survey polygon data looks on map\n",
    "pb_plot = shale_plays_gdf.hvplot(geo=True, color=\"red\", alpha=0.5, line_width=0).opts(\n",
    "    height=600, width=800\n",
    ")\n",
    "survey_plot = surv_data_gdf.hvplot(geo=True, color=\"blue\", alpha=0.5, line_width=0)\n",
    "\n",
    "# bg_map * survey_plot * pb_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the intersection of the survey polygons and the Permian Basin polygon\n",
    "survey_pb_gdf = gpd.overlay(surv_data_gdf, shale_plays_gdf, how=\"intersection\")\n",
    "survey_pb_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# survey_pb_gdf.explore()\n",
    "# surv_data_gdf.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join of the registry_gdf(from fracfocus) and surv_data_gdf\n",
    "registry_join_gdf = gpd.sjoin(\n",
    "    registry_gdf[\n",
    "        [\n",
    "            \"geometry\",\n",
    "            \"api\",\n",
    "            \"operator_name\",\n",
    "            \"well_name\",\n",
    "            \"state\",\n",
    "            \"county_name\",\n",
    "            \"county_number\",\n",
    "        ]\n",
    "    ],\n",
    "    surv_data_gdf,\n",
    ").drop(columns=[\"index_right\"])\n",
    "\n",
    "\n",
    "registry_join_gdf.sort_values(by=\"api\")\n",
    "\n",
    "registry_join_gdf.county_name.value_counts()\n",
    "\n",
    "# create a well_id column from the api column\n",
    "\n",
    "registry_join_gdf[\"tx_api\"] = registry_join_gdf[\"api\"].str[2:10]\n",
    "\n",
    "\n",
    "# merge the welltype column from well_data_gdf to registry_join_gdf on the api_short column\n",
    "\n",
    "registry_join_gdf = (\n",
    "    registry_join_gdf.merge(swell_data_gdf[[\"tx_api\", \"well_type\"]], on=\"tx_api\")\n",
    "    .drop(columns=[\"scrap_file\", \"level4_sur\"])\n",
    "    .rename(columns={\"level2_blo\": \"block\"})\n",
    ")\n",
    "# registry_join_gdf.explore()\n",
    "# plot polygons using geoviews\n",
    "bg_map * gv.Polygons(registry_join_gdf.to_crs(\"EPSG:4269\"), vdims=[\"well_type\"]).opts(\n",
    "    **poly_opts, color=\"well_type\", title=\"Well Types in the Permian Basin\"\n",
    ")\n",
    "\n",
    "bg_map * registry_join_gdf.hvplot(\n",
    "    geo=True,\n",
    "    by=\"well_type\",\n",
    "    alpha=0.8,\n",
    "    legend=\"right\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    size=1,\n",
    "    muted_alpha=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geodatabase files taken from the Texas GLO (General Land Office.)\n",
    "\n",
    "These files contained both the Oil and Gas Leases (active only), managed by the Texas GLO, and Oil & Gas units (active only) which is Oil and Gas pooling agreements managed by the Texas GLO. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the geodataframe of the active leases\n",
    "# active_gdb_dict = read_gdb_from_zip_url(gdb_zip_urls)\n",
    "\n",
    "\n",
    "# get the geodataframe of the active leases using concurrent futures\n",
    "active_gdb_dict = read_gdb_from_zip_url_concurrent(GDB_ZIP_URLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_gdb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_gdb_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the active lease geodatabase\n",
    "active_leases_gdf = active_gdb_dict[\"OAG_Leases_Active\"]\n",
    "# clean column names\n",
    "active_leases_gdf.columns = [pascal_to_snake(col) for col in active_leases_gdf.columns]\n",
    "active_leases_gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the columns with the date in it using regex\n",
    "date_cols = [col for col in active_leases_gdf.columns if re.search(r\"date\", col)]\n",
    "# add any other columns that should be dates\n",
    "date_cols.extend([\"lease_input\"])\n",
    "\n",
    "date_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the date columns to datetime\n",
    "active_leases_gdf[date_cols] = pd.concat(\n",
    "    [pd.to_datetime(active_leases_gdf[col]) for col in date_cols], axis=1\n",
    ")\n",
    "# active_leases_gdf[date_cols] = active_leases_gdf[date_cols].fillna(\n",
    "#     pd.Timestamp(\"1900-06-28\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the columns interested in seeing\n",
    "columns_of_interest = date_cols + [\n",
    "    \"county\",\n",
    "    \"geometry\",\n",
    "    \"land_type\",\n",
    "    \"primary_term_year\",\n",
    "    \"original_lessee\",\n",
    "    \"lessor\",\n",
    "    \"field_name\",\n",
    "    \"lease_type\",\n",
    "    \"lease_status\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_leases_gdf[columns_of_interest].info()\n",
    "active_leases_gdf[active_leases_gdf[columns_of_interest].isna().any(axis=1)][\n",
    "    columns_of_interest\n",
    "].sort_values(by=\"effective_date\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_date_cols = list(set(columns_of_interest) - set(date_cols))\n",
    "pd.concat(\n",
    "    [\n",
    "        active_leases_gdf[non_date_cols],\n",
    "        active_leases_gdf[date_cols].astype(\n",
    "            str\n",
    "        ),  # the .explore() does not work with NaT in datetime columns\n",
    "    ],\n",
    "    axis=1,\n",
    ").explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Production Data Query Dump from RRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dask client with specified configuration\n",
    "# This client will use 4 workers, each with 1 thread and a memory limit of 2GB\n",
    "# The Dask diagnostic dashboard will be served at the address \":8788\"\n",
    "client = Client(\n",
    "    n_workers=4, threads_per_worker=1, memory_limit=\"2GB\", dashboard_address=\":8788\"\n",
    ")\n",
    "\n",
    "# Print the link to the Dask diagnostic dashboard\n",
    "# This link can be used to monitor the progress and performance of Dask computations\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general purpose table paths\n",
    "county_path = Path(\"../data/PDQ_DSV/GP_COUNTY_DATA_TABLE.dsv\")\n",
    "district_path = Path(\"../data/PDQ_DSV/GP_DISTRICT_DATA_TABLE.dsv\")\n",
    "date_range_path = Path(\"../data/PDQ_DSV/GP_DATE_RANGE_CYCLE_DATA_TABLE.dsv\")\n",
    "\n",
    "# production report data paths\n",
    "county_cycle_path = Path(\"../data/PDQ_DSV/OG_COUNTY_CYCLE_DATA_TABLE.dsv\")\n",
    "county_lease_path = Path(\"../data/PDQ_DSV/OG_COUNTY_LEASE_CYCLE_DATA_TABLE.dsv\")\n",
    "district_cycle_path = Path(\"../data/PDQ_DSV/OG_DISTRICT_CYCLE_DATA_TABLE.dsv\")\n",
    "\n",
    "field_cycle_path = Path(\"../data/PDQ_DSV/OG_FIELD_CYCLE_DATA_TABLE.dsv\")\n",
    "field_info_path = Path(\"../data/PDQ_DSV/OG_FIELD_DW_DATA_TABLE.dsv\")\n",
    "\n",
    "lease_cycle_path = Path(\"../data/PDQ_DSV/OG_LEASE_CYCLE_DATA_TABLE.dsv\")\n",
    "lease_disposition_path = Path(\"../data/PDQ_DSV/OG_LEASE_CYCLE_DISP_DATA_TABLE.dsv\")\n",
    "lease_info_path = Path(\"../data/PDQ_DSV/OG_REGULATORY_LEASE_DW_DATA_TABLE.dsv\")\n",
    "\n",
    "operator_cycle_path = Path(\"../data/PDQ_DSV/OG_OPERATOR_CYCLE_DATA_TABLE.dsv\")\n",
    "operator_info_path = Path(\"../data/PDQ_DSV/OG_OPERATOR_DW_DATA_TABLE.dsv\")\n",
    "# summary tables\n",
    "summary_master_path = Path(\"../data/PDQ_DSV/OG_SUMMARY_MASTER_LARGE_DATA_TABLE.dsv\")\n",
    "summary_onshore_path = Path(\"../data/PDQ_DSV/OG_SUMMARY_ONSHORE_LEASE_DATA_TABLE.dsv\")\n",
    "# well completion data\n",
    "well_comp_path = Path(\"../data/PDQ_DSV/OG_WELL_COMPLETION_DATA_TABLE.dsv\")\n",
    "\n",
    "pdq_dsv_paths = [\n",
    "    county_path,\n",
    "    district_path,\n",
    "    date_range_path,\n",
    "    county_cycle_path,\n",
    "    county_lease_path,\n",
    "    district_cycle_path,\n",
    "    field_cycle_path,\n",
    "    field_info_path,\n",
    "    lease_cycle_path,\n",
    "    lease_disposition_path,\n",
    "    lease_info_path,\n",
    "    operator_cycle_path,\n",
    "    operator_info_path,\n",
    "    summary_master_path,\n",
    "    summary_onshore_path,\n",
    "    well_comp_path,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_overview(path):\n",
    "    \"\"\"\n",
    "    Returns an overview of the data in the file at the given path.\n",
    "\n",
    "    This function returns a dictionary containing the file name, the number of rows,\n",
    "    the column names, the first five rows of the data, and the column data types.\n",
    "    It uses Dask to efficiently compute the number of rows in the file, which makes\n",
    "    it suitable for large files.\n",
    "\n",
    "    Parameters:\n",
    "    path (Path or str): The path to the data file.\n",
    "\n",
    "    Returns:\n",
    "    dict: A pandas DataFrame containing the file overview.\n",
    "    \"\"\"\n",
    "    column_types = {}\n",
    "    # Initialize ddf as an empty Dask DataFrame\n",
    "    while True:\n",
    "        try:\n",
    "            ddf = dd.read_csv(path, sep=\"}\", dtype=column_types)\n",
    "            num_rows = len(ddf)\n",
    "            break\n",
    "        except ValueError as e:\n",
    "            match = re.search(r\"dtype=\\{'(.*?)': '(.*?)'\", str(e))\n",
    "            # match = re.search(r\"dtype=\\{'(.*?)': 'object'\", str(e))\n",
    "            if match:\n",
    "                column_name = match.group(1)\n",
    "                column_types[column_name] = \"object\"\n",
    "\n",
    "    sample_data = pd.read_csv(path, sep=\"}\", nrows=1000, dtype=column_types)\n",
    "    columns = sample_data.columns.tolist()\n",
    "\n",
    "    # column_dtypes = ddf.dtypes.tolist()\n",
    "    column_dtypes = sample_data.dtypes.tolist()\n",
    "\n",
    "    overview = pd.DataFrame(\n",
    "        {\n",
    "            \"file_name\": [path.stem],\n",
    "            \"file_extension\": [path.suffix],\n",
    "            \"num_rows\": [num_rows],\n",
    "            \"num_cols\": [len(columns)],\n",
    "            \"cols\": [columns],\n",
    "            \"col_dtypes\": [column_dtypes],\n",
    "            # \"sample_data\": [sample_data.to_json()],\n",
    "        },\n",
    "    )\n",
    "    return overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_table_overview(path):\n",
    "    \"\"\"\n",
    "    Displays an overview of the data in the file at the given path.\n",
    "\n",
    "    This function prints the file name, the number of rows, the column names,\n",
    "    and the first five rows of the data. It uses Dask to efficiently compute\n",
    "    the number of rows in the file, which makes it suitable for large files.\n",
    "\n",
    "    Parameters:\n",
    "    path (Path or str): The path to the data file.\n",
    "    \"\"\"\n",
    "    # make a decorative line\n",
    "    decor_length = len(path.name) + 8\n",
    "    decor = \"~*~\" * (decor_length // 3)\n",
    "\n",
    "    print(f\"{decor}\")\n",
    "    print(f\"File: {path.name}\")\n",
    "    print(f\"{decor}\")\n",
    "\n",
    "    column_types = {}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            ddf = dd.read_csv(path, sep=\"}\", dtype=column_types)\n",
    "            print(f\"Number of rows: {len(ddf)}\")\n",
    "            break\n",
    "        except ValueError as e:\n",
    "            # print(e)\n",
    "            # If a ValueError occurs, parse the error message to get the column name\n",
    "            match = re.search(r\"dtype=\\{'(.*?)': '(.*?)'\", str(e))\n",
    "            # match = re.search(r\"dtype=\\{'(.*?)': 'object'\", str(e))\n",
    "            if match:\n",
    "                column_name = match.group(1)\n",
    "                print(f\"Problematic column: {column_name}\")\n",
    "                # Add the problematic column to the column types dictionary\n",
    "                column_types[column_name] = \"object\"\n",
    "\n",
    "    # columns = pd.read_csv(\n",
    "    #     path, delimiter=\"}\", nrows=1, dtype=column_types\n",
    "    # ).columns.tolist()\n",
    "    columns = ddf.columns.tolist()\n",
    "\n",
    "    print(f\"Number of columns: {len(columns)}\")\n",
    "    print(f\"Columns: {columns}\")\n",
    "    print(f\"Column data types: {ddf.dtypes.tolist()}\")\n",
    "\n",
    "    print(f\"Sample of data in {path.name}:\")\n",
    "    display(pd.read_csv(path, delimiter=\"}\", nrows=20, dtype=column_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_table_overview(operator_cycle_path)\n",
    "# pd.read_csv(operator_cycle_path, sep=\"}\", nrows=1000).dtypes\n",
    "# column_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the table overview for each file in the pdq_dsv_paths list\n",
    "tables_list = []\n",
    "for fpath in pdq_dsv_paths:\n",
    "    print(f\"Getting overview for {fpath.stem}\")\n",
    "    tables_list.append(get_table_overview(fpath))\n",
    "\n",
    "# pdq_dsv_overviews = [get_table_overview(path) for path in tqdm(pdq_dsv_paths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the tables_list into a single DataFrame\n",
    "table_samples_df = pd.concat(tables_list, ignore_index=True)\n",
    "table_samples_df.iloc[:, 0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func to get the column data types\n",
    "def get_column_dtypes(table, row_number):\n",
    "    \"\"\"Returns a dictionary of the column names and their data types\"\"\"\n",
    "    # get the column names from the table\n",
    "    columns = table.loc[row_number, \"cols\"]\n",
    "    # get the column data types from the table\n",
    "    column_dtypes = table.loc[row_number, \"col_dtypes\"]\n",
    "    # create a dictionary of the column names and their data types\n",
    "    column_dtypes_dict = dict(zip(columns, column_dtypes))\n",
    "    return column_dtypes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# row_num = 0\n",
    "# create a defaultdict of dicts\n",
    "table_col_dtype_dict = defaultdict(dict)\n",
    "for i in range(len(table_samples_df)):\n",
    "    # get the column data types for the row\n",
    "    column_dtypes_dict = get_column_dtypes(table_samples_df, i)\n",
    "    # add the column data types to the defaultdict\n",
    "    table_col_dtype_dict[i].update(column_dtypes_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_num = 0\n",
    "# row 0 is the general purpose county table\n",
    "\n",
    "county_gp_ddf = dd.read_csv(\n",
    "    pdq_dsv_paths[row_num], sep=\"}\", dtype=table_col_dtype_dict[row_num]\n",
    ")\n",
    "county_gp_ddf.columns = [pascal_to_snake(col) for col in county_gp_ddf.columns]\n",
    "county_gp_ddf[\"county_code\"] = (\n",
    "    county_gp_ddf[\"county_fips_code\"].astype(\"string\").str.zfill(3)\n",
    ")\n",
    "county_gp_ddf = county_gp_ddf[county_gp_ddf[\"on_shore_flag\"] == \"Y\"]\n",
    "county_gp_ddf = county_gp_ddf.drop(\n",
    "    columns=[\n",
    "        \"county_fips_code\",\n",
    "        \"county_no\",\n",
    "        \"on_shore_flag\",\n",
    "        \"onshore_assc_cnty_flag\",\n",
    "    ]\n",
    ")\n",
    "county_gp_ddf\n",
    "\n",
    "# get_column_dtypes(table_samples_df, row_number=row_num)\n",
    "\n",
    "\n",
    "# ddf_county_gp.columns = [pascal_to_snake(col) for col in ddf_county_gp.columns]\n",
    "# ddf_county_gp.head(5)\n",
    "\n",
    "# nan_columns = ddf_county_gp.columns[ddf_county_gp.isna().all().compute()]\n",
    "# ddf_county_gp = ddf_county_gp.drop(columns=nan_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_num = 3\n",
    "# row 3 is the county cycle table\n",
    "\n",
    "county_cycle_ddf = dd.read_csv(\n",
    "    pdq_dsv_paths[row_num], sep=\"}\", dtype=table_col_dtype_dict[row_num]\n",
    ")\n",
    "county_cycle_ddf.columns = [pascal_to_snake(col) for col in county_cycle_ddf.columns]\n",
    "# filter for years after 2012\n",
    "county_cycle_ddf = county_cycle_ddf[county_cycle_ddf[\"cycle_year\"] > 2012]\n",
    "county_cycle_ddf[\"county_code\"] = (\n",
    "    county_cycle_ddf[\"county_no\"].astype(\"string\").str.zfill(3)\n",
    ")\n",
    "# filter for the county_code in the county_gp_ddf\n",
    "county_cycle_ddf = county_cycle_ddf[county_cycle_ddf[\"county_code\"] < \"508\"]\n",
    "# county_cycle_ddf.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by county_code and cyclle_year and sum cnty_oil_prod_vol\n",
    "county_cycle_grouped = county_cycle_ddf.groupby([\"county_code\", \"cycle_year\"])\n",
    "county_cycle_grouped = county_cycle_grouped[\n",
    "    \"cnty_oil_prod_vol\", \"cnty_gas_prod_vol\", \"cnty_cond_prod_vol\", \"cnty_csgd_prod_vol\"\n",
    "].sum()\n",
    "county_cycle_computed = county_cycle_grouped.compute()\n",
    "county_cycle_computed.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# county_cycle_computed = county_cycle_computed.reset_index()\n",
    "# get only the top 3 counties by oil production for each year\n",
    "max_oil_county_index = (\n",
    "    county_cycle_computed.reset_index()\n",
    "    .groupby(\"cycle_year\")[\"cnty_oil_prod_vol\"]\n",
    "    .idxmax()\n",
    ")\n",
    "max_gas_county_index = county_cycle_computed.groupby(\"cycle_year\")[\n",
    "    \"cnty_gas_prod_vol\"\n",
    "].idxmax()\n",
    "max_cond_county_index = county_cycle_computed.groupby(\"cycle_year\")[\n",
    "    \"cnty_cond_prod_vol\"\n",
    "].idxmax()\n",
    "max_csgd_county_index = county_cycle_computed.groupby(\"cycle_year\")[\n",
    "    \"cnty_csgd_prod_vol\"\n",
    "].idxmax()\n",
    "\n",
    "top_oil_counties = county_cycle_computed.loc[max_oil_county_index]\n",
    "top_gas_counties = county_cycle_computed.loc[max_gas_county_index]\n",
    "top_cond_counties = county_cycle_computed.loc[max_cond_county_index]\n",
    "top_csgd_counties = county_cycle_computed.loc[max_csgd_county_index]\n",
    "top_counties = pd.concat(\n",
    "    [top_oil_counties, top_gas_counties, top_cond_counties, top_csgd_counties]\n",
    ").drop_duplicates()\n",
    "top_counties.sort_values(by=[\"cycle_year\"], ascending=False)\n",
    "\n",
    "computed_county_gp = county_gp_ddf.compute()\n",
    "# merge the top_counties with the county_gp_ddf\n",
    "top_counties = top_counties.merge(computed_county_gp, on=\"county_code\")\n",
    "top_counties.sort_values(by=[\"cycle_year\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_lease = dd.read_csv(\n",
    "    lease_cycle_path,\n",
    "    sep=\"}\",\n",
    "    dtype={\"DISTRICT_NAME\": str, \"GAS_WELL_NO\": str},\n",
    ")\n",
    "ddf_lease.columns = [pascal_to_snake(col) for col in ddf_lease.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_lease.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to a certain yearse\n",
    "ddf_lease_filtered = ddf_lease[ddf_lease[\"cycle_year\"] > 2012]\n",
    "\n",
    "# repartition the dataframe\n",
    "# ddf_lease_filtered = ddf_lease_filtered.repartition(npartitions=1000)\n",
    "\n",
    "operators_by_lease = ddf_lease_filtered[\"operator_name\"].value_counts()\n",
    "\n",
    "top_operators_by_lease = operators_by_lease.nlargest(10)\n",
    "\n",
    "# top_operators_by_lease = ddf_lease_filtered[\"operator_name\"].value_counts().nlargest(10)\n",
    "\n",
    "desc = ddf_lease_filtered.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_operators_computed, operators_computed = dask.compute(\n",
    "    top_operators_by_lease, operators_by_lease\n",
    ")\n",
    "\n",
    "top_operators_computed.hvplot.barh(\n",
    "    height=400,\n",
    "    width=600,\n",
    "    title=\"Top 10 Operators by Lease Count\",\n",
    "    xlabel=\"\",\n",
    "    ylabel=\"\",\n",
    "    xaxis=\"bare\",\n",
    "    hover_cols=\"all\",\n",
    ").opts(\n",
    "    active_tools=[\"box_zoom\"],\n",
    "    toolbar=\"above\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operators_computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = ddf_lease_filtered.groupby([\"cycle_year\", \"operator_name\", \"district_no\"])[\n",
    "    \"lease_oil_prod_vol\",\n",
    "    \"lease_gas_prod_vol\",\n",
    "    \"lease_cond_prod_vol\",\n",
    "    \"lease_csgd_prod_vol\",\n",
    "].sum()\n",
    "\n",
    "# grouped.visualize(optimize_graph=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_computed = grouped.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_product(grouped_computed, product):\n",
    "    max_index = (\n",
    "        grouped_computed.reset_index()\n",
    "        .groupby([\"cycle_year\", \"district_no\"])[f\"lease_{product}_prod_vol\"]\n",
    "        .idxmax()\n",
    "    )\n",
    "    result = grouped_computed.reset_index().loc[max_index]\n",
    "    result[\"product\"] = product\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function for different products\n",
    "oil_result = get_max_product(grouped_computed, \"oil\")\n",
    "gas_result = get_max_product(grouped_computed, \"gas\")\n",
    "cond_result = get_max_product(grouped_computed, \"cond\")\n",
    "csgd_result = get_max_product(grouped_computed, \"csgd\")\n",
    "\n",
    "# Concatenate the results\n",
    "final_result = pd.concat([oil_result, gas_result, cond_result, csgd_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.sort_values(by=[\"cycle_year\", \"district_no\", \"product\"]).info(\n",
    "    memory_usage=\"deep\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Dask client with a higher memory limit\n",
    "client = Client(memory_limit=\"10GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the CSV file\n",
    "# lease_path = \"../data/raw/lease_table.csv\"\n",
    "\n",
    "# Define the column types\n",
    "column_types = {\"DISTRICT_NAME\": str, \"GAS_WELL_NO\": str}\n",
    "\n",
    "# Read the CSV file with Dask\n",
    "lease_table_df = dd.read_csv(lease_cycle_path, sep=\"}\", blocksize=1e8, dtype=column_types)\n",
    "\n",
    "# Convert column names to snake case\n",
    "lease_table_df.columns = [pascal_to_snake(col) for col in lease_table_df.columns]\n",
    "\n",
    "# Repartition the DataFrame into 10 partitions\n",
    "lease_table_df = lease_table_df.repartition(npartitions=10)\n",
    "\n",
    "# Save the DataFrame as a Parquet file\n",
    "lease_table_df.to_parquet(\"../data/processed/og_lease_cycle_data_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = client.scheduler_info()  # get scheduler info\n",
    "\n",
    "workers = info[\"workers\"]  # get the workers\n",
    "\n",
    "print(f\"Number of workers: {len(workers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
