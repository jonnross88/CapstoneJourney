{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This project is part of a Capstone project for Springboard Data Science Career Track.\n",
    "\n",
    "The goal of this project is to develop a machine learning model to rank and predict the likelihood that an oil company will initiate a frac job in a county within the Permian Basin in the first quarter of 2024.\n",
    "How can we develop a machine learning model that can accurately forecast at least \n",
    "- 8 of the top 10 energy companies by production volume of oil, natural gas, condensate, and casinghead gas in at least \n",
    "- 1 out of the 2 top districts in Texas \n",
    "- for the first quarter of 2024, using historical production data, well characteristics, and geographic location as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Logger setup\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Clear existing handlers\n",
    "logger.handlers = []\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "# Create handlers\n",
    "# c_handler = logging.StreamHandler()\n",
    "f_handler = logging.FileHandler(\"CapstoneTwo-DataWrangling.log\")\n",
    "# c_handler.setLevel(logging.ERROR)\n",
    "f_handler.setLevel(logging.ERROR)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "# c_handler.setFormatter(formatter)\n",
    "f_handler.setFormatter(formatter)\n",
    "\n",
    "# logger.addHandler(c_handler)\n",
    "logger.addHandler(f_handler)\n",
    "\n",
    "# Prevent propagation to root logger\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Import statements\n",
    "from collections import defaultdict\n",
    "from http.client import IncompleteRead\n",
    "from time import sleep\n",
    "import concurrent.futures as cf\n",
    "from datetime import datetime\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import tempfile\n",
    "import warnings\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import URLError\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import missingno as msno\n",
    "import cartopy.crs as ccrs\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "import geoviews as gv\n",
    "import geoviews.tile_sources as gts\n",
    "import colorcet as cc\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "import hvplot.pandas  # noqa\n",
    "import hvplot.dask  # noqa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyproj\n",
    "from fiona.io import ZipMemoryFile\n",
    "from pyvis.network import Network\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import dask\n",
    "from dask import persist\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from dask.distributed import Client\n",
    "import panel as pn\n",
    "import panel.widgets as pnw\n",
    "from panel.template import FastListTemplate\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "gv.extension(\"bokeh\")\n",
    "pn.extension(\"tabulator\", template=\"fast\", sizing_mode=\"stretch_width\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Test initial print statement\n",
    "print(\"CapstoneJourney begins!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Statement\n",
    "\n",
    "Texas is the largest oil producing state in the United States. It plays a vital role in the energy future of the US. It is therefore vital that energy companies are able to make informed decisions as to where to allocate resources and capital. The goal of this project is to develop a machine learning model to predict the top ten counties in Texas that will have the highest oil and the highest gas production in the first quarter of 2024. The model will use historical Production data from the Texas RRC, well properties data from the FracFocus website, geological features and ecnomic indicators as input. An interactive vizualization tool that will allow users to explore predicted rankings and factors influencing these rankings. The success of this project hinges on the precision of at least 80% of the top 10 predictions. This will provide with a focused and engaging tool for strategic planning and data driven future planning.\n",
    "\n",
    "How can we develop a machine learning model that can accurately forecast the top ten oil and gas producing counties in Texas for the first quarter of 2024, using historical production data, well characteristics, and geographic location as inputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants\n",
    "Let's start by defining some constants that will be used throughout this notebook.\n",
    "\n",
    "Most of the data was first downloaded from external websites and then uploaded onto a cloud storage bucket. This was done to ensure consistency and availability during the project. A brief description of the data and its original source link is referenced below.\n",
    "\n",
    "### Data Sources\n",
    "\n",
    "The following table provides an overview of the data sources used in this project:\n",
    "\n",
    "| Dataset Name | Source URL | Original Source | Description | Date Downloaded |\n",
    "|--------------|------------|-----------------|-------------|-----------------|\n",
    "| RegistryUpload Table | [link](https://fracfocus.org/data-download) | FracFocus | This table contains each disclosureâ€™s header information such as the job date, API number, location, base water volume, and total vertical depth. | 2023-11-11 |\n",
    "| RBDMSWells | [link](https://gisdata-occokc.opendata.arcgis.com/datasets/OCCOKC::rbdms-wells/about) | Oklahoma Corporation Commission | This table contains Oklahoma RBDMS statewide well data | 2023-11-23 |\n",
    "| Wolfcamp Delaware Play Boundary | [link](https://www.eia.gov/maps/maps.htm)| EIA | Permian Basin, Delaware Sub-Basin: Wolfcamp play boundary (9/4/2018) | 2023-11-19 |\n",
    "| Wolfcamp Midland Play Boundaries | [link](https://www.eia.gov/maps/maps.htm)| EIA | Wolfcamp A, B, C, and D play boundaries, Midland Basin (6/4/2020) | 2023-11-21 |\n",
    "| ShalePlay Delaware | [link](https://www.eia.gov/maps/maps.htm)| EIA |Delaware play boundary (10/8/2019)  | 2023-11-21 |\n",
    "| AboYeso GlorietaYeso Spraberry | [link](https://www.eia.gov/maps/maps.htm)| EIA | Abo-Yeso, Glorieta-Yeso, and Spraberry play boundaries (3/11/2016) | 2023-11-21 |\n",
    "| Production Data Query Dump| [link](https://rrc.texas.gov/resource-center/research/data-sets-available-for-download/)| Railroad Commission of Texas | Production Data Query Dump (11/17/2023) | 2023-11-17 |\n",
    "| All Layers By County | [link](https://rrc.texas.gov/resource-center/research/data-sets-available-for-download/)  | Railroad Commission of Texas | Map & Associated Data: Base Map, Wells, Surveys & Pipelines layers | 2023-11-17 |\n",
    "| Oil & Gas Leases | [link](https://www.glo.texas.gov/land/land-management/gis/index.html) | Texas General Land Office | Active Leases (11/17/2023) | 2023-11-17 |\n",
    "| Oil & Gas Units | [link](https://www.glo.texas.gov/land/land-management/gis/index.html) | Texas General Land Office | Active Units (11/17/2023) | 2023-11-17 |\n",
    "| U.S. County Boundaries | [link](https://www2.census.gov/geo/tiger/TIGER2022/COUNTY/tl_2022_us_county.zip) | United States Census Bureau | County (2022-10-31). Data is downloaded directly in the code. | N/A |\n",
    "| U.S. County FIPS Codes | [link](https://en.wikipedia.org/wiki/List_of_United_States_FIPS_codes_by_county) | Wikipedia | List of United States FIPS codes by county. Data is downloaded directly in the code. | N/A |\n",
    "\n",
    "Each row in the table represents a different dataset. The columns are:\n",
    "\n",
    "- **Dataset Name**: The name of the dataset.\n",
    "- **Source URL**: The URL where the dataset can be downloaded. Click on \"link\" to access the webpage.\n",
    "- **Original Source**: The original source of the data.\n",
    "- **Description**: A brief description of the dataset.\n",
    "- **Date Downloaded**: The date when the dataset was downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "# URLs to retrieve data taken from data sources named above.\n",
    "# They have been predownloaded and stored in a Google Cloud Storage bucket.\n",
    "\n",
    "\n",
    "# FracFocus Chemical Disclosure Registry CSV files.\n",
    "# There are 24 files in total, named FracFocusRegistry_i.csv where i ranges from 1 to 24.\n",
    "\n",
    "DATA_URLS1 = [\n",
    "    f\"https://storage.googleapis.com/mrprime_dataset/fracfocus/FracFocusRegistry_{i}.csv\"\n",
    "    for i in range(1, 25)\n",
    "]\n",
    "\n",
    "\n",
    "# FracFocus registryupload CSV files.\n",
    "# There are 3 files in total, named registryupload_i.csv where i ranges from 1 to 3.\n",
    "\n",
    "DATA_URLS2 = [\n",
    "    f\"https://storage.googleapis.com/mrprime_dataset/fracfocus/registryupload_{j}.csv\"\n",
    "    for j in range(1, 4)\n",
    "]\n",
    "\n",
    "\n",
    "# FracFocus readme.txt file\n",
    "\n",
    "DATA_README_URL = [\n",
    "    \"https://storage.googleapis.com/mrprime_dataset/fracfocus/readme.txt\"\n",
    "]\n",
    "\n",
    "\n",
    "# Oklahoma Corporation Commission well data\n",
    "\n",
    "OCC_PARQUET_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/occ/rbdms_wells.parquet\"\n",
    "\n",
    "\n",
    "# Energy Information Administration (EIA) shale play geospatial data\n",
    "# url for the shapefiles of Permian Basin, Delaware Sub-Basin: Wolfcamp play boundary\n",
    "WOLFCAMP_ZIP_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/eia/Wolfcamp_Delaware_Play_Boundary.zip\"\n",
    "MIDLAND_ZIP_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/eia/Wolfcamp_Midland_Play_Boundaries_EIA.zip\"\n",
    "DELAWARE_ZIP_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/eia/ShalePlay_Delaware_EIA.zip\"\n",
    "ABOYESO_ZIP_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/eia/ShalePlays_AboYeso_GlorietaYeso_Spraberry_EIA.zip\"\n",
    "\n",
    "# group EIA shale play geospatial data urls into a list\n",
    "basins_url_list = [WOLFCAMP_ZIP_URL, MIDLAND_ZIP_URL,\n",
    "                   DELAWARE_ZIP_URL, ABOYESO_ZIP_URL]\n",
    "\n",
    "# Railroad Commission of Texas (RRC) Production Report data query\n",
    "PDQ_URL = (\n",
    "    \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/rrc/PDQ_DSV.zip\"\n",
    ")\n",
    "\n",
    "# county numbers for each county fips number in Texas\n",
    "county_nums = [str(i).zfill(3) for i in range(1, 508) if i % 2]\n",
    "\n",
    "# RRC statewide map layers shapefiles\n",
    "SHP_ZIP_URLS = [\n",
    "    f\"https://storage.googleapis.com/mrprime_dataset/capstone_journey/rrc/all_layers_rrc_20231117/Shp{num}.zip\"\n",
    "    for num in county_nums\n",
    "]\n",
    "# RRC descriptive information on the oil and gas wells\n",
    "API_URLS = [\n",
    "    f\"https://storage.googleapis.com/mrprime_dataset/capstone_journey/rrc/statewide_api_rrc_20231205/api{num}.dbf\"\n",
    "    for num in county_nums\n",
    "]\n",
    "\n",
    "# Texas General Land Office (GLO) oil and gas lease data\n",
    "GDB_ZIP_URLS = [\n",
    "    \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/glo/GDB_ActiveLeases.zip\",\n",
    "    \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/glo/GDB_ActiveUnits.zip\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Url for the shapefile for US counties from the Census Bureau's website.\n",
    "CENSUS_COUNTY_MAP_URL = (\n",
    "    \"https://www2.census.gov/geo/tiger/TIGER2022/COUNTY/tl_2022_us_county.zip\"\n",
    ")\n",
    "# Url for a Wikipedia page containing a table of FIPS codes for US counties.\n",
    "FIPS_WIKI_URL = (\n",
    "    \"https://en.wikipedia.org/wiki/List_of_United_States_FIPS_codes_by_county\"\n",
    ")\n",
    "# Bounds of the continental US in longitude and latitude.\n",
    "USA_BOUNDS = (-124.77, 24.52, -66.95, 49.38)\n",
    "# bounds of the continental US in Web Mercator coordinates.\n",
    "USA_BOUNDS_MERCATOR = (-13874905.0, 2870341.0, -7453304.0, 6338219.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function definitions\n",
    "Next, let's define some functions that will be used throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def get_county_data():\n",
    "    county = gpd.read_file(CENSUS_COUNTY_MAP_URL)[\n",
    "        [\"GEOID\", \"STATEFP\", \"COUNTYFP\", \"NAME\", \"geometry\"]\n",
    "    ]\n",
    "    county.columns = county.columns.str.lower()\n",
    "    return county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def read_csv_concurrent(urls_list):\n",
    "    \"\"\"Reads a list of CSV files concurrently\"\"\"\n",
    "    # Create a thread pool\n",
    "    with cf.ThreadPoolExecutor() as executor:\n",
    "        # Use map to apply pd.read_csv to each URL\n",
    "        results = list(tqdm(executor.map(pd.read_csv, urls_list), total=len(urls_list)))\n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def extract_matching_shp_files_from_zip_urls(\n",
    "    zip_urls: list[str], regex_patterns: list[str]\n",
    ") -> dict[str, gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Reads shapefiles from a list of zip file urls and returns a dictionary\n",
    "    where the keys are the names of the shapefiles and the values are GeoDataFrames.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store the GeoDataFrames\n",
    "    shp_dict = {}\n",
    "    # compile the regex patterns\n",
    "    patterns = [re.compile(pattern) for pattern in regex_patterns]\n",
    "\n",
    "    # Loop over the list of zip file urls\n",
    "    for zip_url in tqdm(zip_urls, desc=\"Processing zip files\"):\n",
    "        # download the zip file\n",
    "        with urlopen(zip_url) as u:\n",
    "            zip_data = u.read()\n",
    "        # create a ZipMemoryFile from the zip data\n",
    "        with ZipMemoryFile(zip_data) as z:\n",
    "            # get the list of files in the zip file\n",
    "            zip_files = z.listdir()\n",
    "            # filter for shapefiles that match any of the patterns\n",
    "            shp_files = [\n",
    "                f\n",
    "                for f in zip_files\n",
    "                for pattern in patterns\n",
    "                if pattern.search(f) and f.endswith(\".shp\")\n",
    "            ]\n",
    "            # read the shapefiles into GeoDataFrames\n",
    "            for shp_file in shp_files:\n",
    "                with z.open(shp_file) as f:\n",
    "                    shp_dict[Path(shp_file).stem] = gpd.GeoDataFrame.from_features(\n",
    "                        f, crs=f.crs\n",
    "                    )\n",
    "    # Return the dictionary of GeoDataFrames\n",
    "    return shp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def process_zip_url(\n",
    "    zip_url: str, patterns: list[re.Pattern]\n",
    ") -> Optional[dict[str, gpd.GeoDataFrame]]:\n",
    "    \"\"\"Downloads a zip file url and returns a dictionary of GeoDataFrames for shapefiles that match the patterns\"\"\"\n",
    "\n",
    "    shp_dict = {}\n",
    "\n",
    "    retries = 5\n",
    "    delay = 1\n",
    "\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            with urlopen(zip_url) as u:\n",
    "                zip_data = u.read()\n",
    "            with ZipMemoryFile(zip_data) as z:\n",
    "                zip_files = z.listdir()\n",
    "\n",
    "                shp_files = [\n",
    "                    f\n",
    "                    for f in zip_files\n",
    "                    for pattern in patterns\n",
    "                    if pattern.search(f) and f.endswith(\".shp\")\n",
    "                ]\n",
    "\n",
    "                for shp_file in shp_files:\n",
    "                    with z.open(shp_file) as f:\n",
    "                        shp_dict[Path(shp_file).stem] = gpd.GeoDataFrame.from_features(\n",
    "                            f, crs=f.crs\n",
    "                        )\n",
    "\n",
    "            return shp_dict\n",
    "\n",
    "        except (IncompleteRead, URLError) as e:\n",
    "            if i < retries - 1:\n",
    "                sleep(delay)\n",
    "                delay *= 2\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                logger.error(\n",
    "                    f\"Error: {e} on try {i+1} of {retries} for {zip_url}\")\n",
    "                raise\n",
    "\n",
    "\n",
    "def extract_matching_shp_files_from_zip_urls_concurrent(\n",
    "    zip_urls: list[str], regex_patterns: list[str]\n",
    ") -> dict[str, gpd.GeoDataFrame]:\n",
    "    \"\"\"Reads shapefiles from a list of zip file urls and returns a dictionary where the keys are the names of the shapefiles and the values are GeoDataFrames.\"\"\"\n",
    "\n",
    "    shp_dict = {}\n",
    "\n",
    "    patterns = [re.compile(pattern) for pattern in regex_patterns]\n",
    "\n",
    "    with cf.ThreadPoolExecutor() as executor:\n",
    "        future_to_url = {\n",
    "            executor.submit(process_zip_url, url, patterns): url for url in zip_urls\n",
    "        }\n",
    "\n",
    "        futures = tqdm(\n",
    "            cf.as_completed(future_to_url),\n",
    "            total=len(future_to_url),\n",
    "            desc=\"Processing URLs\",\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "\n",
    "            if result:\n",
    "                shp_dict.update(result)\n",
    "    return shp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def concat_gdf_from_dict(gdf_dict: dict[str, gpd.GeoDataFrame]) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Given a dictionary of GeoDataFrames, returns a single GeoDataFrame\n",
    "    with a new column indicating the source of the data.\n",
    "    \"\"\"\n",
    "    # use a dictionary comprehension to create a new dictionary\n",
    "    gdf_data = {k: gdf.assign(source_file=k) for k, gdf in gdf_dict.items()}\n",
    "    # return the concatenated GeoDataFrame\n",
    "    return pd.concat(gdf_data.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def extract_gdfs_from_zip_url(zip_url: str) -> Optional[dict[str, gpd.GeoDataFrame]]:\n",
    "    \"\"\"\n",
    "    Downloads a ZIP file from a URL, reads shapefiles from the ZIP file, and returns a dictionary of GeoDataFrames.\n",
    "    \"\"\"\n",
    "    gdfs = {}\n",
    "    # Open the URL\n",
    "    with urlopen(zip_url) as u:\n",
    "        # Read the content of the response into a byte stream\n",
    "        zip_data = u.read()\n",
    "        # Open the ZIP file from the byte stream\n",
    "        with ZipMemoryFile(zip_data) as z:\n",
    "            # Get the list of files in the ZIP file\n",
    "            zip_contents = z.listdir()\n",
    "            # Find the shapefiles\n",
    "            shp_files = [f for f in zip_contents if f.endswith(\".shp\")]\n",
    "            for shp_file in shp_files:\n",
    "                # Read the shapefile into a GeoDataFrame\n",
    "                with z.open(shp_file) as f:\n",
    "                    gdf = gpd.GeoDataFrame.from_features(f, crs=f.crs)\n",
    "                gdfs[Path(shp_file).stem] = gdf\n",
    "\n",
    "    # If no shapefile was found, return None\n",
    "    return gdfs if gdfs else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def process_shp_url(zip_url: str):\n",
    "    \"\"\"Downloads a zip file url and returns a dictionary of GeoDataFrames for shapefiles that match the patterns\"\"\"\n",
    "    shp_dict = {}\n",
    "    with urlopen(zip_url) as u:\n",
    "        zip_data = u.read()\n",
    "    with ZipMemoryFile(zip_data) as z:\n",
    "        zip_files = z.listdir()\n",
    "        shp_files = [f for f in zip_files if f.endswith(\".shp\")]\n",
    "        for shp_file in shp_files:\n",
    "            with z.open(shp_file) as f:\n",
    "                shp_dict[Path(shp_file).stem] = gpd.GeoDataFrame.from_features(\n",
    "                    f, crs=f.crs\n",
    "                )\n",
    "    return shp_dict\n",
    "\n",
    "\n",
    "def extract_gdfs_from_zip_url_concurrent(\n",
    "    zip_urls: list[str],\n",
    ") -> dict[str, gpd.GeoDataFrame]:\n",
    "    \"\"\"Reads shapefiles from a list of zip file urls and returns a dictionary\n",
    "    where the keys are the names of the shapefiles and the values are GeoDataFrames.\"\"\"\n",
    "    shp_dict = {}\n",
    "    with cf.ThreadPoolExecutor() as executor:\n",
    "        future_to_url = {executor.submit(process_shp_url, url): url for url in zip_urls}\n",
    "        futures = tqdm(\n",
    "            cf.as_completed(future_to_url),\n",
    "            total=len(future_to_url),\n",
    "            desc=\"Processing URLs\",\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "        for future in futures:\n",
    "            shp_dict.update(future.result())\n",
    "    return shp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def read_gdb_from_zip_url(gdb_urls_list: list[str]):\n",
    "    \"\"\"Reads a list of zip file urls containing geodatabases and returns a dictionary of GeoDataFrames\"\"\"\n",
    "    # initialize an empty dictionary\n",
    "    gdb_dict = {}\n",
    "    # loop through each zip file\n",
    "    for gdb_url in gdb_urls_list:\n",
    "        # create a temporary directory\n",
    "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "            # download the zip file\n",
    "            with urlopen(gdb_url) as u, open(f\"{tmp_dir}/data.zip\", \"wb\") as f_out:\n",
    "                f_out.write(u.read())\n",
    "            # extract the zip file\n",
    "            with ZipFile(f\"{tmp_dir}/data.zip\", \"r\") as zip_ref:\n",
    "                zip_ref.extractall(tmp_dir)\n",
    "            # get the list of extracted files\n",
    "            extracted_files = list(Path(tmp_dir).iterdir())\n",
    "            # filter for gdb folders\n",
    "            gdb_folders = [f for f in extracted_files if f.suffix == \".gdb\"]\n",
    "            # if there is a gdb folder in the extracted files\n",
    "            if gdb_folders:\n",
    "                # get it and read it into a GeoDataFrame\n",
    "                gdb_folder = gdb_folders[0]\n",
    "                gdb_dict[Path(gdb_folder).stem] = gpd.read_file(gdb_folder).to_crs(\n",
    "                    \"EPSG:4269\"\n",
    "                )\n",
    "    # return the dictionary of GeoDataFrames\n",
    "    return gdb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def process_gdb_url(gdb_url):\n",
    "    \"\"\"Downloads a zip file url containing a geodatabase and returns a GeoDataFrame\"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        # download the zip file\n",
    "        with urlopen(gdb_url) as u, open(f\"{tmp_dir}/data.zip\", \"wb\") as f_out:\n",
    "            f_out.write(u.read())\n",
    "        # extract the zip file\n",
    "        with ZipFile(f\"{tmp_dir}/data.zip\", \"r\") as zip_ref:\n",
    "            zip_ref.extractall(tmp_dir)\n",
    "        # get the list of extracted files\n",
    "        extracted_files = list(Path(tmp_dir).iterdir())\n",
    "        # filter for gdb folders\n",
    "        gdb_folders = [f for f in extracted_files if f.suffix == \".gdb\"]\n",
    "        # if there is a gdb folder in the extracted files\n",
    "        if gdb_folders:\n",
    "            # get it and read it into a GeoDataFrame\n",
    "            gdb_folder = gdb_folders[0]\n",
    "            return Path(gdb_folder).stem, gpd.read_file(gdb_folder)\n",
    "\n",
    "\n",
    "def read_gdb_from_zip_url_concurrent(gdb_urls_list: list[str]):\n",
    "    \"\"\"Reads a list of zip file urls containing geodatabases and returns a dictionary of GeoDataFrames\"\"\"\n",
    "    # initialize an empty dictionary\n",
    "    gdb_dict = {}\n",
    "    # create a ThreadPoolExecutor\n",
    "    with cf.ThreadPoolExecutor() as executor:\n",
    "        # submit the process_gdb_url function for each url and gather the results\n",
    "        future_to_url = {\n",
    "            executor.submit(process_gdb_url, url): url for url in gdb_urls_list\n",
    "        }\n",
    "        for future in cf.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                key, data = future.result()\n",
    "                gdb_dict[key] = data\n",
    "            except Exception as exc:\n",
    "                print(f\"{url} generated an exception: {exc}\")\n",
    "    # return the dictionary of GeoDataFrames\n",
    "    return gdb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "def pascal_to_snake(name: str):\n",
    "    \"\"\"Converts a string from PascalCase to snake_case\"\"\"\n",
    "    # (?<=[A-Za-z0-9]) - positive lookbehind for any alphanumeric character\n",
    "    # (?=[A-Z][a-z]) - positive lookahead for any uppercase followed by lowercase\n",
    "    pattern = re.compile(r\"(?<=[A-Za-z0-9])(?=[A-Z][a-z])\")\n",
    "    name = pattern.sub(\"_\", name).lower()\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define func to plot hbar of missing columns\n",
    "def plot_missing_hbars(df, **kwargs):\n",
    "    \"\"\"Plots horizontal bars of the missing data in each column of a dataframe\"\"\"\n",
    "    data = df.copy()\n",
    "    # Calculate the percentage of missing values in each column\n",
    "    missing_data_percent = (data.notna().mean() * 100).rename(\"Percent\")\n",
    "    # Create a DataFrame of the counts of missing values\n",
    "    missing_count = data.notna().sum().rename(\"Count\")\n",
    "    # Concatenate the two DataFrames along the columns\n",
    "    missing_data = pd.concat([missing_data_percent, missing_count], axis=1)\n",
    "    # Create a horizontal bar plot of the percentage of missing data\n",
    "    hbar_plot = missing_data.hvplot.barh(\n",
    "        y=\"Percent\",\n",
    "        width=800,\n",
    "        height=600,\n",
    "        ylabel=\"\",\n",
    "        xlabel=\"\",\n",
    "        xaxis=\"bare\",\n",
    "        hover_cols=\"all\",\n",
    "        **kwargs,\n",
    "    ).opts(\n",
    "        active_tools=[\"box_zoom\"],\n",
    "        toolbar=\"above\",\n",
    "    )\n",
    "    return hbar_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def plot_nonmissing_hbar(df):\n",
    "    # Calculate the percentage of non-missing values in each column\n",
    "    missing_data_percent = (df.notnull().mean()).rename(\"Percent\")\n",
    "\n",
    "    # Create a DataFrame of the counts of non-missing values\n",
    "    if isinstance(df, dd.DataFrame):\n",
    "        non_missing_count, missing_data_percent = dask.compute(\n",
    "            df.count().rename(\"Count\"), (missing_data_percent * 100)\n",
    "        )\n",
    "    else:\n",
    "        missing_data_percent = missing_data_percent * 100\n",
    "        non_missing_count = df.notnull().sum().rename(\"Count\")\n",
    "\n",
    "    # Concatenate the two DataFrames along the columns\n",
    "    non_missing_data = pd.concat([missing_data_percent, non_missing_count], axis=1)\n",
    "\n",
    "    # Create a horizontal bar plot of the percentage of non-missing data\n",
    "    hbar_plot = non_missing_data.hvplot.barh(\n",
    "        y=\"Percent\",\n",
    "        width=800,\n",
    "        height=600,\n",
    "        title=\"Percentage of Non-Missing Data in Each Column\",\n",
    "        ylabel=\"\",\n",
    "        xlabel=\"\",\n",
    "        xaxis=\"bare\",\n",
    "        hover_cols=\"all\",\n",
    "    ).opts(\n",
    "        active_tools=[\"box_zoom\"],\n",
    "        toolbar=\"above\",\n",
    "    )\n",
    "\n",
    "    return hbar_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def unify_crs(\n",
    "    dataframe: pd.DataFrame,\n",
    "    lon_col: str = \"longitude\",\n",
    "    lat_col: str = \"latitude\",\n",
    "    crs_col: str = \"crs\",\n",
    "    final_crs: str = \"EPSG:4269\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a DataFrame with lon/lat or x/y coordinates,\n",
    "    converts the coordinates to a unified crs and combines\n",
    "    into a single GeoDataframe with a geometry column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the main columns that will be used for the conversion\n",
    "    main_cols = [lon_col, lat_col, crs_col]\n",
    "\n",
    "    # Get the other columns in the dataframe\n",
    "    other_cols = list(set(dataframe.columns) - set(main_cols))\n",
    "\n",
    "    # Create a subframe with only the main columns\n",
    "    subframe = dataframe[main_cols]\n",
    "\n",
    "    # Create a list of GeoDataFrames, each with a different CRS\n",
    "    geo_dfs = [\n",
    "        gpd.GeoDataFrame(\n",
    "            # Use the data for this CRS\n",
    "            data=data,\n",
    "            # Create a geometry column from the lon/lat columns\n",
    "            geometry=gpd.points_from_xy(x=data[lon_col].values, y=data[lat_col].values),\n",
    "            # Set the CRS for this GeoDataFrame\n",
    "            crs=pyproj.CRS(crs_val),\n",
    "            # Convert the GeoDataFrame to the final CRS\n",
    "        ).to_crs(final_crs)\n",
    "        # Do this for each unique CRS in the subframe\n",
    "        for crs_val, data in subframe.groupby(crs_col)\n",
    "    ]\n",
    "\n",
    "    # Merge the GeoDataFrames back together and return the result\n",
    "    return pd.merge(\n",
    "        # Concatenate the GeoDataFrames\n",
    "        pd.concat(geo_dfs, sort=True),\n",
    "        # Add the other columns back in\n",
    "        dataframe[other_cols],\n",
    "        # Merge on the index\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# @lru_cache(maxsize=3)\n",
    "def get_background_map(bgcolor=\"black\", alpha=0.5):\n",
    "    \"\"\"Returns a GeoViews background map\"\"\"\n",
    "    return gts.CartoLight().opts(bgcolor=bgcolor, alpha=alpha)\n",
    "\n",
    "\n",
    "def platecaree_to_mercator_vectorised(x, y):\n",
    "    \"\"\"Use Cartopy to convert PlateCarree coordinates to Mercator\"\"\"\n",
    "    return ccrs.GOOGLE_MERCATOR.transform_points(ccrs.PlateCarree(), x, y)[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "def transform_polygon(polygon, src_crs, tgt_crs):\n",
    "    \"\"\"Transform the coordinates of a polygon from src_crs to tgt_crs.\"\"\"\n",
    "    return Polygon(\n",
    "        [tgt_crs.transform_point(x, y, src_crs) for x, y in polygon.exterior.coords]\n",
    "    )\n",
    "\n",
    "\n",
    "def platecaree_to_mercator_polygons(gdf):\n",
    "    \"\"\"Transform the coordinates of all polygons in a GeoDataFrame from PlateCarree to Mercator.\"\"\"\n",
    "    src_crs = ccrs.PlateCarree()\n",
    "    tgt_crs = ccrs.GOOGLE_MERCATOR\n",
    "    gdf[\"geometry\"] = gdf[\"geometry\"].apply(transform_polygon, args=(src_crs, tgt_crs))\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def format_in_000(num):\n",
    "    \"\"\"Formats a number in thousands\"\"\"\n",
    "    for unit in [\"\", \"thousand\", \"million\", \"billion\", \"trillion\"]:\n",
    "        if abs(num) < 1000.0:\n",
    "            return f\"{num:3.2f} {unit}\"\n",
    "        num /= 1000.0\n",
    "    return f\"{num:.2f} quadrillion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def split_datetime(df, column):\n",
    "    \"\"\"Splits a datetime column into year, month, and day columns\"\"\"\n",
    "    column_stem = column.replace(\"_date\", \"\") if \"_date\" in column else column\n",
    "    datetime_series = pd.to_datetime(\n",
    "        df.get(column, pd.Series(dtype=\"float64\")), errors=\"coerce\"\n",
    "    )\n",
    "    if datetime_series.isna().any():\n",
    "        logger.info(f\"Some datetime values did not convert in column {column}.\")\n",
    "    df[column_stem + \"_year\"] = datetime_series.dt.year\n",
    "    df[column_stem + \"_month\"] = datetime_series.dt.month\n",
    "    df[column_stem + \"_day\"] = datetime_series.dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blobs_into_memory(bucket_name, prefix):\n",
    "    \"\"\"Downloads all blobs with a given prefix from a bucket into memory and reads them into GeoDataFrames.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Get all blobs in the bucket that match the prefix\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "\n",
    "    # Create a dictionary to hold the GeoDataFrames\n",
    "    gdfs = {}\n",
    "\n",
    "    # Loop over the blobs and read each one into a GeoDataFrame\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith(\".dbf\"):\n",
    "            contents = blob.download_as_bytes()  # download as bytes\n",
    "            gdf = gpd.read_file(contents)  # read into a GeoDataFrame\n",
    "            gdfs[blob.name] = gdf  # add to the dictionary\n",
    "\n",
    "    return gdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def format_in_B(num):\n",
    "    \"\"\"Formats a number in thousands\"\"\"\n",
    "    for unit in [\"\", \"k\", \"M\", \"G\", \"T\"]:\n",
    "        if abs(num) < 1024:\n",
    "            return f\"{num:3.2f} {unit}B\"\n",
    "        num /= 1024\n",
    "    return f\"{num:.1f} TB\"\n",
    "\n",
    "\n",
    "def get_table_overview_from_frame(table_name_dict, table_name):\n",
    "    \"\"\"Returns an overview of the data in a dask dataframe\"\"\"\n",
    "\n",
    "    # get the dask dataframe from the dictionary\n",
    "    ddf = table_name_dict[table_name]\n",
    "    # get the number of rows and columns\n",
    "    num_rows = ddf.shape[0]\n",
    "    # get the size on disk\n",
    "    size = ddf.memory_usage(deep=True).sum()\n",
    "    # get the column names\n",
    "    columns = ddf.columns.tolist()\n",
    "    # get the column dtypes\n",
    "    dtypes = ddf.dtypes.to_list()\n",
    "    # get the number of partitions\n",
    "\n",
    "    # compute values and put these properties in 1 row of a dataframe\n",
    "    num_rows, size = dask.compute(num_rows, size)\n",
    "    # create a dataframe with the properties\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"table\": [table_name],\n",
    "            \"rows\": [f\"{num_rows:,}\"],\n",
    "            \"cols\": [len(columns)],\n",
    "            \"columns\": [columns],\n",
    "            \"size\": [format_in_B(size)],\n",
    "            \"col_dtypes\": [dtypes],\n",
    "        }\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_table_description_from_frame(table_name_dict, table_name):\n",
    "    \"\"\"Returns a Dask Dataframe with the statistical description of the data in the table\"\"\"\n",
    "    # get the dask dataframe from the dictionary\n",
    "    ddf = table_name_dict[table_name]\n",
    "    # compute the descriptive statistics for the numeric columns\n",
    "    desc_ddf = ddf.describe(include=\"all\")\n",
    "    # add row with value in each of the columns in the describe dataframe for the dtype\n",
    "    ddf_dtypes = dd.from_pandas(\n",
    "        pd.DataFrame(desc_ddf.dtypes.apply(\n",
    "            lambda x: x.name), columns=[\"dtype\"]).T,\n",
    "        npartitions=1,\n",
    "    )\n",
    "    # Return the descriptive statistics as a tuple of Dask dataframes\n",
    "    desc_ddf = dd.concat([desc_ddf, ddf_dtypes])\n",
    "    return desc_ddf\n",
    "\n",
    "\n",
    "def get_data_for_plots(ddf):\n",
    "    \"\"\"takes in a dask dataframe and returns the df data to be used for plotting\"\"\"\n",
    "    # define order for the columns\n",
    "    column_order = [\n",
    "        \"dtype\",\n",
    "        \"count\",\n",
    "        \"unique\",\n",
    "        \"top\",\n",
    "        \"freq\",\n",
    "        \"mean\",\n",
    "        \"std\",\n",
    "        \"min\",\n",
    "        \"25%\",\n",
    "        \"50%\",\n",
    "        \"75%\",\n",
    "        \"max\",\n",
    "    ]\n",
    "    # get the data for the plots\n",
    "    computed_ddf = ddf.compute()\n",
    "    return computed_ddf.T[column_order].sort_values(by=\"dtype\", ascending=False)\n",
    "\n",
    "\n",
    "def plot_stats_table_hbar(ddf, title=\"\"):\n",
    "    \"\"\"Plots a barh plot and a table in a layout.\"\"\"\n",
    "    # state opts of barh plot\n",
    "    hbar_opts = dict(\n",
    "        title=title,\n",
    "        ylabel=\"\",\n",
    "        xlabel=\"\",\n",
    "        xaxis=\"bare\",\n",
    "        tools=[\"hover\"],\n",
    "    )\n",
    "    df = pd.DataFrame()\n",
    "    df = get_data_for_plots(ddf)\n",
    "    # turn the count column into an int dtype column\n",
    "    df[\"count\"] = pd.to_numeric(df[\"count\"]).astype(int)\n",
    "    # round the values of the floats to integers\n",
    "    float_cols = [\"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"]\n",
    "    for col in float_cols:\n",
    "        df[col] = pd.to_numeric(df[col].fillna(\n",
    "            \"0.0\"), errors=\"coerce\").astype(int)\n",
    "    # df[float_cols] = df[float_cols].round(0)\n",
    "    # create a horizontal bartplot of the count column using hvplot\n",
    "    df[\"fraction_nonmissing\"] = round(df[\"count\"] / df[\"count\"].max(), 4)\n",
    "    element_height = (1 + df.shape[0]) * 33\n",
    "    # set the index to the query_field column\n",
    "    table_panel = pnw.Tabulator(df.iloc[::-1], height=element_height)\n",
    "\n",
    "    barh_plot = df.hvplot.barh(\n",
    "        y=\"fraction_nonmissing\", height=element_height, **hbar_opts\n",
    "    ).opts(active_tools=[\"box_zoom\"], toolbar=\"above\")\n",
    "    barh_panel = pn.panel(barh_plot)\n",
    "\n",
    "    # return a panel row with the tabulator table and the bar plot\n",
    "    return pn.Row(\n",
    "        barh_panel,\n",
    "        table_panel,\n",
    "        sizing_mode=\"stretch_width\",\n",
    "    )\n",
    "\n",
    "\n",
    "def display_table_overview_from_frame(\n",
    "    persisted_overviews, table_name_dict, table_name, nrows=10, column_types={}\n",
    "):\n",
    "    \"\"\"Display an overview of the data in a Dask DataFrame.\"\"\"\n",
    "    over_view_table = persisted_overviews[table_name]\n",
    "    # over_view_table = get_table_overview_from_frame(\n",
    "    #     table_name_dict, table_name, column_types\n",
    "    # )\n",
    "\n",
    "    # display the shape of the table with decorative ~*~ around it\n",
    "    num_rows = over_view_table[\"rows\"].values[0]\n",
    "    num_cols = over_view_table[\"cols\"].values[0]\n",
    "    print(f\"~*~ {table_name} ~*~\")\n",
    "    print(f\"{num_rows} rows, {num_cols} columns\")\n",
    "\n",
    "    # get ddf from the dictionary\n",
    "    ddf = table_name_dict[table_name]\n",
    "    # display nrows of the dask dataframe\n",
    "    display(ddf.head(nrows))\n",
    "\n",
    "\n",
    "def get_table_card_from_frame(table_name_dict, table_name):\n",
    "    \"\"\"returns a panel card with the table and the bar plot\"\"\"\n",
    "    results = get_table_description_from_frame(table_name_dict, table_name)\n",
    "    bar_table = plot_stats_table_hbar(results, title=table_name)\n",
    "    return pn.Card(bar_table, title=table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. FracFocus Dataset\n",
    "First dataset is from FracFocus. It contains the readme file, which contains the data dictionary for the dataset, and the 3 data files in csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# get readme data\n",
    "readme = urlopen(DATA_README_URL[0]).read().decode(\"windows-1252\")\n",
    "\n",
    "# We can collect all the dataframe into a list and then concatenate them\n",
    "df_list = read_csv_concurrent(DATA_URLS2)\n",
    "dfs = pd.concat(df_list).reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(readme)\n",
    "print()\n",
    "registry_df = pd.DataFrame()\n",
    "registry_df = dfs.copy()\n",
    "registry_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. US Census bureau and Wikipedia\n",
    "Next dataset to load is the fips geodataframe dataset. This dataset contains the county boundaries for the United States. This geo data can be reliably used as a reference for spatial boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# get the US counties map data and the FIPS codes data\n",
    "\n",
    "\n",
    "county_gdf = get_county_data()\n",
    "fips_df = pd.read_html(FIPS_WIKI_URL)[1]\n",
    "fips_df.columns = [\"geoid\", \"county\", \"state\"]\n",
    "fips_df[\"geoid\"] = fips_df[\"geoid\"].astype(\"string\").str.zfill(5)\n",
    "county_fips_gdf = county_gdf.merge(fips_df, on=\"geoid\")\n",
    "county_fips_gdf.sample(3)\n",
    "\n",
    "\n",
    "county_gdf.info(memory_usage=\"deep\")\n",
    "county_gdf.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Oklahoma Corporation Commission\n",
    "This dataset is from the Oklahoma Corporation Commission. It contains the oil and gas well locations within the state of Oklahoma. It was converted to parquet format for faster loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Read in OK well data\n",
    "\n",
    "\n",
    "occ_wells = pd.read_parquet(OCC_PARQUET_URL)\n",
    "# Convert the WKT column back to a geometry column\n",
    "occ_wells[\"geometry\"] = occ_wells[\"geometry\"].apply(lambda x: wkt.loads(x))\n",
    "# Convert the DataFrame to a GeoDataFrame, specifying the CRS\n",
    "occ_wells = gpd.GeoDataFrame(\n",
    "    occ_wells, geometry=\"geometry\", crs=occ_wells[\"crs\"].iloc[0]\n",
    ")\n",
    "occ_wells.info(memory_usage=\"deep\")\n",
    "# look at sample rows of the dataframe\n",
    "occ_wells.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Railroad Commission of Texas\n",
    "Next few datasets are from the Texas Railroad Commission (RRC). First we have a zip file of all the map layers shape files. There is one for each county, so that is 254 in total in TX. The shapefiles contain various data ranging from point geometry well locations to survey data polygon area. We will use the land survey data, bottom well data, and surface well data which are each in separate shapefiles within the zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# regex patterns to identify which shapefiles to extract\n",
    "# we are grabbing the survey lines, surface wells, and bottom well locations shp files\n",
    "patterns = [r\"surv\\d{3}p\", r\"well\\d{3}s\", r\"well\\d{3}b\"]\n",
    "\n",
    "# Look at the survey lines polygons and the surface wells points. Data saved from RRC website\n",
    "shp_dict = extract_matching_shp_files_from_zip_urls_concurrent(SHP_ZIP_URLS, patterns)\n",
    "\n",
    "# use the patterns to separate the gdf in the dict based on the pattern\n",
    "surv_dict = {k: shp_dict[k] for k, v in shp_dict.items() if re.search(patterns[0], k)}\n",
    "swell_dict = {k: shp_dict[k] for k, v in shp_dict.items() if re.search(patterns[1], k)}\n",
    "bwell_dict = {k: shp_dict[k] for k, v in shp_dict.items() if re.search(patterns[2], k)}\n",
    "\n",
    "# concatenate the gdf from the dict\n",
    "surv_gdf = concat_gdf_from_dict(surv_dict)\n",
    "swell_gdf = concat_gdf_from_dict(swell_dict)\n",
    "bwell_gdf = concat_gdf_from_dict(bwell_dict)\n",
    "\n",
    "# show a sample of the data\n",
    "surv_gdf.info(memory_usage=\"deep\")\n",
    "display(surv_gdf.sample(3))\n",
    "swell_gdf.info(memory_usage=\"deep\")\n",
    "display(swell_gdf.sample(3))\n",
    "bwell_gdf.info(memory_usage=\"deep\")\n",
    "display(bwell_gdf.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also from the RRC is the Statewide API dataset. These files contain descriptive information about oil and gas wells which are maintained by RRC mapping system. These files are organized by map area and contains API number, survey name, well number, plug date, completion date, lease name, and lease I.D. The file is organized by USGS 7.5 minute quadrangle and is sorted by API number within each quadrangle. It is intended to be used along with location information extracted from the well mapping system. Records contained in these files will match the API numbers identified in the quad maps, but may not represent all known API numbers in the map area. They have been unzipped from the original zipfile so are now in `.dbs` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "def load_dbf(url):\n",
    "    try:\n",
    "        dbf = gpd.read_file(url)\n",
    "        return dbf\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read data from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Create a ThreadPoolExecutor\n",
    "with cf.ThreadPoolExecutor() as executor:\n",
    "    # Use the executor to map the load_dbf function to the URLs\n",
    "    futures = [executor.submit(load_dbf, url) for url in API_URLS]\n",
    "\n",
    "    # Initialize an empty list to hold the GeoDataFrames\n",
    "    dbfs = []\n",
    "\n",
    "    # loop over the futures as they complete\n",
    "    for future in tqdm(cf.as_completed(futures), total=len(API_URLS)):\n",
    "        # Get the result from the future\n",
    "        dbf = future.result()\n",
    "\n",
    "        # If the result is not None, add it to the list\n",
    "        if dbf is not None:\n",
    "            dbfs.append(dbf)\n",
    "\n",
    "\n",
    "# Concatenate the GeoDataFrames into a single DataFrame\n",
    "api_df = pd.concat(dbfs).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# show sample of the dataframe\n",
    "api_df.info(memory_usage=\"deep\")\n",
    "api_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, from the RRC is the Production Data Query Dump. This is a complete dump of the Production Data and Historical Ledger databases and includes production from 1993 to current. We have unzipped and converted to `parquet` format.  Parquet is a particular good format for use with `Dask` because it is a columnar storage file format that is optimized to use with big data processing frameworks. Some of these data tables are very large to load into memory so we will leverage the `dask` partitioning capability to be able to read and anlyze the data in chunks. There are 16 data tables in the query dump.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### Production Data Query Dump from RRC\n",
    "\n",
    "| Table | Description|\n",
    "|---|---|\n",
    "|GP_COUNTY | General purpose table that stores county information.|\n",
    "|GP_DATE_RANGE_CYCLE | General purpose table of PDQ data range ( Jan. 1993-current production report month/year). |\n",
    "|GP_DISTRICT | General purpose table that contains district information. |\n",
    "|OG_COUNTY_CYCLE | Contains production report data reported by lease and month (YYYYMM) aggregated by the county in which the wells are located.  |This is an estimate only based on allowables and potentials.\n",
    "|OG_COUNTY_LEASE_CYCLE | Contains production report data reported by lease and month (YYYYMM) aggregated by lease and county in which the wells  |are located. This is an estimate only based on allowables and potentials.\n",
    "|OG_DISTRICT_CYCLE | Contains production report data reported by lease and month (YYYYMM) aggregated by the completion district for the lease ID. |\n",
    "|OG_FIELD_CYCLE | Contains production report data reported by lease and month (YYYYMM) aggregated by the field in which the well(s) for the lease  |are completed.\n",
    "|OG_FIELD_DW | Table of field identifying data. |\n",
    "|OG_LEASE_CYCLE | Contains production report data reported by lease and month (YYYYMM). |\n",
    "|OG_LEASE_CYCLE_DISP | Contains production report disposition data reported by lease and month (YYYYMM). |\n",
    "|OG_OPERATOR_CYCLE | Contains production report data reported by lease and month (YYYYMM) aggregated by the operator of the lease. |\n",
    "|OG_OPERATOR_DW | This table contains identifying operator information. |\n",
    "|OG_REGULATORY_LEASE_DW | This table contains identifying lease information. |\n",
    "|OG_SUMMARY_MASTER_LARGE | Summary table. (Used for query purposes at the operator level) |\n",
    "|OG_SUMMARY_ONSHORE_LEASE | Summary table. (Used for query purposes on the leases in onshore counties) |\n",
    "|OG_WELL_COMPLETION | This table contains identifying well completion information. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dask Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Dask client with specified configuration\n",
    "# This client will use 4 workers, each with 1 thread and a memory limit of 3GB\n",
    "# The Dask diagnostic dashboard will be served at the address \":8788\"\n",
    "client = Client(\n",
    "    n_workers=6, threads_per_worker=2, memory_limit=\"3GB\", dashboard_address=\":8788\"\n",
    ")\n",
    "\n",
    "# Print the link to the Dask diagnostic dashboard\n",
    "# This link can be used to monitor the progress and performance of Dask computations\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data as parquet files from GCS. We were able to bring in the data without loading it on to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Create a client\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# Define the bucket name and the prefix\n",
    "bucket_name = \"mrprime_dataset\"\n",
    "prefix = \"capstone_journey/rrc/processed/\"\n",
    "\n",
    "# Get the bucket\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "# Get the blobs (files) in the bucket that match the prefix\n",
    "blobs = bucket.list_blobs(prefix=prefix)\n",
    "\n",
    "# Get the URLs of the data tables\n",
    "data_tables = set(\n",
    "    blob.name.rsplit(\"/\", 2)[1] for blob in blobs if \"_DATA_TABLE.parquet\" in blob.name\n",
    ")\n",
    "\n",
    "# remove the tables that we don't want to load for the data_tables set\n",
    "data_tables = {\n",
    "    table\n",
    "    for table in data_tables\n",
    "    if all(\n",
    "        x not in table.lower()\n",
    "        for x in [\"og_county\", \"district_data\", \"date_range\", \"disp\", \"master\"]\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "# Load each data table into a separate DataFrame\n",
    "dfs = {\n",
    "    data_table: dd.read_parquet(\n",
    "        f\"gs://{bucket_name}/{prefix}{data_table}/*.parquet\", engine=\"pyarrow\"\n",
    "    )\n",
    "    for data_table in tqdm(data_tables)\n",
    "}\n",
    "dfs = {k.split(\".\")[0]: v for k, v in dfs.items()}\n",
    "\n",
    "data_table_list = list(dfs.keys())\n",
    "short_table = [k.replace(\"_DATA_TABLE\", \"\").lower() for k in data_table_list]\n",
    "\n",
    "table_zip = zip(short_table, data_table_list)\n",
    "\n",
    "\n",
    "# Make the short table names the keys with dfs dict values\n",
    "data_tables_dict = {k: dfs[v] for k, v in table_zip}\n",
    "print(\"Keys for data_table_dict:\")\n",
    "list(data_tables_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only initiate the `Client` and load in the dask dataframes in a dict for now. We will compute the dataframes later when we need to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Texas General Land Office\n",
    "Next dataset is from the Texas General Land Office (GLO). These files contained both the Oil and Gas Leases (active only), managed by the Texas GLO, and Oil & Gas units (active only) which is Oil and Gas pooling agreements managed by the Texas GLO. They are in `gdb` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# get the geodataframe of the active leases\n",
    "\n",
    "\n",
    "active_gdb_dict = read_gdb_from_zip_url_concurrent(GDB_ZIP_URLS)\n",
    "\n",
    "logger.info(\"Dataset request for GLO gdb files complete\")\n",
    "\n",
    "for k, gdf in active_gdb_dict.items():\n",
    "    print(f\"{k}| Shape:{gdf.shape}| CRS:{gdf.crs.to_string()}\")\n",
    "    # gdf.info(memory_usage=\"deep\")\n",
    "    # display(gdf.sample(3))\n",
    "    print()\n",
    "\n",
    "active_units_gdf = active_gdb_dict[\"OAG_Units_Active\"]\n",
    "active_units_gdf.info(memory_usage=\"deep\")\n",
    "display(active_units_gdf.sample(3))\n",
    "active_leases_gdf = active_gdb_dict[\"OAG_Leases_Active\"]\n",
    "active_leases_gdf.info(memory_usage=\"deep\")\n",
    "display(active_leases_gdf.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Energy Information Administration\n",
    "Final dataset is from the Energy Information Administration (EIA). These files contain the shapefiles for the \n",
    "- Permian Basin, \n",
    "- Delaware Sub-Basin: Wolfcamp play boundary (9/4/2018), \n",
    "- Wolfcamp A, B, C, and D play boundaries, \n",
    "- Midland Basin (6/4/2020), \n",
    "- Delaware play boundary (10/8/2019), and \n",
    "- Abo-Yeso, Glorieta-Yeso, and \n",
    "- Spraberry play boundaries (3/11/2016). \n",
    "\n",
    "They are in gdb format. They are just single row GeoDataframe of polygons showing the geographical region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "logger.info(\"Dataset request for EIA shapefiles of formations\")\n",
    "# Use the function 'extract_gdfs_from_zip_url_concurrent' to get GeoDataFrames from the URLs in 'basins_url_list'\n",
    "# This function concurrently downloads and extracts GeoDataFrames from the given URLs\n",
    "basins_dict = extract_gdfs_from_zip_url_concurrent(basins_url_list)\n",
    "\n",
    "logger.info(\"Dataset request for EIA shapefiles of formations complete\")\n",
    "\n",
    "print(f\"Number of sub basins/ geodataframes: {len(basins_dict)}\\n\")\n",
    "\n",
    "for k, gdf in basins_dict.items():\n",
    "    print(f\"{k}| Shape:{gdf.shape}| CRS:{gdf.crs.to_string()}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# Concatenate the GeoDataFrames in 'basins_dict' into a single GeoDataFrame using the function 'concat_gdf_from_dict'\n",
    "basins_gdf = concat_gdf_from_dict(basins_dict)\n",
    "basins_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the missing values it is interesting to see that most missing values are from the `TVD`, `TotalBaseWaterVolume` and `TotalBaseNonWaterVolume`. One reason for this may be found in the data limitations on terms of use on the FracFocus website. It states:\n",
    "-  Disclosures submitted using the FracFocus 1.0 format (January, 2011 to May 31, 2013) will contain only header data. \n",
    "-  Disclosures submitted using the FracFocus 2.0 format (November 2012 to present) will contain both header and chemical data. NOTE: Between November, 2012 and May 31, 2013 disclosures in both 1.0 and 2.0 formats were submitted to the system. \n",
    "-  After May 31, 2013 only disclosures submitted in the 2.0 format were accepted.\n",
    "-  Data submitted appears as it was submitted by the operator or operatorâ€™s authorized agent. FracFocus does not warrant the data in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show barplot to see the percentage of non-missing data in each column\n",
    "plot_missing_hbars(registry_df, title=\"Non Missing Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some of the rows of the dataframe\n",
    "display(registry_df.head(3))\n",
    "display(registry_df.sample(5, random_state=628))\n",
    "display(registry_df.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our first look at a few sample rows some things stick out immediately.\n",
    "1. The dataset may be in chronological order and the values of the `JobStartDate`/`JobEndDate` at both of the extremes may be incorrect.\n",
    "2. There may be an abundance for `StateNumber` `42` if 4 out of the 5 draws of the 200k+ rows drawn at random had a `StateNumber` of `42`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we jump into cleaning the data in the columns, let's make the columns look more pythonic by changing the column names to snake_case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "registry_df.columns = [pascal_to_snake(col) for col in registry_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# get a number for the number of duplicates in the dataframe\n",
    "num_duplicates = registry_df.iloc[:, 1:].duplicated().sum()\n",
    "display(print(f\"Number of duplicates: {num_duplicates}\"))\n",
    "\n",
    "# drop the duplicates\n",
    "registry_df = registry_df.iloc[:, 1:].drop_duplicates()\n",
    "registry_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can remove the columns with only null values. These are the last 2 columns in the dataframe, `source` and `dtmod`. Also we can drop the `total_non_base_water_volume` column since we may not have much need for it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "registry_df = registry_df.drop(\n",
    "    columns=[\"source\", \"dtmod\", \"total_base_non_water_volume\"]\n",
    ")\n",
    "registry_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will fix some of the dtypes of the columns.\n",
    "- Both the `job_start_date` and the `job_end_date` columns are object dtypes, so we will convert those to datetime dtypes and drop the timestamp.\n",
    "- We can also separate out the date components into its various components. This may come in handy for feature engineering later on.\n",
    "- The `projection` column is an object dtype. That can be converted to a string dtype and shorten to `crs` as it represents the Cooordinate Reference System used in the `latitude` and `longitude` columns values. We can dig into what CRS is later on.\n",
    "- The `federal_well` and `indian_well` columns are both boolean type columns. They may be more aptly named as `is_federal_well` and `is_indian_well` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the function on 'job_start_date' and 'job_end_date'\n",
    "split_datetime(registry_df, \"job_start_date\")\n",
    "split_datetime(registry_df, \"job_end_date\")\n",
    "registry_df[[col for col in registry_df.columns if re.search(\"start|end\", col)]].info(\n",
    "    memory_usage=\"deep\"\n",
    ")\n",
    "# show the values which are null still\n",
    "registry_df[\n",
    "    registry_df[[col for col in registry_df.columns if re.search(\"start|end\", col)]]\n",
    "    .isna()\n",
    "    .any(axis=1)\n",
    "].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert 'job_start_date' to datetime format and format it as 'YYYY-MM-DD'\n",
    "registry_df[\"job_start_date\"] = pd.to_datetime(\n",
    "    registry_df[\"job_start_date\"], errors=\"coerce\"\n",
    ").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Convert 'job_end_date' to datetime format and format it as 'YYYY-MM-DD'\n",
    "registry_df[\"job_end_date\"] = pd.to_datetime(\n",
    "    registry_df[\"job_end_date\"], errors=\"coerce\"\n",
    ").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# drop rows with null values in 'job_start_date' and 'job_end_date'\n",
    "# registry_df = registry_df.dropna(subset=[\"job_start_date\", \"job_end_date\"])\n",
    "\n",
    "\n",
    "# Rename some columns for clarity\n",
    "registry_df.rename(\n",
    "    columns={\n",
    "        \"federal_well\": \"is_federal_well\",\n",
    "        \"indian_well\": \"is_indian_well\",\n",
    "        \"projection\": \"crs\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Display the information of the DataFrame\n",
    "registry_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will look at the `api_number` column.\n",
    "We learned from the read me that \n",
    "> APINumber - The American Petroleum Institute well identification number formatted as follows xx-xxx-xxxxx0000 Where: \n",
    "> - First two digits represent the state, \n",
    "> - second three digits represent the county, \n",
    "> - third 5 digits represent the well.\n",
    "\n",
    "Theoretically, we could just grab the first two characters of the `APINnumber` and use that as the state number according to the definition of the `APINumber` above. Actually, that would not be a good idea, and here is why.<br>\n",
    "Although the column was called `APINumber`, it is not actually a number, so if it starts with a leading `0` that first character `0`, cannot be omitted from the value. Let's look at some of the rows with a single digit state numbers.\n",
    "\n",
    "Right now, \n",
    "- the `api_number` column is an object dtype, but a better option would be a `string` dtype, as `object` dtype can be mixed . We can also shorten that column name to `api`.\n",
    "- the `state_number` column and the `county_number` column are both `int64` dtypes right now. `string` type may be a stronger option.\n",
    "- `state_code` and `county_code` may be better names for the `state_number` and `county_number` columns respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows where the state_number is a single digit\n",
    "registry_df[\n",
    "    (registry_df[\"state_number\"] == 3) | (registry_df[\"state_number\"] == 5)\n",
    "].sample(5, random_state=628)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rows' `api_number` values have leading `0`, which is correct, but some do not. The rows without the leading `0` though are 13 characters long instead of 14. Maybe we can just add a leading `0` where needed until all API number values are 14 characters long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of characters in the api_number column\n",
    "registry_df[\"api_number\"].astype(\"string\").str.len().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most are 14 characters long, but some are 13 characters long, like the ones we saw above without the leading `0`. Let's assume the ones with 13 characters are missing the leading `0` and not something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert 'api' to string and pad it with zeros to make it 14 characters long\n",
    "registry_df[\"api\"] = registry_df[\"api_number\"].astype(\"string\").str.zfill(14)\n",
    "\n",
    "# Convert 'state_number' to string and pad it with zeros to make it 2 characters long\n",
    "registry_df[\"state_code\"] = registry_df[\"state_number\"].astype(\"string\").str.zfill(2)\n",
    "\n",
    "# Convert 'county_number' to string and pad it with zeros to make it 3 characters long\n",
    "registry_df[\"county_code\"] = registry_df[\"county_number\"].astype(\"string\").str.zfill(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# check which rows may have the api with the first two digits not matching the state number\n",
    "api_state_mismatch_mask = registry_df[\"state_code\"] != registry_df[\"api\"].str[0:2]\n",
    "# api_state_mismatch_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which rows may have the api with the first two digits not matching the state number\n",
    "registry_df[api_state_mismatch_mask][\n",
    "    [\"api_number\", \"api\", \"state_code\", \"state_name\", \"county_code\", \"county_name\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expected to get 2 rows here, since we checked the length of the `api_number` column above we saw that 1 row had 10 and another row had 12 characters. It is only two rows, so this may be an easy fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove leading zeros and pad to 14 digits on mismatches\n",
    "registry_df.loc[api_state_mismatch_mask, \"api\"] = (\n",
    "    registry_df.loc[api_state_mismatch_mask, \"api\"].str.lstrip(\"0\").str.ljust(14, \"0\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which rows may have the api with the first two digits not matching the state number\n",
    "registry_df[api_state_mismatch_mask][\n",
    "    [\"api_number\", \"api\", \"state_code\", \"state_name\", \"county_code\", \"county_name\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# check which rows may have the api with the 3-5 digits not matching the county number\n",
    "api_county_mismatch_mask = registry_df[\"county_code\"] != registry_df[\"api\"].str[2:5]\n",
    "registry_df[api_county_mismatch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State name should not have more than 50 possible values, given that there are only 50 states in the US. If we were to check the number of unique values in the `state_name` column, we would see 95. This is due to the variation in the way the `state_name` value is entered. Although not as obvious, we can assume the same for the `county_name` column. Luckily, the `api` includes both the `state_number` and the `county_number`. With this we can do \n",
    "1. data validation ensuring that these corresponding columns match\n",
    "2. Ensure that the `state_name` and the `county_name` columns are correct. Important to note that \n",
    "> The state codes used in an API number are DIFFERENT from another standard which is the Federal Information Processing Standard (FIPS) state code established in 1987 by NIST. ([source](https://en.wikipedia.org/wiki/API_well_number#State_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'Number of different values in state_name column: {registry_df[\"state_name\"].nunique()}'\n",
    ")\n",
    "print(\n",
    "    f'Number of different values in state_number column: {registry_df[\"state_number\"].nunique()}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# group by state_code and find the mode of the state_name\n",
    "state_code_mode = (\n",
    "    registry_df.groupby(\"state_code\")[\"state_name\"]\n",
    "    .apply(lambda x: x.mode().iloc[0])\n",
    "    .reset_index()\n",
    ")\n",
    "state_code_mode = state_code_mode.rename(columns={\"state_name\": \"state\"})\n",
    "state_code_mode.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "registry_df = registry_df.merge(state_code_mode.rename(columns={\"state_name\": \"state\"}))\n",
    "\n",
    "\n",
    "registry_df.sample(3, random_state=628)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for colors\n",
    "colors = {state: \"gray\" for state in registry_df[\"state\"].unique()}\n",
    "colors[\"Texas\"] = \"salmon\"\n",
    "\n",
    "state_grouped = registry_df.groupby(\"state\").count()\n",
    "state_grouped[\"bar_color\"] = state_grouped.index.map(colors)\n",
    "\n",
    "state_grouped.hvplot.barh(\n",
    "    y=\"state_code\",\n",
    "    color=\"bar_color\",\n",
    "    height=600,\n",
    "    width=800,\n",
    "    title=\"Well Count\",\n",
    "    xlabel=\"\",\n",
    "    ylabel=\"\",\n",
    "    xaxis=\"bare\",\n",
    ").opts(\n",
    "    active_tools=[\"box_zoom\"],\n",
    "    toolbar=\"above\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus our efforts in the most recent 10 years. We will also put our focus in one specific area,Texas. Texas is the leading oil and gas producing state in the United States, is the most active area of exploration and production in the US presently, and understanding the drilling activity in Texas most relevant to the energy future of the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# registry_df = registry_df.drop([\"colors\", \"bar_color\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# create mask for from 2013 onwards\n",
    "post_2012_mask = registry_df[\"job_start_date\"] >= \"2013-01-01\"\n",
    "registry_df_post_2012 = registry_df[post_2012_mask].copy()\n",
    "\n",
    "# find all the rows with null values\n",
    "null_mask = registry_df_post_2012.isna().any(axis=1)\n",
    "registry_df_post_2012[null_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how many nans we still have in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# registry_df_post_2012.info(memory_usage=\"deep\")\n",
    "registry_df_post_2012.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the `county_number` for the rows with a nan value in the `county_name` column, we can see why there is a nan for the `county_name`. Those numbers are most likely incorrect as small states like `North Dakota` and `Arkansas` do not have large `county_number` values. However we can still try to impute what the correct values by cross referencing with other sources or by using the `latitude` and `longitude` values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the rows with a null value for the county_name column\n",
    "registry_df_post_2012[registry_df_post_2012[\"county_name\"].isna()].style.highlight_null(\n",
    "    color=\"#62862828\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# get the index of one of the rows with a null value for the county_name column (3rd one down)\n",
    "index_vanorsdale = registry_df_post_2012.query(\n",
    "    \"api_number == '03729439000000'\").index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oklahoma Commission Corporation (OOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some search engine investigating, we can learn that WFD Oil Corporation is a PC in Oklahoma. We also learn that the well name is `VANORSDOL` ,the well number is `#1-29`, and the API number is `3503729439` `0000`. We can correct some of the data which was entered incorrectly in FracFocus.\n",
    "\n",
    "The data we are looking for is in this markdown cell so we can manually input it in but we will do it through code instead. We will query the api number in the data we have on the wells in OK.\n",
    "\n",
    "| Column Name | Value |\n",
    "| --- | --- |\n",
    "API\t|3503729439\n",
    "WELL_NAME|\tVANORSDOL\n",
    "WELL_NUM|\t#1-29\n",
    "OPERATOR|\tWFD OIL CORPORATION\n",
    "WELLSTATUS|\tAC\n",
    "WELLTYPE|\tOIL\n",
    "SH_LAT\t|35.749381\n",
    "SH_LON\t|-96.370355\n",
    "COUNTY\t|CREEK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the 'api' column to string\n",
    "occ_wells[\"api\"] = occ_wells[\"api\"].astype(\"int64\").astype(\"string\")\n",
    "occ_wells.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# make a copy of the well_name column\n",
    "registry_df_post_2012[\"well\"] = registry_df_post_2012[\"well_name\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# query the well_name column for 'vanors\n",
    "# occ_wells[occ_wells[\"well_name\"].str.contains(\"vanors\", case=False, na=False)]\n",
    "vanorsdol_row = occ_wells.query(\n",
    "    'well_name.fillna(\"\").str.contains(\"vanors\", case=False) & (api == \"3503729439\")',\n",
    ")[\n",
    "    [\n",
    "        \"api\",\n",
    "        \"well_name\",\n",
    "        \"well_num\",\n",
    "        \"operator\",\n",
    "        \"sh_lat\",\n",
    "        \"sh_lon\",\n",
    "        \"county\",\n",
    "    ]\n",
    "].rename(\n",
    "    columns={\"sh_lat\": \"latitude\", \"sh_lon\": \"longitude\"}\n",
    ")\n",
    "vanorsdol_row[\"well\"] = (\n",
    "    vanorsdol_row[\"well_name\"].str.title()\n",
    "    + \" \"\n",
    "    + vanorsdol_row[\"well_num\"].astype(\"string\")\n",
    ")\n",
    "index_vanorsdol = vanorsdol_row.index\n",
    "\n",
    "columns_to_replace = [\"api\", \"well\", \"latitude\", \"longitude\"]\n",
    "for col in columns_to_replace:\n",
    "    registry_df_post_2012.loc[index_vanorsdale, col] = vanorsdol_row.loc[\n",
    "        index_vanorsdol, col\n",
    "    ].values\n",
    "\n",
    "# check that the values have been replaced\n",
    "registry_df_post_2012.loc[index_vanorsdale, columns_to_replace]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the api column to 14 characters again as the OK api had 10 characters\n",
    "registry_df_post_2012[\"api\"] = registry_df_post_2012[\"api\"].str.ljust(14, \"0\")\n",
    "\n",
    "registry_df_post_2012[registry_df_post_2012[\"county_name\"].isna()].style.highlight_null(\n",
    "    color=\"#62862828\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with `latitude` and `longitude` coordinates for all 4 rows with missing `county_name`, let's find out which counties they belong to spatially.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geodataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the boundary coordinates for all the counties in the US from the [census.gov](https://www.census.gov/) website. We loaded this data in earlier as `county_gdf.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "county_gdf.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick note about `GeoDataFrames`: they must have a column called `geometry` and this column contains the geometric objects. This call is what enables `geopandas` to perform spatial operations, and can also contain certain attributes like `.crs` which is the coordinate reference system.\n",
    "\n",
    "\n",
    "Commonly used datums in North America are NAD27, NAD83, and WGS84. More info [here](https://webhelp.esri.com/arcgisdesktop/9.3/index.cfm?TopicName=Projection_basics_the_GIS_professional_needs_to_know).<br>\n",
    "\n",
    "The county geodataframe uses `EPSG:4269` which is the EPSG code for the NAD83 coordinate system. Let's create a geodataframe with the `latitude` and `longitude` values that we have and put all of the points to the same CRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_fips_gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# ensures each row of the geodataframe is in the same CRS\n",
    "registry_gdf = unify_crs(registry_df_post_2012, crs_col=\"crs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# We can now perform a spatial join on the 2 GeoDataFrame.\n",
    "# fitler for rows with null county_name\n",
    "\n",
    "joined_gdf = (\n",
    "    registry_gdf[registry_gdf[\"county_name\"].isna()]\n",
    "    .sjoin(county_fips_gdf.drop(columns=[\"county\"]), how=\"left\", predicate=\"intersects\")\n",
    "    .drop(columns=[\"index_right\"])\n",
    ")\n",
    "joined_gdf.style.map(\n",
    "    lambda x: \"background-color: #62862828\",\n",
    "    subset=[\n",
    "        \"name\",\n",
    "        \"county_name\",\n",
    "        \"state_right\",\n",
    "        \"state_left\",\n",
    "        \"county_code\",\n",
    "        \"countyfp\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `North Dakota` `county_number` should be `061`, which is `Mountrail` county, not `610`. \n",
    "- The `Utah` `county_number` though was actually correct. The error was the state number which should have been `42`, not `43`. This error is somewhat significant as according to the data dictionary:\n",
    "> APINumber - The American Petroleum Institute well identification number formatted as follows xx-xxx-xxxxx0000 Where: First two digits \n",
    "represent the state, second three digits represent the county, third 5 digits represent the well.<br>\n",
    "\n",
    "All this means is the `api` number is also incorrect. It should be `42317428660000` (<u><b>42</b></u>-317-42866-0000) instead of `43317428660000` (<u><b>43</b></u>-317-42866-0000).<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's corect theos values putting the county_code and\n",
    "registry_gdf[\"county\"] = registry_gdf[\"county_name\"].copy()\n",
    "\n",
    "# replace the county_code and county columns with the values from the joined_gdf\n",
    "registry_gdf.loc[joined_gdf.index, \"county\"] = joined_gdf[\"name\"]\n",
    "registry_gdf.loc[joined_gdf.index, \"county_code\"] = joined_gdf[\"countyfp\"]\n",
    "\n",
    "# change the api column of the last row in joined_gdf to 42317428660000 instead of 43317428660000\n",
    "# registry_gdf.loc[joined_gdf.index[-1], \"api\"] = \"42317428660000\"\n",
    "registry_gdf[\"api\"] = registry_gdf[\"api\"].replace(\n",
    "    \"43317428660000\", \"42317428660000\")\n",
    "# correct the state_code values for where the api was changed\n",
    "registry_gdf[\"state_code\"] = registry_gdf[\"api\"].str[0:2]\n",
    "# Create a mapping from 'state_code' to 'state'\n",
    "state_mapping = state_code_mode.set_index(\"state_code\")[\"state\"].to_dict()\n",
    "\n",
    "# Use the mapping to update the 'state' column in 'registry_gdf'\n",
    "registry_gdf[\"state\"] = registry_gdf[\"state_code\"].map(state_mapping)\n",
    "\n",
    "\n",
    "# check that the values have been replaced\n",
    "registry_gdf[registry_gdf[\"county_name\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to impute the missing `county_name` values would have been by using the `well_name` in the `registry_df_post_2012` dataframe. Assuming the other wells on the same pad has the correct `state_code` and `state` values. However, this may not have worked with `Vanorsdol` as there's only one well with that name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_df_post_2012[\n",
    "    registry_df_post_2012[\"well_name\"].str.contains(\n",
    "        \"vanors\", case=False, na=False)\n",
    "][[\"well_name\", \"operator_name\", \"api\", \"state\", \"county_code\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show other wells with a similar name to rhea 1-6\n",
    "registry_df_post_2012[\n",
    "    registry_df_post_2012[\"well\"].str.contains(\n",
    "        \"Rhea 1-6\", case=False, na=False)\n",
    "][[\"well_name\", \"operator_name\", \"api\", \"state\", \"county_code\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_df_post_2012[\n",
    "    registry_df_post_2012[\"well\"].str.contains(\"Trulson\", case=False)\n",
    "][[\"well_name\", \"operator_name\", \"api\", \"state\", \"county_code\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# county_fips_gdf[county_fips_gdf[\"state\"].isin(permian_states)]\n",
    "registry_df_post_2012[\n",
    "    registry_df_post_2012[\"well\"].str.contains(\"palermo\", case=False)\n",
    "][[\"well_name\", \"operator_name\", \"api\", \"state\", \"county_code\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# make the well column uppercase to lower variation among for wells on the same pad\n",
    "registry_gdf[\"well\"] = registry_gdf[\"well\"].str.upper()\n",
    "# make the operator column uppercase to lower the variation among entry for the same operator\n",
    "registry_gdf[\"operator\"] = registry_gdf[\"operator_name\"].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we did for those 4 wells to find the `county` and `state` for which they belong to, we can do that for all the wells. Let's see which well have a different `county` and `state` than what their coordinates suggest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# create 2 new columns, lat and lon in the registry_gdf for corrections\n",
    "registry_gdf[\"lat\"] = registry_gdf[\"latitude\"].copy()\n",
    "registry_gdf[\"lon\"] = registry_gdf[\"longitude\"].copy()\n",
    "\n",
    "# Check which other wells may have a state value which did not agree with the spatial join result\n",
    "registry_county_gdf = registry_gdf.sjoin(\n",
    "    county_fips_gdf, how=\"left\", predicate=\"intersects\"\n",
    ").drop(columns=[\"index_right\"])\n",
    "\n",
    "# create a list of columns to use to check for mismatches\n",
    "trimmed_column_set = [\n",
    "    \"api\",\n",
    "    \"well\",\n",
    "    \"state_left\",\n",
    "    \"state_right\",\n",
    "    \"county_left\",\n",
    "    \"county_right\",\n",
    "    \"county_code\",\n",
    "    \"countyfp\",\n",
    "    \"operator\",\n",
    "    \"lon\",\n",
    "    \"lat\",\n",
    "    \"geometry\",\n",
    "]\n",
    "\n",
    "# rows which the spatial join did not match for the both dataframes\n",
    "mismatch_geo = registry_county_gdf[\n",
    "    registry_county_gdf[\"state_left\"] != registry_county_gdf[\"state_right\"]\n",
    "]\n",
    "print(f\"Number of rows: {len(mismatch_geo)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ones with a nan in the `state_right` column are those that we could not find a match for based on the `geometry`. We can match those for OK using the `api` number and see if the coordinates match for the wells in FracFocus match those from the OCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# get the rows with Oklahoma in the state_left column\n",
    "mismatch_geo_ok = mismatch_geo.query('state_left.str.contains(\"Oklahoma\")')\n",
    "print(f\"Number of rows from OK state: {len(mismatch_geo_ok)}\")\n",
    "if len(mismatch_geo_ok) > 3:\n",
    "    display(mismatch_geo_ok.sample(3, random_state=628))\n",
    "else:\n",
    "    display(\"No rows from OK state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# Store the original index in a new column\n",
    "mismatch_geo_ok[\"original_index\"] = mismatch_geo_ok.index\n",
    "# alter api to match the format in the occ_wells dataframe\n",
    "mismatch_geo_ok[\"ok_api\"] = mismatch_geo_ok[\"api\"].str[:10]  # ok for Oklahoma\n",
    "# drop the api column\n",
    "mismatch_geo_ok.drop(columns=[\"api\"], inplace=True)\n",
    "# rename the api column in occ_wells to ok_api\n",
    "occ_wells.rename(columns={\"api\": \"ok_api\"}, inplace=True)\n",
    "# look for the api in the occ_wells dataframe and merge that row to mismatch_geo_ok\n",
    "joined_mismatch_ok = mismatch_geo_ok.merge(\n",
    "    occ_wells[\n",
    "        [\"ok_api\", \"well_name\", \"well_num\", \"operator\", \"sh_lat\", \"sh_lon\", \"county\"]\n",
    "    ],\n",
    "    how=\"left\",\n",
    "    on=\"ok_api\",\n",
    ")\n",
    "joined_mismatch_ok[[\"geometry\", \"latitude\", \"longitude\", \"sh_lat\", \"sh_lon\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# replace the lat and lon values with the sh_lat and sh_lon values\n",
    "# Replace the 'lat' and 'lon' values\n",
    "joined_mismatch_ok.loc[\n",
    "    joined_mismatch_ok[\"sh_lat\"].notna(), \"lat\"\n",
    "] = joined_mismatch_ok[\"sh_lat\"]\n",
    "joined_mismatch_ok.loc[\n",
    "    joined_mismatch_ok[\"sh_lon\"].notna(), \"lon\"\n",
    "] = joined_mismatch_ok[\"sh_lon\"]\n",
    "\n",
    "joined_mismatch_ok.set_index(\"original_index\", inplace=True)\n",
    "\n",
    "# occ_wells[occ_wells[\"api\"].isin(mismatch_geo_ok[\"ok_api\"])][\n",
    "#     [\"api\", \"well_name\", \"well_num\", \"operator\", \"sh_lat\", \"sh_lon\", \"county\"]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# Update 'lat' and 'lon' in the original DataFrame\n",
    "registry_gdf.loc[joined_mismatch_ok.index, \"lat\"] = joined_mismatch_ok[\"lat\"]\n",
    "registry_gdf.loc[joined_mismatch_ok.index, \"lon\"] = joined_mismatch_ok[\"lon\"]\n",
    "# registry_gdf\n",
    "\n",
    "\n",
    "# Update 'geometry' in the original DataFrame\n",
    "registry_gdf.loc[joined_mismatch_ok.index, \"geometry\"] = [\n",
    "    Point(xy) for xy in zip(joined_mismatch_ok.lon, joined_mismatch_ok.lat)\n",
    "]\n",
    "# Check which other wells may have a state value which did not agree with the spatial join result again\n",
    "registry_county_gdf = registry_gdf.sjoin(\n",
    "    county_fips_gdf, how=\"left\", predicate=\"intersects\"\n",
    ").drop(columns=[\"index_right\"])\n",
    "\n",
    "# mismatch_geo = registry_county_gdf[registry_county_gdf[\"state_right\"].isna()]\n",
    "\n",
    "mismatch_geo = registry_county_gdf[\n",
    "    registry_county_gdf[\"state_left\"] != registry_county_gdf[\"state_right\"]\n",
    "]\n",
    "print(f\"Number of rows with mismatched state values: {mismatch_geo.shape[0]}\")\n",
    "# mismatch_geo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25 rows had incorrect Coordinates for OK. We can do the same for Texas. The `state_left` column is the state from the FracFocus data and the `state_right` column is the state from the `county_fips_gdf` geodataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# query for the wells in Texas\n",
    "mismatch_geo_tx = mismatch_geo[trimmed_column_set].query(\n",
    "    'state_left.str.contains(\"Texas\") | state_right.str.contains(\"Texas\")'\n",
    ")\n",
    "# store the original index in a new column\n",
    "mismatch_geo_tx[\"original_index\"] = mismatch_geo_tx.index\n",
    "\n",
    "mismatch_geo_tx[\"tx_api\"] = mismatch_geo_tx[\"api\"].str[2:10]\n",
    "\n",
    "# All those with stae_left='Texas' but state_right=(something else),\n",
    "# those have the geo coords wrong as they were not found in the county_fips_gdf\n",
    "# For those, let's assume that the api is correct, can we match those correct api with the RRC well data?\n",
    "mismatch_geo_tx.loc[\n",
    "    :,\n",
    "    [\n",
    "        \"geometry\",\n",
    "        \"api\",\n",
    "        \"well\",\n",
    "        \"state_left\",\n",
    "        \"state_right\",\n",
    "        \"county_left\",\n",
    "        \"county_right\",\n",
    "        \"county_code\",\n",
    "        \"countyfp\",\n",
    "    ],\n",
    "]\n",
    "# len(mismatch_geo_tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texas Railroad Commission (RRC) - Surface and bottom hole well data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data from the RRC is already loaded in . We will compare it to the surface well data and/or the bottom well data. First we will remind ourselves what these look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Surface well geodataframe shape:\", swell_gdf.shape)\n",
    "display(swell_gdf.sample(3))\n",
    "print(f\"Bottom well geodataframe shape:\", bwell_gdf.shape)\n",
    "display(bwell_gdf.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth nothing here is that although these are well, they are not all oil and gas wells. Some are water wells, injection wells, etc. We can tell the type of well by lookin gat the `symnum` column which we should make more pythonic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O&G well symnum is a number that indicates the type of well simplified for fewer bins\n",
    "wells_symnum_dict = defaultdict(lambda: \"Other\")\n",
    "well_symnum_dict = {\n",
    "    2: \"Permitted Location\",\n",
    "    3: \"Dry Hole\",\n",
    "    4: \"Oil/Gas\",  # oil\n",
    "    5: \"Oil/Gas\",  # gas\n",
    "    6: \"Oil/Gas\",  # oil/ gas\n",
    "    7: \"Plugged/Shut-in\",  # oil\n",
    "    8: \"Plugged/Shut-in\",  # gas\n",
    "    9: \"Canceled Location\",\n",
    "    10: \"Plugged/Shut-in\",\n",
    "    11: \"Injection/Disposal\",\n",
    "    12: \"Core Test\",\n",
    "    17: \"Storage\",  # oil\n",
    "    18: \"Storage\",  # gas\n",
    "    19: \"Plugged/Shut-in\",  # oil\n",
    "    20: \"Plugged/Shut-in\",  # gas\n",
    "    21: \"Injection/Disposal\",  # oil\n",
    "    22: \"Injection/Disposal\",  # gas\n",
    "    23: \"Injection/Disposal\",  # oil/ gas\n",
    "    73: \"Brine Mining\",\n",
    "    74: \"Water Supply\",\n",
    "    75: \"Water Supply\",  # oil\n",
    "    76: \"Water Supply\",  # gas\n",
    "    77: \"Water Supply\",  # oil/ gas\n",
    "    86: \"Horizontal\",  # Horizontal Well Surface Location\",\n",
    "    87: \"Horizontal\",  # Directional/Sidetrack Well Surface Location,\n",
    "    88: \"Storage\",\n",
    "    103: \"Storage\",  # oil/gas\n",
    "}\n",
    "# well_symnum_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swell_gdf = concat_gdf_from_dict(swell_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column names to snake case for consistency\n",
    "swell_gdf.columns = [pascal_to_snake(col) for col in swell_gdf.columns]\n",
    "\n",
    "# get the county code from the source_file column\n",
    "swell_gdf[\"county_code\"] = swell_gdf[\"source_file\"].str.extract(r\"(\\d{3})\")\n",
    "\n",
    "# map the dictionary to the SYMNUM column and fill the rare values with 'Other'\n",
    "swell_gdf[\"well_type\"] = swell_gdf[\"symnum\"].map(\n",
    "    well_symnum_dict).fillna(\"Other\")\n",
    "\n",
    "# Display a sample of 3 rows from the DataFrame\n",
    "display(swell_gdf.sample(3))\n",
    "\n",
    "\n",
    "# Convert the column names to snake case for consistency\n",
    "bwell_gdf.columns = [pascal_to_snake(col) for col in bwell_gdf.columns]\n",
    "# get the county code from the source_file column\n",
    "bwell_gdf[\"county_code\"] = bwell_gdf[\"source_file\"].str.extract(r\"(\\d{3})\")\n",
    "\n",
    "# map the dictionary to the SYMNUM column and fill the rare values with 'Other'\n",
    "bwell_gdf[\"well_type\"] = bwell_gdf[\"symnum\"].map(\n",
    "    well_symnum_dict).fillna(\"Other\")\n",
    "\n",
    "# Display a sample of 3 rows from the DataFrame\n",
    "display(bwell_gdf.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swell_gdf[\"symnum\"].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barh_opts = dict(\n",
    "    height=400,\n",
    "    width=600,\n",
    "    xlabel=\"\",\n",
    "    ylabel=\"\",\n",
    "    xaxis=\"bare\",\n",
    "    hover_cols=\"all\",\n",
    ")\n",
    "\n",
    "# create a plot the well types on a horizontal bar chart\n",
    "\n",
    "\n",
    "swell_barh = (\n",
    "    swell_gdf[\"well_type\"]\n",
    "    .value_counts()\n",
    "    .hvplot.barh(**barh_opts, title=\"Surface Well Types Count\")\n",
    "    .opts(\n",
    "        active_tools=[\"box_zoom\"],\n",
    "        toolbar=\"above\",\n",
    "    )\n",
    ")\n",
    "bwell_barh = (\n",
    "    bwell_gdf[\"well_type\"]\n",
    "    .value_counts()\n",
    "    .hvplot.barh(**barh_opts, title=\"Bottom Well Types Count\")\n",
    "    .opts(\n",
    "        active_tools=[\"box_zoom\"],\n",
    "        toolbar=\"above\",\n",
    "    )\n",
    ")\n",
    "# compare the surface dataset and the bottom hole dataset side by side\n",
    "swell_barh + bwell_barh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swell_barh.data[\"well_type\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a lot of dry holes. Let's see if they are in any particular location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swell_gdf.to_crs(\"EPSG:4326\", inplace=True)\n",
    "sxsy = platecaree_to_mercator_vectorised(\n",
    "    swell_gdf[\"geometry\"].x, swell_gdf[\"geometry\"].y\n",
    ")\n",
    "swell_gdf[\"x\"] = sxsy[:, 0]\n",
    "swell_gdf[\"y\"] = sxsy[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the background map\n",
    "bg_map = get_background_map()\n",
    "# get the unique values in the symnum column\n",
    "# well_types = swell_gdf[\"well_type\"].unique()\n",
    "well_types = swell_barh.data[\"well_type\"].tolist()\n",
    "\n",
    "\n",
    "# Define a list of colors\n",
    "random_color_list = [\n",
    "    \"#\" + \"\".join([random.choice(\"0123456789ABCDEF\") for j in range(6)])\n",
    "    for i in range(len(well_types))\n",
    "]\n",
    "\n",
    "# Define a dictionary mapping well types to colors\n",
    "well_type_colors = {\n",
    "    well_type: color for well_type, color in zip(well_types, random_color_list)\n",
    "}\n",
    "\n",
    "\n",
    "# Define common options\n",
    "common_opts = dict(\n",
    "    legend_position=\"right\",\n",
    "    tools=[\"hover\", \"tap\"],\n",
    "    show_legend=True,\n",
    "    click_policy=\"hide\",\n",
    "    active_tools=[\"box_zoom\"],\n",
    "    toolbar=\"above\",\n",
    "    height=400,\n",
    "    width=600,\n",
    ")\n",
    "\n",
    "\n",
    "# Define a function to return the points for the well type\n",
    "def get_points(well_type, pt_size):\n",
    "    points = gv.Points(\n",
    "        swell_gdf[swell_gdf[\"well_type\"] == well_type][[\"well_type\", \"x\", \"y\", \"api\"]],\n",
    "        [\"x\", \"y\"],\n",
    "        [\"well_type\", \"api\"],\n",
    "        crs=ccrs.GOOGLE_MERCATOR,\n",
    "    ).opts(\n",
    "        size=pt_size,\n",
    "        color=\"well_type\",\n",
    "        cmap=well_type_colors,\n",
    "        title=f\"{well_type}: {len(swell_gdf[swell_gdf['well_type'] == well_type]):,} wells\",\n",
    "        tools=[\"hover\", \"tap\"],\n",
    "    )\n",
    "    return (bg_map * points).opts(\n",
    "        **common_opts,\n",
    "    )\n",
    "\n",
    "\n",
    "# Int slider for size\n",
    "pt_size_slider = pnw.IntSlider(\n",
    "    name=\"Point Size\",\n",
    "    start=1,\n",
    "    end=10,\n",
    "    step=1,\n",
    "    value=2,\n",
    "    width=200,\n",
    "    align=\"end\",\n",
    ")\n",
    "# Create a DiscreteSlider widget for the well types\n",
    "well_type_slider = pn.widgets.DiscreteSlider(name=\"Well Type\", options=well_types)\n",
    "\n",
    "# Bind the function to the slider\n",
    "swell_panel = pn.panel(\n",
    "    pn.bind(get_points, well_type=well_type_slider, pt_size=pt_size_slider)\n",
    ")\n",
    "# hbar plot of well_types\n",
    "swell_barh_panel = pn.panel(swell_barh)\n",
    "\n",
    "# Display the widgets\n",
    "pn.Column(\n",
    "    pn.Row(pt_size_slider, well_type_slider),\n",
    "    pn.Row(swell_panel, swell_barh_panel),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### matching the mismatched wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swell_columns = [\"api\", \"lat83\", \"long83\", \"well_type\"]\n",
    "mismatch_columns = [\n",
    "    \"geometry\",\n",
    "    \"tx_api\",\n",
    "    \"lon\",\n",
    "    \"lat\",\n",
    "    \"original_index\",\n",
    "    \"state_left\",\n",
    "    \"state_right\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see here athat althought he api match, the geo coords are different\n",
    "mismatch_geo_tx[mismatch_columns].merge(\n",
    "    swell_gdf[swell_columns], how=\"left\", left_on=\"tx_api\", right_on=\"api\"\n",
    ")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_mismatch_tx = mismatch_geo_tx[mismatch_columns].merge(\n",
    "    swell_gdf[swell_columns], how=\"left\", left_on=\"tx_api\", right_on=\"api\"\n",
    ")\n",
    "\n",
    "# set the index to the original_index column\n",
    "joined_mismatch_tx.set_index(\"original_index\", inplace=True)\n",
    "\n",
    "# replace the lat and lon values with the lat83 and lon83 values if not nan\n",
    "joined_mismatch_tx.loc[joined_mismatch_tx[\"lat83\"].notna(), \"lat\"] = joined_mismatch_tx[\n",
    "    \"lat83\"\n",
    "]\n",
    "joined_mismatch_tx.loc[\n",
    "    joined_mismatch_tx[\"long83\"].notna(), \"lon\"\n",
    "] = joined_mismatch_tx[\"long83\"]\n",
    "\n",
    "# Update 'lat' and 'lon' in the original DataFrame\n",
    "\n",
    "\n",
    "registry_gdf.loc[joined_mismatch_tx.index, \"lat\"] = joined_mismatch_tx[\"lat\"]\n",
    "registry_gdf.loc[joined_mismatch_tx.index, \"lon\"] = joined_mismatch_tx[\"lon\"]\n",
    "\n",
    "\n",
    "# Update 'geometry' in the original DataFrame\n",
    "registry_gdf.loc[joined_mismatch_tx.index, \"geometry\"] = [\n",
    "    Point(xy) for xy in zip(joined_mismatch_tx.lon, joined_mismatch_tx.lat)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the spatial join with registry_gdf and county_fips_gdf\n",
    "registry_county_gdf_2 = registry_gdf.sjoin(\n",
    "    county_fips_gdf, how=\"left\", predicate=\"intersects\"\n",
    ").drop(columns=[\"index_right\"])\n",
    "\n",
    "mismatched_geo_2 = registry_county_gdf_2[\n",
    "    registry_county_gdf_2[\"state_left\"] != registry_county_gdf_2[\"state_right\"]\n",
    "]\n",
    "print(f\"Number of rows with mismatched state values: {mismatched_geo_2.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another check is for rows where the `county_code` is an even number. Although the API number does not follow the state fips code as its first 2 numbers, the next 3 numbers do follow the county fips code. County fips codes are mostly odd numbers (almost exclusively) so any even number in the `county_code` column is most likely incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the the county_code which are even numbers\n",
    "mismatched_geo_2[\n",
    "    mismatched_geo_2[\"county_code\"].astype(int).apply(lambda x: x % 2 == 0)\n",
    "][trimmed_column_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which other wells may have a state value which did not agree with the spatial join result again\n",
    "registry_county_gdf = registry_gdf.sjoin(\n",
    "    county_fips_gdf, how=\"left\", predicate=\"intersects\"\n",
    ").drop(columns=[\"index_right\"])\n",
    "mismatch_geo = registry_county_gdf[registry_county_gdf[\"state_right\"].isna()]\n",
    "mismatch_geo.shape\n",
    "print(f\"Number of rows with mismatched state values: {mismatch_geo.shape[0]}\")\n",
    "if mismatch_geo.shape[0] > 0:\n",
    "    display(mismatch_geo.sample(3))\n",
    "else:\n",
    "    print(\"No rows with mismatched state values\")\n",
    "# display(mismatch_geo.sample(3))\n",
    "\n",
    "# drop the rows with null values in the state_right column of mismatch_geo from the registry_gdf\n",
    "print(f\"Number of rows before dropping: {registry_gdf.shape[0]}\")\n",
    "registry_gdf.drop(mismatch_geo.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rows after dropping: {registry_gdf.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_gdf.sjoin(county_fips_gdf, how=\"left\", predicate=\"intersects\").drop(\n",
    "    columns=[\"index_right\"]\n",
    ")[trimmed_column_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the row where the county_left and county_right columns do not match\n",
    "trimmed_joined_gdf = registry_gdf.sjoin(\n",
    "    county_fips_gdf, how=\"left\", predicate=\"intersects\"\n",
    ").drop(columns=[\"index_right\"])[trimmed_column_set]\n",
    "state_mismatch_mask = (\n",
    "    trimmed_joined_gdf[\"state_left\"] != trimmed_joined_gdf[\"state_right\"]\n",
    ")\n",
    "state_mismatch_gdf = trimmed_joined_gdf[state_mismatch_mask]\n",
    "print(\n",
    "    f\"Number of rows with mismatched county values: {state_mismatch_gdf.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_mismatch_gdf.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the columns to keep\n",
    "columns_we_want = [\n",
    "    \"api\",\n",
    "    \"well\",\n",
    "    \"state_code\",\n",
    "    \"state_left\",\n",
    "    \"state_right\",\n",
    "    \"county_code\",\n",
    "    \"county_left\",\n",
    "    \"countyfp\",\n",
    "    \"name\",\n",
    "    \"operator\",\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"geoid\",\n",
    "    \"geometry\",\n",
    "]\n",
    "registry_county_gdf = registry_gdf.sjoin(\n",
    "    county_fips_gdf, how=\"left\", predicate=\"intersects\"\n",
    ").drop(columns=[\"index_right\"])[columns_we_want]\n",
    "\n",
    "print(f\"Number of rows: {registry_county_gdf.shape[0]}\")\n",
    "registry_county_gdf[\n",
    "    registry_county_gdf[\"state_left\"] != registry_county_gdf[\"state_right\"]\n",
    "][columns_we_want].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the rows where the state_left is California with the state_right being Texas\n",
    "# This would represent where the geometry said Texas but the api or original dataset had California\n",
    "# we check the api with\n",
    "cali_tx_query = registry_county_gdf.query(\n",
    "    'state_left == \"California\" & state_right == \"Texas\"'\n",
    ")\n",
    "# from the query, take the last 9 of the api (well_id) with 42 at beginning\n",
    "# with county_code from county that was matched with the geometry\n",
    "cali_tx_query[\"state_code\"] = \"42\"\n",
    "cali_tx_query[\"county_code\"] = cali_tx_query[\"countyfp\"]\n",
    "cali_tx_query[\"well_id\"] = cali_tx_query[\"api\"].str[-9:]\n",
    "cali_tx_query[\"api\"] = (\n",
    "    cali_tx_query[\"state_code\"]\n",
    "    + cali_tx_query[\"county_code\"]\n",
    "    + cali_tx_query[\"well_id\"]\n",
    ")\n",
    "\n",
    "# put api, county_code and state_code into the registry_gdf\n",
    "registry_gdf.loc[\n",
    "    cali_tx_query.index, [\"api\", \"county_code\", \"state_code\"]\n",
    "] = cali_tx_query[[\"api\", \"county_code\", \"state_code\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_county_gdf[\n",
    "    registry_county_gdf[columns_we_want][\"state_left\"]\n",
    "    != registry_county_gdf[columns_we_want][\"state_right\"]\n",
    "][columns_we_want]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_callback(x_range, y_range):\n",
    "    cvs = ds.Canvas(plot_width=600, plot_height=600,\n",
    "                    x_range=x_range, y_range=y_range)\n",
    "    agg = cvs.polygons(surv_gdf, geometry=\"geometry\", agg=ds.any())\n",
    "    # agg = cvs.line(surv_gdf, geometry=\"geometry\", agg=ds.any(), line_width=0.1)\n",
    "\n",
    "    return gv.Image(tf.shade(agg)).opts(\n",
    "        width=600,\n",
    "        height=600,\n",
    "        alpha=0.8,\n",
    "        # framewise=True,\n",
    "    )\n",
    "\n",
    "\n",
    "path_dmap = hv.DynamicMap(poly_callback, streams=[hv.streams.RangeXY()])\n",
    "path_dmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the GeoDataFrames in surv_dict into a single GeoDataFrame\n",
    "surv_gdf = concat_gdf_from_dict(surv_dict)\n",
    "\n",
    "# Convert the column names to snake case for consistency\n",
    "surv_gdf.columns = [pascal_to_snake(col) for col in surv_gdf.columns]\n",
    "# addd a coulmn for the county_code\n",
    "surv_gdf[\"county_code\"] = surv_gdf[\"source_file\"].str.extract(r\"(\\d{3})\")\n",
    "\n",
    "# Display a sample of 3 rows from the DataFrame\n",
    "display(surv_gdf.sample(3))\n",
    "\n",
    "# Display information about the DataFrame, including the number of non-null entries in each column\n",
    "surv_gdf.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the length of the tx_api values\n",
    "swell_gdf[\"api\"].astype(\"string\").str.len().value_counts()\n",
    "\n",
    "# drop the rows with the api number length of 3\n",
    "swell_gdf.drop(\n",
    "    swell_gdf[swell_gdf[\"api\"].astype(\"string\").str.len() == 3].index,\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map files taken from the EIA website.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot shale plays and basin boundaries of the different formations in the Permian Basin\n",
    "get_background_map() * basins_gdf[[\"geometry\", \"shale_play\"]].hvplot(\n",
    "    geo=True,\n",
    "    alpha=0.4,\n",
    "    title=\"Shale Plays in the Permian Basin\",\n",
    "    legend=True,\n",
    "    by=\"shale_play\",\n",
    "    muted_alpha=0.01,\n",
    ").opts(\n",
    "    tools=[\"hover\", \"tap\"],\n",
    "    legend_position=\"right\",\n",
    "    height=600,\n",
    "    width=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissolve the geometries of the basins_gdf GeoDataFrame into a single geometry\n",
    "\n",
    "\n",
    "shale_plays_gdf = basins_gdf[[\"geometry\"]].dissolve()\n",
    "# get the counties in the Permian Basin\n",
    "permian_counties = county_fips_gdf.intersects(shale_plays_gdf)\n",
    "# plot the counties in the Permian Basin\n",
    "get_background_map() * gv.Polygons(county_fips_gdf[permian_counties]).opts(\n",
    "    title=\"Counties in the Permian Basin\",\n",
    "    tools=[\"hover\"],\n",
    "    height=600,\n",
    "    width=800,\n",
    "    alpha=0.5,\n",
    "    color=\"skyblue\",\n",
    ")\n",
    "# plot an outline of the Permian Basin over the counties\n",
    "overlay = (\n",
    "    get_background_map()\n",
    "    * gv.Polygons(county_fips_gdf[permian_counties])\n",
    "    * gv.Path(shale_plays_gdf)\n",
    "    # * gpoints\n",
    ")\n",
    "\n",
    "overlay.opts(\n",
    "    opts.Polygons(alpha=0.5, cmap=[\"#73d2ff\"], line_color=\"gray\"),\n",
    "    opts.Path(alpha=0.5, color=\"black\"),\n",
    "    opts.Overlay(tools=[\"hover\"], height=600, width=800),\n",
    "    opts.Points(color=\"crimson\"),\n",
    ")\n",
    "\n",
    "\n",
    "# plot to confirm that the geometries have been dissolved\n",
    "# bg_map * gv.Polygons(shale_plays_gdf).opts(\n",
    "#     # geo=True,\n",
    "#     title=\"Dissolved Shale play in the Permian Basin\",\n",
    "#     height=600,\n",
    "#     width=800,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column to the surv_gdf with the county number\n",
    "# the county_number wil be the numbers in the source_file column\n",
    "surv_gdf[\"county_number\"] = surv_gdf[\"source_file\"].str.extract(r\"(\\d{3})\")\n",
    "\n",
    "# using just the geometry and the county_number columns, intersect with the permian basin gdf\n",
    "surv_permian_gdf = surv_gdf[[\"geometry\", \"county_number\"]].sjoin(\n",
    "    shale_plays_gdf[[\"geometry\"]], how=\"inner\", predicate=\"intersects\"\n",
    ")\n",
    "# see which counties are in the permian basin\n",
    "pb_county_numbers = surv_permian_gdf[\"county_number\"].unique().tolist()\n",
    "\n",
    "# plot the survey lines in the permian basin\n",
    "bg_map * gv.Polygons(\n",
    "    surv_permian_gdf.to_crs(\"EPSG:4269\"), vdims=[\"county_number\"]\n",
    ").opts(\n",
    "    tools=[\"hover\"],\n",
    "    height=600,\n",
    "    width=800,\n",
    "    alpha=0.5,\n",
    "    line_width=0,\n",
    "    title=\"Permian Basin Survey Lines\",\n",
    ")\n",
    "\n",
    "# surv_gdf.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how land survey polygon data looks on map\n",
    "pb_plot = shale_plays_gdf.hvplot(geo=True, color=\"red\", alpha=0.5, line_width=0).opts(\n",
    "    height=600, width=800\n",
    ")\n",
    "survey_plot = surv_gdf.hvplot(geo=True, color=\"blue\", alpha=0.5, line_width=0)\n",
    "\n",
    "# bg_map * survey_plot * pb_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the intersection of the survey polygons and the Permian Basin polygon\n",
    "survey_pb_gdf = gpd.overlay(surv_gdf, shale_plays_gdf, how=\"intersection\")\n",
    "survey_pb_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join of the registry_gdf(from fracfocus) and surv_data_gdf\n",
    "registry_join_gdf = gpd.sjoin(\n",
    "    registry_gdf[\n",
    "        [\n",
    "            \"geometry\",\n",
    "            \"api\",\n",
    "            \"operator_name\",\n",
    "            \"well_name\",\n",
    "            \"state\",\n",
    "            \"county_name\",\n",
    "            \"county_number\",\n",
    "        ]\n",
    "    ],\n",
    "    surv_gdf,\n",
    ").drop(columns=[\"index_right\"])\n",
    "\n",
    "\n",
    "registry_join_gdf.sort_values(by=\"api\")\n",
    "\n",
    "registry_join_gdf.county_name.value_counts()\n",
    "\n",
    "# create a well_id column from the api column\n",
    "\n",
    "registry_join_gdf[\"tx_api\"] = registry_join_gdf[\"api\"].str[2:10]\n",
    "\n",
    "\n",
    "# merge the welltype column from well_data_gdf to registry_join_gdf on the api_short column\n",
    "swell_gdf[\"tx_api\"] = swell_gdf[\"api\"].copy()\n",
    "\n",
    "registry_join_gdf = (\n",
    "    registry_join_gdf.merge(swell_gdf[[\"tx_api\", \"well_type\"]], on=\"tx_api\")\n",
    "    .drop(columns=[\"scrap_file\", \"level4_sur\"])\n",
    "    .rename(columns={\"level2_blo\": \"block\"})\n",
    ")\n",
    "# registry_join_gdf.explore()\n",
    "# plot polygons using geoviews\n",
    "# bg_map * gv.Polygons(registry_join_gdf.to_crs(\"EPSG:4269\"), vdims=[\"well_type\"]).opts(\n",
    "#     **poly_opts, color=\"well_type\", title=\"Well Types in the Permian Basin\"\n",
    "# )\n",
    "\n",
    "bg_map * registry_join_gdf.hvplot(\n",
    "    geo=True,\n",
    "    by=\"well_type\",\n",
    "    alpha=0.8,\n",
    "    legend=\"right\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    size=1,\n",
    "    muted_alpha=0.01,\n",
    "    tools=[\"box_select\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_gdb_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, gdf in active_gdb_dict.items():\n",
    "    print(f\"{k}| Shape:{gdf.shape}| CRS:{gdf.crs.to_string()}\")\n",
    "    display(gdf.sample(3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the active lease geodatabase\n",
    "active_leases_gdf = active_gdb_dict[\"OAG_Leases_Active\"]\n",
    "# clean column names\n",
    "active_leases_gdf.columns = [pascal_to_snake(col) for col in active_leases_gdf.columns]\n",
    "active_leases_gdf.describe(include=\"all\").T.sort_values(\n",
    "    by=\"unique\", ascending=False\n",
    ").fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the columns with the date in it using regex\n",
    "date_cols = [col for col in active_leases_gdf.columns if re.search(r\"date\", col)]\n",
    "# add any other columns that should be dates\n",
    "date_cols.extend([\"lease_input\"])\n",
    "\n",
    "date_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the date columns to datetime\n",
    "active_leases_gdf[date_cols] = pd.concat(\n",
    "    [pd.to_datetime(active_leases_gdf[col]) for col in date_cols], axis=1\n",
    ")\n",
    "# active_leases_gdf[date_cols] = active_leases_gdf[date_cols].fillna(\n",
    "#     pd.Timestamp(\"1900-06-28\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the columns interested in seeing\n",
    "columns_of_interest = date_cols + [\n",
    "    \"county\",\n",
    "    \"geometry\",\n",
    "    \"land_type\",\n",
    "    \"primary_term_year\",\n",
    "    \"original_lessee\",\n",
    "    \"lessor\",\n",
    "    \"field_name\",\n",
    "    \"lease_type\",\n",
    "    \"lease_status\",\n",
    "    \"lease_number\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_leases_gdf[columns_of_interest].info()\n",
    "active_leases_gdf[active_leases_gdf[columns_of_interest].isna().any(axis=1)][\n",
    "    columns_of_interest\n",
    "].sort_values(by=\"effective_date\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_leases_gdf_trimmed = active_leases_gdf[columns_of_interest]\n",
    "\n",
    "active_leases_gdf_trimmed[\"lease_type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_date_cols = list(set(columns_of_interest) - set(date_cols))\n",
    "pd.concat(\n",
    "    [\n",
    "        active_leases_gdf[non_date_cols],\n",
    "        active_leases_gdf[date_cols].astype(\n",
    "            str\n",
    "        ),  # the .explore() does not work with NaT in datetime columns\n",
    "    ],\n",
    "    axis=1,\n",
    ").explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the active units geodatabase\n",
    "active_units_gdf = active_gdb_dict[\"OAG_Units_Active\"]\n",
    "# clean column names\n",
    "active_units_gdf.columns = [pascal_to_snake(col) for col in active_units_gdf.columns]\n",
    "active_units_gdf.describe(include=\"all\").T.sort_values(\n",
    "    by=\"unique\", ascending=False\n",
    ").fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_units_gdf.field_name.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_units_gdf.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Production Data Query Dump from RRC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loaded in the `Dask` dataframes earlier. In case you are not familiar with `dask`, by loading in a dask dataframe, we actually only load in a small portion of the data to memory, 1 partition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tables_dict[\"og_operator_cycle\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need any values which require more than 1 partition, we will need to `compute()` it. Even wehen we `.compute()`, dask does nto necessarily load in all the data into memory.It plans out what it needs to do, and then does it without overwhelming the system. It is also closely integrated with `pandas` so it is very similar syntactically.\n",
    "\n",
    "We load 10 of the 16 tables from the Production Report Query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "persisted_overviews = {}\n",
    "\n",
    "for table_name in tqdm(data_tables_dict.keys()):\n",
    "    overview_table = get_table_overview_from_frame(\n",
    "        data_tables_dict, table_name)\n",
    "    (persisted_overview,) = persist(overview_table)\n",
    "    persisted_overviews[table_name] = persisted_overview\n",
    "\n",
    "persisted_overviews_list = [\n",
    "    persisted_overviews[table_name] for table_name in data_tables_dict\n",
    "]\n",
    "overview_dfs_from_frame = pd.concat(\n",
    "    persisted_overviews_list, ignore_index=True)\n",
    "overview_dfs_from_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# size multiplier dict\n",
    "size_multiplier_dict = {\n",
    "    \"B\": 1,\n",
    "    \"kB\": 1e3,\n",
    "    \"MB\": 1e6,\n",
    "    \"GB\": 1e9,\n",
    "}\n",
    "# sum the size column by converting the string to a float and multiplying by the corresponding multiplier\n",
    "total_size = (\n",
    "    overview_dfs_from_frame[\"size\"]\n",
    "    .str.split(\" \")\n",
    "    .apply(lambda x: float(x[0]) * size_multiplier_dict[x[1]])\n",
    "    .sum()\n",
    ")\n",
    "print(f\"Total size of all tables: {format_in_B(total_size)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. County Table : gp_county\n",
    "\n",
    "General purpose table that stores county information.\n",
    "\n",
    "Below is the district_no, a number representing the RRC district_name in the RRC system associated with lease reporting.\n",
    "\n",
    " DISTRICT_no (RRC VALUE) | DISTRICT_NAME |\n",
    "|--|--|\n",
    "01|01 \n",
    "02|02 \n",
    "04|04 \n",
    "05|05 \n",
    "06|06 \n",
    "07|6E (oil only)\n",
    "|08|7B\n",
    "|10|08\n",
    "|11|8A\n",
    "|13|09\n",
    "|14|10\n",
    "This value is not used. 12 | 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"gp_county\")\n",
    "get_table_card_from_frame(data_tables_dict, \"gp_county\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table is small enough to just load into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dask dataframe\n",
    "gp_county_df = data_tables_dict[\"gp_county\"].compute()\n",
    "\n",
    "gp_county_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the the column names to snake case\n",
    "gp_county_df.columns = [pascal_to_snake(col) for col in gp_county_df.columns]\n",
    "# create column is_onshore based on on_shore_flag column\n",
    "gp_county_df[\"is_onshore\"] = gp_county_df[\"on_shore_flag\"].map(\n",
    "    {\"Y\": True, \"N\": False})\n",
    "gp_county_df[\"is_onshore_assc_cnty\"] = gp_county_df[\"onshore_assc_cnty_flag\"].map(\n",
    "    {\"Y\": True, \"N\": False}\n",
    ")\n",
    "\n",
    "# create a small table to link the district_no to the district_name\n",
    "gp_district_name = (\n",
    "    gp_county_df[[\"district_no\", \"district_name\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=\"district_no\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "gp_district_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some of the extra columns\n",
    "column_to_drop = [\n",
    "    \"county_fips_code\",\n",
    "    \"district_name\",\n",
    "    \"on_shore_flag\",\n",
    "    \"onshore_assc_cnty_flag\",\n",
    "]\n",
    "columns_to_keep = [\n",
    "    col for col in gp_county_df.columns if col not in column_to_drop]\n",
    "gp_county_df = gp_county_df[columns_to_keep]\n",
    "gp_county_df.info(memory_usage=\"deep\")\n",
    "gp_county_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of counties in each district\n",
    "district_county_count = (\n",
    "    gp_county_df.groupby(\n",
    "        \"district_no\").county_no.nunique().rename(\"county_count\")\n",
    ")\n",
    "\n",
    "district_county_count.hvplot.bar(\n",
    "    title=\"Count of Counties in each District\", xlabel=\"\", ylabel=\"\", hover_cols=\"all\"\n",
    ").opts(\n",
    "    toolbar=\"above\",\n",
    "    active_tools=[\"box_zoom\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = (\n",
    "    gp_county_df.groupby([\"district_no\", \"is_onshore\", \"is_onshore_assc_cnty\"])\n",
    "    .count()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_labels(var_1, var_2, col1, col2):\n",
    "    # Filter the data for the given 'is_onshore' value\n",
    "    data = counts[(counts[col1] == var_1) & (counts[col2] == var_2)]\n",
    "\n",
    "    # Create a bar plot\n",
    "    bar_plot = data.hvplot.bar(\n",
    "        x=\"district_no\",\n",
    "        y=\"county_no\",\n",
    "        stacked=True,\n",
    "        xlabel=\"\",\n",
    "        yaxis=\"bare\",\n",
    "        title=f\"Count of Counties in each District No. where {col1}={var_1} & {col2}={var_2}\",\n",
    "    )\n",
    "\n",
    "    # Create a list to hold the text elements\n",
    "    texts = []\n",
    "\n",
    "    # Loop over the DataFrame and create a text element for each row\n",
    "    for row in data.itertuples():\n",
    "        # The position of the text is the center of the bar\n",
    "        x = getattr(row, \"district_no\")\n",
    "        y = getattr(row, \"county_no\") / 2\n",
    "        # The text is the height of the bar\n",
    "        text = str(getattr(row, \"county_no\"))\n",
    "        # Create the text element and add it to the list\n",
    "        texts.append(hv.Text(x, y, text).opts(text_color=\"white\"))\n",
    "\n",
    "    # Overlay the text elements on the plot\n",
    "    labelled_plot = bar_plot * hv.Overlay(texts)\n",
    "\n",
    "    return labelled_plot\n",
    "\n",
    "\n",
    "# Create a DynamicMap with the 'plot_with_labels' function\n",
    "dynamic_map = hv.DynamicMap(\n",
    "    lambda var_1, var_2: plot_with_labels(\n",
    "        var_1, var_2, \"is_onshore\", \"is_onshore_assc_cnty\"\n",
    "    ),\n",
    "    kdims=[\n",
    "        hv.Dimension(\"var_1\", label=\"is_onshore\"),\n",
    "        hv.Dimension(\"var_2\", label=\"is_onshore_assc_cnty\"),\n",
    "    ],\n",
    ").redim.range(\n",
    "    var_1=(0, 1),\n",
    "    var_2=(0, 1),\n",
    ")\n",
    "dynamic_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gp_county_ddf[gp_county_ddf[\"is_onshore\"] == 0].compute()\n",
    "gp_county_df[gp_county_df[\"is_onshore_assc_cnty\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_shore_assc_cnty = (\n",
    "    gp_county_df[gp_county_df[\"is_onshore_assc_cnty\"] == 1][\"county_no\"]\n",
    "    .astype(str)\n",
    "    .str.zfill(3)\n",
    "    .tolist()\n",
    ")\n",
    "# get the county fips table\n",
    "county_fips_gdf = get_county_data()\n",
    "# get the counties that are in Texas with those county numbers\n",
    "\n",
    "poly_assc_cnty = county_fips_gdf[\n",
    "    county_fips_gdf[\"countyfp\"].isin(on_shore_assc_cnty)\n",
    "    & county_fips_gdf[\"statefp\"].str.contains(\"48\")\n",
    "]\n",
    "# plot these counties on a map using the county_fips_gdf for boundaries\n",
    "get_background_map() * gv.Polygons(poly_assc_cnty).opts(\n",
    "    title=\"Is Onshore Assc Counties\",\n",
    "    tools=[\"hover\"],\n",
    "    height=600,\n",
    "    width=800,\n",
    "    alpha=0.5,\n",
    "    color=\"skyblue\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with is_onshore==False\n",
    "gp_county_df = gp_county_df[gp_county_df[\"is_onshore\"] == True]\n",
    "# Drop the 2 extra columns\n",
    "gp_county_df.drop(columns=[\"is_onshore\", \"is_onshore_assc_cnty\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_polys_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get county gdf and filter for 48(TX)\n",
    "tx_polys_gdf = county_fips_gdf.query('statefp.str.contains(\"48\")')\n",
    "# convert the county_no to strin gto merge with the county_fips_gdf\n",
    "gp_county_df[\"county_no\"] = gp_county_df[\"county_no\"].astype(str).str.zfill(3)\n",
    "\n",
    "tx_polys_gdf = (\n",
    "    tx_polys_gdf[[\"geometry\", \"countyfp\"]]\n",
    "    .merge(gp_county_df, how=\"left\", left_on=\"countyfp\", right_on=\"county_no\")\n",
    "    .drop(columns=[\"countyfp\"])\n",
    ")\n",
    "# Dissolve tx_polys_gdf by district_no\n",
    "tx_polys_gdf = (\n",
    "    tx_polys_gdf.dissolve(by=\"district_no\")\n",
    "    .drop(columns=[\"county_no\", \"county_name\"])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate the centroids of the districts\n",
    "tx_polys_gdf[\"centroid\"] = tx_polys_gdf[\"geometry\"].centroid\n",
    "\n",
    "# Convert the centroids to a separate GeoDataFrame\n",
    "centroids_gdf = gpd.GeoDataFrame(tx_polys_gdf, geometry=\"centroid\")\n",
    "\n",
    "# vectorize to mercator\n",
    "sxsy = platecaree_to_mercator_vectorised(\n",
    "    centroids_gdf[\"centroid\"].x, centroids_gdf[\"centroid\"].y\n",
    ")\n",
    "\n",
    "# Plot the polygons and labels\n",
    "tx_districts_polygons = tx_polys_gdf.hvplot(\n",
    "    geo=True,\n",
    "    tiles=\"EsriWorldBoundariesAndPlacesAlternate\",\n",
    "    fill_alpha=0,\n",
    "    line_width=3,\n",
    "    line_color=\"orange\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    # hover_cols=\"district_no\",\n",
    "    title=\"Districts in Texas\",\n",
    "    xaxis=\"bare\",\n",
    "    yaxis=\"bare\",\n",
    "    colorbar=False,\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "# Create labels\n",
    "tx_districts_labels = hv.Labels(\n",
    "    {\n",
    "        \"x\": sxsy[:, 0],\n",
    "        \"y\": sxsy[:, 1],\n",
    "        \"Label\": centroids_gdf[\"district_no\"].values,\n",
    "    }\n",
    ").opts(\n",
    "    text_color=\"orange\",\n",
    "    text_font_size=\"14pt\",\n",
    "    text_align=\"center\",\n",
    "    text_baseline=\"middle\",\n",
    ")\n",
    "\n",
    "tx_districts_polygons * tx_districts_labels\n",
    "\n",
    "# get_background_map() * gv.Polygons(\n",
    "#     tx_polys_gdf.dissolve(by=\"district_no\"), vdims=[\"district_no\"]\n",
    "# ).opts(\n",
    "#     tools=[\"hover\"],\n",
    "#     height=600,\n",
    "#     width=800,\n",
    "#     alpha=0.5,\n",
    "#     line_width=1,\n",
    "#     line_color=\"white\",\n",
    "#     cmap=\"lightcyan\",\n",
    "#     title=\"Districts in Texas\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table Lease Cycle : og_lease_cycle\n",
    "\n",
    "Contains production report data reported by lease and month (YYYYMM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"og_lease_cycle\"\n",
    ")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_lease_cycle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dask dataframe\n",
    "og_lease_cycle_ddf = data_tables_dict[\"og_lease_cycle\"]\n",
    "\n",
    "# convert the column names to lower case\n",
    "og_lease_cycle_ddf = og_lease_cycle_ddf.rename(columns=str.lower)\n",
    "columns_list = og_lease_cycle_ddf.columns.tolist()\n",
    "\n",
    "# drop certain columns based on substrings\n",
    "\n",
    "# create column is_gas based on the oil_gas_code column\n",
    "\n",
    "og_lease_cycle_ddf[\"is_gas\"] = (\n",
    "    og_lease_cycle_ddf[\"oil_gas_code\"].map({\"G\": True, \"O\": False}).astype(\"int8\")\n",
    ")\n",
    "# Define the regex pattern\n",
    "\n",
    "pattern = re.compile(\n",
    "    r\"_code$|_name$|cycle_month|cycle_year$|^district_no|lease_no$|_limit$|_lift$|tot_disp$|_allow$|_ending_bal$|inj_vol$\"\n",
    ")\n",
    "# Filter the columns\n",
    "\n",
    "columns_to_keep = [col for col in columns_list if not pattern.search(col)]\n",
    "\n",
    "og_lease_cycle_ddf = og_lease_cycle_ddf[columns_to_keep]\n",
    "\n",
    "# filter out the row with cycle_year_month < 201300\n",
    "og_lease_cycle_ddf = og_lease_cycle_ddf[og_lease_cycle_ddf.cycle_year_month > 201300]\n",
    "\n",
    "# get shape\n",
    "num_rows = og_lease_cycle_ddf.shape[0]\n",
    "num_cols = og_lease_cycle_ddf.shape[1]\n",
    "# drop the rows with prod_report_filed_flag = 'N'\n",
    "og_lease_cycle_ddf = og_lease_cycle_ddf[\n",
    "    og_lease_cycle_ddf.prod_report_filed_flag == \"Y\"\n",
    "]\n",
    "# Filter out the prod_report_filed_flag column\n",
    "columns_to_keep = [\n",
    "    col for col in og_lease_cycle_ddf.columns if col != \"prod_report_filed_flag\"\n",
    "]\n",
    "og_lease_cycle_ddf = og_lease_cycle_ddf[columns_to_keep]\n",
    "\n",
    "\n",
    "# lease_no are unique within districts\n",
    "num_lease_districts = og_lease_cycle_ddf.lease_no_district_no.nunique()\n",
    "# get the memory size of the new filtered ddf\n",
    "\n",
    "mem_size = og_lease_cycle_ddf.memory_usage(deep=True).sum()\n",
    "num_rows, num_lease_districts, mem_size = dask.compute(\n",
    "    num_rows, num_lease_districts, mem_size\n",
    ")\n",
    "print(f\"Shape of og_lease_cycle_ddf: ({num_rows} , {num_cols})\")\n",
    "print(f\"Number of unique lease_no_district_no: {num_lease_districts:,}\")\n",
    "print(f\"Memory size of og_lease_cycle_ddf: {format_in_B(mem_size)}\")\n",
    "og_lease_cycle_ddf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lease_prod_cols = [\n",
    "    \"lease_oil_prod_vol\",\n",
    "    \"lease_gas_prod_vol\",\n",
    "    \"lease_cond_prod_vol\",\n",
    "    \"lease_csgd_prod_vol\",\n",
    "]\n",
    "persisted_field_type_grouped = (\n",
    "    og_lease_cycle_ddf.groupby(\"field_type\")\n",
    "    .agg({**{col: \"sum\" for col in lease_prod_cols}, **{\"field_type\": \"count\"}})\n",
    "    .persist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persisted_field_type_grouped.rename(\n",
    "    columns={\"field_type\": \"field_type_count\"},\n",
    ").compute().sort_values(by=\"field_type_count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_gas = og_lease_cycle_ddf.nlargest(20, \"lease_gas_prod_vol\").compute()\n",
    "top_n_gas.lease_no_district_no = top_n_gas.lease_no_district_no.astype(str)\n",
    "# get the top 10 gas leases_no_district_no\n",
    "top_n_gas_ld = top_n_gas.lease_no_district_no.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_lease_cycle_ddf[\"district_no\"] = (\n",
    "    og_lease_cycle_ddf[\"lease_no_district_no\"].astype(str).str[-2:]\n",
    ").astype(\"int8\")\n",
    "# total production numbers by district\n",
    "persisted_og_lease_cycle_ddf = (\n",
    "    og_lease_cycle_ddf.groupby(\"district_no\")[lease_prod_cols].sum().persist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_total_lease_prod_vol = persisted_og_lease_cycle_ddf.compute().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a callback function to return the plot\n",
    "def plot_chloropleth(product):\n",
    "    \"\"\"Returns a choropleth plot of the total production of the selected product\"\"\"\n",
    "    chloro_data = computed_total_lease_prod_vol[[\"district_no\", product]]\n",
    "    geo_chloro_data = tx_polys_gdf.merge(\n",
    "        chloro_data, on=\"district_no\", how=\"left\"\n",
    "    ).fillna(0)\n",
    "\n",
    "    polygons = [\n",
    "        gv.Polygons(geo_chloro_data, vdims=[product, \"district_no\"]).opts(\n",
    "            fill_alpha=0.8,\n",
    "            line_width=2,\n",
    "            color=product,\n",
    "            cnorm=\"eq_hist\",\n",
    "            height=600,\n",
    "            width=800,\n",
    "            colorbar=True,\n",
    "            xaxis=\"bare\",\n",
    "            yaxis=\"bare\",\n",
    "            colorbar_position=\"bottom\",\n",
    "            tools=[\"hover\"],\n",
    "            title=f\"Total Production: {product}\",\n",
    "        )\n",
    "    ]\n",
    "    return gv.Overlay(get_background_map() * gv.Layout(polygons))\n",
    "\n",
    "\n",
    "product_button = pnw.RadioButtonGroup(\n",
    "    options=lease_prod_cols,\n",
    "    value=\"lease_oil_prod_vol\",\n",
    "    name=\"Product\",\n",
    "    width=400,\n",
    "    orientation=\"vertical\",\n",
    ")\n",
    "# bind the widget and the function\n",
    "chloropleth_dmap = gv.DynamicMap(\n",
    "    pn.bind(plot_chloropleth, product=product_button.param.value)\n",
    ")\n",
    "pn.Row(product_button, chloropleth_dmap).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table District Cycle : og_district_cycle\n",
    "\n",
    "Contains production report data reported by lease and month (YYYYMM) aggregated by the completion district for the lease ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"og_district_cycle\"\n",
    ")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_district_cycle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dask dataframe\n",
    "og_district_cycle_df = data_tables_dict[\"og_district_cycle\"].compute()\n",
    "# convert the column names to lower case\n",
    "og_district_cycle_df = og_district_cycle_df.rename(columns=str.lower)\n",
    "# get list of columns\n",
    "columns_list = og_district_cycle_df.columns.tolist()\n",
    "\n",
    "# drop the columns we don't want from columns list\n",
    "columns_list = [\n",
    "    col\n",
    "    for col in columns_list\n",
    "    if col not in [\"district_name\", \"cycle_month\", \"cycle_year\"]\n",
    "]\n",
    "# drop the column from the dataframe\n",
    "og_district_cycle_df = og_district_cycle_df[columns_list]\n",
    "\n",
    "# drop  row with cycle_year < 2013\n",
    "og_district_cycle_df = og_district_cycle_df[\n",
    "    # (og_district_cycle_df.cycle_year_month > 201300)\n",
    "    # &\n",
    "    (og_district_cycle_df.cycle_year_month < 202309)\n",
    "]\n",
    "\n",
    "# compute the number of rows and memory usage and show values\n",
    "num_rows = og_district_cycle_df.shape[0]\n",
    "memory_usage = og_district_cycle_df.memory_usage(index=True, deep=True).sum()\n",
    "print(f\"Shape of og_district_cycle_df: ({num_rows} , {len(columns_list)}\")\n",
    "print(f\"Total memory usage: {format_in_B(memory_usage)}\")\n",
    "\n",
    "og_district_cycle_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the production columns\n",
    "dist_prod_vol_cols = [\n",
    "    \"dist_oil_prod_vol\",\n",
    "    \"dist_gas_prod_vol\",\n",
    "    \"dist_cond_prod_vol\",\n",
    "    \"dist_csgd_prod_vol\",\n",
    "]\n",
    "og_district_cycle_df[\"cycle_year_month\"] = pd.to_datetime(\n",
    "    og_district_cycle_df[\"cycle_year_month\"], format=\"%Y%m\"\n",
    ")\n",
    "og_district_cycle_df[\"cycle_year_month\"] = og_district_cycle_df[\n",
    "    \"cycle_year_month\"\n",
    "].astype(\"Period[M]\")\n",
    "# og_district_cycle_df[dist_prod_vol_cols] = og_district_cycle_df[dist_prod_vol_cols] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_date = og_district_cycle_df[\"cycle_year_month\"].to_frame(\n",
    ").reset_index(drop=True)\n",
    "#  create a player widget for the month_date\n",
    "month_date_player = pnw.Player(\n",
    "    name=\"year_month\",\n",
    "    start=0,\n",
    "    end=30 * 12 + 8,\n",
    "    value=0,\n",
    "    step=1,\n",
    "    interval=500,\n",
    "    loop_policy=\"loop\",\n",
    "    width=400,\n",
    ")\n",
    "# create a redio buttion group for the production columns\n",
    "dist_prod_button = pnw.RadioButtonGroup(\n",
    "    options=dist_prod_vol_cols,\n",
    "    value=\"dist_oil_prod_vol\",\n",
    "    name=\"District Production\",\n",
    "    width=400,\n",
    "    orientation=\"vertical\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bubble_plot_by_month_callback(index, product):\n",
    "    \"\"\"Callback to update the bubble plot based on the month selected\"\"\"\n",
    "    # get the month selected\n",
    "    month_year = month_date.iat[index, 0]\n",
    "    # filter the dataframe for the month selected\n",
    "    selected_data = og_district_cycle_df[\n",
    "        og_district_cycle_df[\"cycle_year_month\"] == month_year\n",
    "    ]\n",
    "\n",
    "    bubbles = hv.Points(\n",
    "        selected_data, kdims=[\"district_no\", product], vdims=[product]\n",
    "    ).opts(\n",
    "        height=600,\n",
    "        width=800,\n",
    "        tools=[\"hover\"],\n",
    "        size=np.log(hv.dim(product).norm() + 1) * 50,\n",
    "        title=f\"Production by District | {month_year} | {product}\",\n",
    "        # logy=True,\n",
    "    )\n",
    "    # insert big text to show the month selected and the product\n",
    "    text = hv.Labels({\"x\": [0], \"y\": [0], \"Label\": [f\"{month_year}\"]}).opts(\n",
    "        text_font_size=\"24pt\", text_color=\"gray\", text_align=\"left\"\n",
    "    )\n",
    "    return bubbles * text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.state.kill_all_servers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.state.kill_all_servers()\n",
    "monthly_bubbly = pn.panel(\n",
    "    pn.bind(\n",
    "        bubble_plot_by_month_callback,\n",
    "        index=month_date_player.param.value,\n",
    "        product=dist_prod_button.param.value,\n",
    "    )\n",
    ")\n",
    "\n",
    "player_dashboard = FastListTemplate(\n",
    "    site=\"Panel\",\n",
    "    title=\"District Production by Month\",\n",
    "    sidebar_width=400,\n",
    ")\n",
    "player_dashboard.main.append(\n",
    "    pn.Row(\n",
    "        pn.Column(dist_prod_button, month_date_player),\n",
    "        monthly_bubbly,\n",
    "    )\n",
    ")\n",
    "player_dashboard.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame\n",
    "max_district_df = pd.DataFrame()\n",
    "\n",
    "for col in dist_prod_vol_cols:\n",
    "    # Get the maximum value for that col for each cycle_year_month\n",
    "    max_values = og_district_cycle_df.groupby(\"cycle_year_month\")[col].max()\n",
    "    # Merge with the original DataFrame to get the corresponding district_no\n",
    "    max_districts = (\n",
    "        og_district_cycle_df[og_district_cycle_df[col].isin(max_values)]\n",
    "        .groupby(\"cycle_year_month\")[\"district_no\"]\n",
    "        .first()\n",
    "    )\n",
    "    # Put that district_no in a new column in the new DataFrame\n",
    "    max_district_df[f\"max_{col}_district_no\"] = max_districts\n",
    "\n",
    "max_district_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_on_chart(index):\n",
    "    \"\"\"Callback to update the line on the chart based on the month selected\"\"\"\n",
    "    # get the month selected\n",
    "    month_year = month_date.iat[index, 0]\n",
    "    # create a vertical line at the month selected\n",
    "    vline = hv.VLine(month_year.to_timestamp())\n",
    "    return vline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_max_df_and_chart(index):\n",
    "    # get the month selected\n",
    "    month_year = month_date.iat[index, 0]\n",
    "    # Filter the data to include only rows up to the current time\n",
    "    filtered_data = og_district_cycle_df[\n",
    "        og_district_cycle_df[\"cycle_year_month\"] <= month_year\n",
    "    ]\n",
    "    # Create the max_df DataFrame\n",
    "    max_df = pd.DataFrame(\n",
    "        filtered_data[\"cycle_year_month\"].unique(), columns=[\"cycle_year_month\"]\n",
    "    )\n",
    "    for col in dist_prod_vol_cols:\n",
    "        max_values = filtered_data.groupby(\"cycle_year_month\")[col].max().reset_index()\n",
    "        max_df = max_df.merge(max_values, on=\"cycle_year_month\", how=\"left\")\n",
    "\n",
    "    max_df = max_df.set_index(\"cycle_year_month\")\n",
    "    max_df.columns = [\"max_\" + col for col in max_df.columns]\n",
    "\n",
    "    # Create the max_chart plot\n",
    "    max_chart = max_df.hvplot(\n",
    "        height=600, width=800, logy=True, title=\"Max Production by Month\"\n",
    "    )\n",
    "    return max_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_production_chart(index, product):\n",
    "    # get the month selected\n",
    "    month_year = month_date.iat[index, 0]\n",
    "    # Filter the data to include only rows up to the current time\n",
    "    filtered_data = og_district_cycle_df[\n",
    "        og_district_cycle_df[\"cycle_year_month\"] <= month_year\n",
    "    ]\n",
    "    # Create the production_chart plot\n",
    "    production_chart = filtered_data.hvplot(\n",
    "        x=\"cycle_year_month\",\n",
    "        y=product,\n",
    "        xlabel=\"\",\n",
    "        ylabel=\"\",\n",
    "        height=600,\n",
    "        width=800,\n",
    "        logy=True,\n",
    "        title=\"Production by Month\",\n",
    "        by=\"district_no\",\n",
    "        muted_alpha=0.01,\n",
    "    ).opts(toolbar=\"above\", active_tools=[\"box_zoom\"])\n",
    "    return production_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bind the widget and the function and create a dynamic map\n",
    "vline_dmap = hv.DynamicMap(pn.bind(line_on_chart, index=month_date_player.param.value))\n",
    "max_chart_dmap = hv.DynamicMap(\n",
    "    pn.bind(update_max_df_and_chart, index=month_date_player.param.value)\n",
    ")\n",
    "district_chart_dmap = hv.DynamicMap(\n",
    "    pn.bind(\n",
    "        update_production_chart,\n",
    "        index=month_date_player.param.value,\n",
    "        product=dist_prod_button.param.value,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.state.kill_all_servers()\n",
    "\n",
    "# create dashboard\n",
    "player_dashboard = FastListTemplate(\n",
    "    site=\"Panel\",\n",
    "    title=\"District Production by Month\",\n",
    "    sidebar_width=400,\n",
    ")\n",
    "player_dashboard.sidebar.append(\n",
    "    pn.Column(dist_prod_button, month_date_player, width=330)\n",
    ")\n",
    "player_dashboard.main.append(pn.Row(vline_dmap * max_chart_dmap))\n",
    "\n",
    "player_dashboard.main.append(pn.Row(vline_dmap * district_chart_dmap))\n",
    "player_dashboard.main.append(pn.Row(monthly_bubbly))\n",
    "player_dashboard.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_year_month_list = og_district_cycle_df[\"cycle_year_month\"].unique().tolist()\n",
    "max_df = pd.DataFrame(cycle_year_month_list, columns=[\"cycle_year_month\"])\n",
    "\n",
    "cols_top = []\n",
    "for col in dist_prod_vol_cols:\n",
    "    # get the max value for each column group by cycle_year_month\n",
    "    max_values = (\n",
    "        og_district_cycle_df.groupby(\"cycle_year_month\")[col].max().reset_index()\n",
    "    )\n",
    "    # merge it to the max_df dataframe\n",
    "    max_df = max_df.merge(max_values, on=\"cycle_year_month\", how=\"left\")\n",
    "\n",
    "# convert the cycle_year_month column to datetime and set it as the index\n",
    "max_df[\"cycle_year_month\"] = pd.to_datetime(max_df[\"cycle_year_month\"], format=\"%Y%m\")\n",
    "max_df = max_df.set_index(\"cycle_year_month\")\n",
    "# rename the columns\n",
    "max_df.columns = [\"max_\" + col for col in max_df.columns]\n",
    "# plot the max_df\n",
    "max_chart = max_df.hvplot(\n",
    "    height=600, width=800, logy=True, title=\"Max Production by Month\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_district_cycle_df[\"cycle_year_month\"] = pd.to_datetime(\n",
    "    og_district_cycle_df[\"cycle_year_month\"].astype(str), format=\"%Y%m\", errors=\"coerce\"\n",
    ")\n",
    "\n",
    "og_district_cycle_df.hvplot.scatter(\n",
    "    x=\"cycle_year_month\",\n",
    "    y=\"dist_gas_prod_vol\",\n",
    "    xlabel=\"\",\n",
    "    ylabel=\"\",\n",
    "    title=\"Gas Production by District\",\n",
    "    height=600,\n",
    "    width=800,\n",
    "    by=\"district_no\",\n",
    "    muted_alpha=0.01,\n",
    ").opts(toolbar=\"above\", active_tools=[\"box_zoom\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 8 - Field Cycle\n",
    "\n",
    "Contains production report data reported by lease and month (YYYYMM) aggregated by the field in which the well(s) for the lease are completed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"og_field_cycle\"\n",
    ")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_field_cycle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_field_cycle_ddf = data_tables_dict[\"og_field_cycle\"]\n",
    "# convert the column names to lower case\n",
    "og_field_cycle_ddf = og_field_cycle_ddf.rename(columns=str.lower)\n",
    "# drop the columns we don't want from columns list\n",
    "columns_list = og_field_cycle_ddf.columns.tolist()\n",
    "columns_to_keep = [\n",
    "    col\n",
    "    for col in columns_list\n",
    "    if col not in [\"district_name\", \"county_name\", \"cycle_month\", \"cycle_year\"]\n",
    "]\n",
    "# drop the column from the dataframe\n",
    "og_field_cycle_ddf = og_field_cycle_ddf[columns_to_keep]\n",
    "# drop  row with cycle_year_month < 201300\n",
    "og_field_cycle_ddf = og_field_cycle_ddf[\n",
    "    (og_field_cycle_ddf.cycle_year_month > 201300)\n",
    "    & (og_field_cycle_ddf.cycle_year_month < 202309)\n",
    "]\n",
    "# drop the rows with null values for the field_name column\n",
    "og_field_cycle_ddf = og_field_cycle_ddf.dropna(subset=[\"field_name\"])\n",
    "# get new number of rows and memory usage\n",
    "num_rows = og_field_cycle_ddf.shape[0]\n",
    "memory_usage = og_field_cycle_ddf.memory_usage(index=True, deep=True).sum()\n",
    "# compute the number of rows and memory usage\n",
    "num_rows, memory_usage = dask.compute(num_rows, memory_usage)\n",
    "# show first few rows and shape\n",
    "print(\n",
    "    f\"Shape of og_field_cycle_ddf: ({num_rows} , {len(columns_list)})\\nTotal memory usage: {format_in_B(memory_usage)}\"\n",
    ")\n",
    "og_field_cycle_ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table Operator Cycle\n",
    "\n",
    "Contains production report data reported by lease and month (YYYYMM) aggregated by the operator of the lease.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"og_operator_cycle\"\n",
    ")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_operator_cycle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dask dataframe\n",
    "# og_operator_cycle_ddf = dd.read_csv(file_path_9, sep=\"}\", dtype=csv_dtypes)\n",
    "og_operator_cycle_df = data_tables_dict[\"og_operator_cycle\"].compute()\n",
    "# convert the column names to lower case\n",
    "og_operator_cycle_df = og_operator_cycle_df.rename(columns=str.lower)\n",
    "# drop the 'cycle_month' and 'cycle_year' columns\n",
    "columns_to_keep = [\n",
    "    col\n",
    "    for col in og_operator_cycle_df.columns\n",
    "    if col not in [\"cycle_month\", \"cycle_year\"]\n",
    "]\n",
    "og_operator_cycle_df = og_operator_cycle_df[columns_to_keep]\n",
    "# filter out the row with cycle_year_month < 201300\n",
    "og_operator_cycle_df = og_operator_cycle_df[\n",
    "    (og_operator_cycle_df.cycle_year_month > 201300)\n",
    "    & (og_operator_cycle_df.cycle_year_month < 202309)\n",
    "]\n",
    "# drop the rows with null values for the operator_name column\n",
    "og_operator_cycle_df = og_operator_cycle_df.dropna(subset=[\"operator_name\"])\n",
    "# show first few rows and get shape\n",
    "num_rows = og_operator_cycle_df.shape[0]\n",
    "num_cols = og_operator_cycle_df.shape[1]\n",
    "memory_usage = og_operator_cycle_df.memory_usage(index=True, deep=True).sum()\n",
    "\n",
    "print(\n",
    "    f\"Shape of og_operator_cycle_ddf: ({num_rows} , {num_cols})\\nTotal memory usage: {format_in_B(memory_usage)}\"\n",
    ")\n",
    "og_operator_cycle_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column for the total production to filter out the non producing operators\n",
    "og_operator_cycle_df[\"total_oper_prod_vol\"] = (\n",
    "    og_operator_cycle_df[\"oper_oil_prod_vol\"]\n",
    "    + og_operator_cycle_df[\"oper_gas_prod_vol\"]\n",
    "    + og_operator_cycle_df[\"oper_cond_prod_vol\"]\n",
    "    + og_operator_cycle_df[\"oper_csgd_prod_vol\"]\n",
    ")\n",
    "og_operator_cycle_df[\"is_producing\"] = og_operator_cycle_df[\"total_oper_prod_vol\"] > 0\n",
    "producing_operator_nos = og_operator_cycle_df[og_operator_cycle_df[\"is_producing\"]][\n",
    "    \"operator_no\"\n",
    "].unique()\n",
    "print(f\"Number of producing operators: {len(producing_operator_nos)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oper_prod_vol_cols = [\n",
    "    \"oper_oil_prod_vol\",\n",
    "    \"oper_gas_prod_vol\",\n",
    "    \"oper_cond_prod_vol\",\n",
    "    \"oper_csgd_prod_vol\",\n",
    "]\n",
    "\n",
    "# filtered out producers with any 0s\n",
    "filtered_operators = og_operator_cycle_df[\n",
    "    (og_operator_cycle_df[oper_prod_vol_cols] != 0).all(axis=1)\n",
    "]\n",
    "\n",
    "filtered_operators[\"operator_no\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 10 - Field DW\n",
    "\n",
    "\n",
    "Table of field identifying data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"og_field_dw\")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_field_dw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dask dataframe\n",
    "og_field_dw_ddf = data_tables_dict[\"og_field_dw\"]\n",
    "\n",
    "# can load the whole dataframe into memory as it is small\n",
    "og_field_dw_df = og_field_dw_ddf.compute()\n",
    "\n",
    "# convert the column names to lower case\n",
    "og_field_dw_df = og_field_dw_df.rename(columns=str.lower)\n",
    "\n",
    "columns_list = og_field_dw_df.columns.tolist()\n",
    "\n",
    "# drop the columns with all nans and get the columns list\n",
    "substrings = {\"modify\", \"remarks\", \"rev_rule\", \"create\", \"district_name\"}\n",
    "columns_list = [\n",
    "    col for col in columns_list if not any(substring in col for substring in substrings)\n",
    "]\n",
    "# drop the column from the dataframe\n",
    "og_field_dw_df = og_field_dw_df[columns_list]\n",
    "\n",
    "# convert field_no to an int then zfill(8)\n",
    "og_field_dw_df[\"field_no\"] = (\n",
    "    og_field_dw_df[\"field_no\"].astype(int).astype(str).str.zfill(8)\n",
    ")\n",
    "# if the o_county_no is null, fill it with the g_county_no\n",
    "og_field_dw_df[\"o_county_no\"].fillna(og_field_dw_df[\"g_county_no\"], inplace=True)\n",
    "# rename the o_county_no to county_no and drop the g_county_no\n",
    "og_field_dw_df = og_field_dw_df.rename(columns={\"o_county_no\": \"county_no\"})\n",
    "og_field_dw_df = og_field_dw_df.drop(columns=[\"g_county_no\"])\n",
    "# drop the rows with null values in the county_no column\n",
    "og_field_dw_df = og_field_dw_df.dropna(subset=[\"county_no\"])\n",
    "\n",
    "\n",
    "# drop rows with COMMISSION USE ONLY in the field_name column\n",
    "og_field_dw_df = og_field_dw_df[\n",
    "    ~og_field_dw_df[\"field_name\"].str.contains(\"COMMISSION USE ONLY\")\n",
    "]\n",
    "\n",
    "# get the number of rows and memory usage\n",
    "num_rows = og_field_dw_df.shape[0]\n",
    "memory_usage = og_field_dw_df.memory_usage(index=True, deep=True).sum()\n",
    "\n",
    "# show first few rows and shape\n",
    "print(f\"Shape of og_field_dw_df: ({og_field_dw_df.shape}\")\n",
    "print(f\"Total memory usage: {format_in_B(memory_usage)}\")\n",
    "og_field_dw_df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_no_name = og_field_dw_df[[\"field_no\", \"field_name\"]]\n",
    "field_no_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_field_dw_df[\"county_no\"].value_counts().tail(50)\n",
    "og_field_dw_df[og_field_dw_df[\"field_no\"].str.contains(\"12849001|24806333\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table Operator DW\n",
    "\n",
    "This table contains identifying operator information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"og_operator_dw\"\n",
    ")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_operator_dw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dask dataframe\n",
    "# og_operator_dw_ddf = dd.read_csv(file_path_11, sep=\"}\", dtype=csv_dtypes)\n",
    "og_operator_dw_ddf = data_tables_dict[\"og_operator_dw\"]\n",
    "\n",
    "# load in as pandas dataframe as it is small\n",
    "og_operator_dw_df = og_operator_dw_ddf.compute()\n",
    "\n",
    "# convert the column names to lower case\n",
    "og_operator_dw_df.columns = [pascal_to_snake(col) for col in og_operator_dw_df.columns]\n",
    "\n",
    "# remove rows with RAILROAD COMMISSION in the operator_name column\n",
    "rrc_pattern = \"RAILROAD COMMISSION\"\n",
    "og_operator_dw_df = og_operator_dw_df[\n",
    "    ~og_operator_dw_df[\"operator_name\"].str.contains(rrc_pattern, regex=True)\n",
    "]\n",
    "\n",
    "\n",
    "# drop the columns with all nans and other useless columns\n",
    "pattern = re.compile(r\"modify|efile|record|create\")\n",
    "columns_to_keep = [col for col in og_operator_dw_df.columns if not pattern.search(col)]\n",
    "\n",
    "# convert p5Llast_filed_dt to datetime\n",
    "og_operator_dw_df[\"p5_last_filed_dt\"] = pd.to_datetime(\n",
    "    og_operator_dw_df[\"p5_last_filed_dt\"].astype(str), errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# drop the column from the dataframe\n",
    "og_operator_dw_df = og_operator_dw_df[columns_to_keep]\n",
    "\n",
    "# strip the excess space from the Letter in the p5_status_code column\n",
    "og_operator_dw_df[\"p5_status_code\"] = og_operator_dw_df[\"p5_status_code\"].str.strip()\n",
    "\n",
    "# get the number of rows and memory usage\n",
    "num_rows = og_operator_dw_df.shape[0]\n",
    "memory_usage = og_operator_dw_df.memory_usage(index=True, deep=True).sum()\n",
    "\n",
    "# show first few rows and shape\n",
    "print(f\"Shape of og_operator_dw_df: ({num_rows} , {len(columns_to_keep)})\")\n",
    "print(f\"Total memory usage: {format_in_B(memory_usage)}\")\n",
    "og_operator_dw_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a table with the operator_no and operator_name\n",
    "operator_no_name = og_operator_dw_df[[\"operator_no\", \"operator_name\"]]\n",
    "operator_no_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producing_operator_nos_list = producing_operator_nos.tolist()\n",
    "og_operator_dw_df[og_operator_dw_df[\"operator_no\"].isin(producing_operator_nos_list)]\n",
    "\n",
    "# get the operator_name of the producing_operator_nos\n",
    "# filtered_df = og_operator_dw_ddf[\n",
    "#     og_operator_dw_ddf[\"operator_no\"].isin(producing_operator_nos)\n",
    "# ].persist()\n",
    "\n",
    "# filtered_df.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table Regulatory Lease DW : og_regulatory_lease_dw\n",
    "\n",
    "This table contains identifying lease information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"og_regulatory_lease_dw\"\n",
    ")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_regulatory_lease_dw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dask dataframe\n",
    "# og_regulatory_lease_dw_ddf = dd.read_csv(file_path_12, sep=\"}\", dtype=csv_dtypes)\n",
    "og_regulatory_lease_dw_ddf = data_tables_dict[\"og_regulatory_lease_dw\"]\n",
    "og_regulatory_lease_dw_df = data_tables_dict[\"og_regulatory_lease_dw\"].compute()\n",
    "# convert the column names to lower case\n",
    "og_regulatory_lease_dw_ddf.columns = [\n",
    "    pascal_to_snake(col) for col in og_regulatory_lease_dw_ddf.columns\n",
    "]\n",
    "# create column is_gas based on the oil_gas_code column\n",
    "og_regulatory_lease_dw_ddf[\"is_gas\"] = (\n",
    "    og_regulatory_lease_dw_ddf[\"oil_gas_code\"].map({\"G\": True, \"O\": False}).astype(bool)\n",
    ")\n",
    "# drop the columns we do not need to keep\n",
    "pattern = re.compile(r\"field_name|operator_name|district_name|oil_gas_code|field_name\")\n",
    "columns_to_keep = [\n",
    "    col for col in og_regulatory_lease_dw_ddf.columns if not pattern.search(col)\n",
    "]\n",
    "# drop the column from the dataframe\n",
    "og_regulatory_lease_dw_ddf = og_regulatory_lease_dw_ddf[columns_to_keep]\n",
    "# add in a lease_no_district_no column\n",
    "og_regulatory_lease_dw_ddf[\"lease_no_district_no\"] = og_regulatory_lease_dw_ddf[\n",
    "    \"lease_no\"\n",
    "].astype(str) + og_regulatory_lease_dw_ddf[\"district_no\"].astype(str).str.zfill(2)\n",
    "\n",
    "# get the string type columns\n",
    "string_columns = [\n",
    "    col\n",
    "    for col in og_regulatory_lease_dw_ddf.columns\n",
    "    if og_regulatory_lease_dw_ddf[col].dtype == \"string\"\n",
    "]\n",
    "# display(string_columns)\n",
    "# display(og_regulatory_lease_dw_ddf.dtypes)\n",
    "for col in string_columns:\n",
    "    og_regulatory_lease_dw_ddf[col] = og_regulatory_lease_dw_ddf[col].str.strip()\n",
    "\n",
    "# get the number of rows and memory usage\n",
    "num_rows = og_regulatory_lease_dw_ddf.shape[0]\n",
    "\n",
    "\n",
    "memory_usage = og_regulatory_lease_dw_ddf.memory_usage(index=True, deep=True).sum()\n",
    "num_rows, memory_usage = dask.compute(num_rows, memory_usage)\n",
    "# show first few rows and shape\n",
    "print(\n",
    "    f\"Shape of og_regulatory_lease_dw_ddf: ({num_rows} , {len(columns_to_keep)})\\nTotal memory usage: {format_in_B(memory_usage)}\"\n",
    ")\n",
    "og_regulatory_lease_dw_ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_regulatory_lease_dw_df = og_regulatory_lease_dw_ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_regulatory_lease_dw_df[\n",
    "    og_regulatory_lease_dw_df[\"lease_no_district_no\"].str.contains(r\"^277801$\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_regulatory_lease_dw_df = og_regulatory_lease_dw_ddf.compute()\n",
    "\n",
    "\n",
    "counts_df = (\n",
    "    og_regulatory_lease_dw_df.groupby([\"district_no\", \"is_gas\"])[\"lease_no\"]\n",
    "    .count()\n",
    "    .reset_index()\n",
    "    .set_index(\"district_no\")\n",
    ")\n",
    "counts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df[counts_df[\"is_gas\"] == 1][\"lease_no\"].hvplot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lease_count_bars(gas):\n",
    "    filtered_data = og_regulatory_lease_dw_df[\n",
    "        og_regulatory_lease_dw_df[\"is_gas\"] == gas\n",
    "    ]\n",
    "    counts_df = filtered_data.groupby(\n",
    "        \"district_no\")[\"lease_no\"].count().reset_index()\n",
    "    counts_df.columns = [\"district_no\", \"lease_count\"]\n",
    "\n",
    "    return counts_df.hvplot.bar(\n",
    "        x=\"district_no\",\n",
    "        y=\"lease_count\",\n",
    "        xlabel=\"\",\n",
    "        ylabel=\"\",\n",
    "        title=f\"Gas Leases\" if gas else f\"Oil Leases\",\n",
    "    ).opts(\n",
    "        toolbar=\"above\",\n",
    "        active_tools=[\"box_zoom\"],\n",
    "    )\n",
    "\n",
    "\n",
    "is_gas_slider = pnw.IntSlider(name=\"is_gas\", start=0, end=1, step=1, value=0)\n",
    "is_gas_checkbox = pnw.Checkbox(name=\"gas_lease\", value=False)\n",
    "\n",
    "ibars = pn.panel(pn.bind(plot_lease_count_bars,\n",
    "                 gas=is_gas_checkbox), width=880)\n",
    "\n",
    "pn.Card(is_gas_checkbox, ibars, title=\"Lease Count by District No.\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lease_count = (\n",
    "    og_regulatory_lease_dw_ddf.compute()\n",
    "    .groupby([\"district_no\", \"is_gas\"])[\"lease_no\"]\n",
    "    .count()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "# create a dynamic map to plot the number of leases in each district bar plot and is_gas widget controller\n",
    "def lease_count_bars_with_labels(var1, col1):\n",
    "    # Filter the data for the given 'is_gas' value\n",
    "    data = lease_count[(lease_count[col1] == var1)]\n",
    "\n",
    "    # Create a bar plot\n",
    "    bar_plot = data.hvplot.bar(\n",
    "        x=\"district_no\",\n",
    "        y=\"lease_no\",\n",
    "        stacked=True,\n",
    "        xlabel=\"\",\n",
    "        yaxis=\"bare\",\n",
    "        title=f\"Gas Leases\" if var1 else f\"Oil Leases\",\n",
    "        hover_cols=\"all\",\n",
    "    )\n",
    "\n",
    "    # Create a list to hold the text elements\n",
    "    texts = []\n",
    "\n",
    "    # Loop over the DataFrame and create a text element for each row\n",
    "    for row in data.itertuples():\n",
    "        # The position of the text is center of bar\n",
    "        x = getattr(row, \"district_no\")\n",
    "        y = getattr(row, \"lease_no\") * 1.01\n",
    "        # The text is the height of the bar\n",
    "        text = str(getattr(row, \"lease_no\"))\n",
    "        # Create the text element and add it to the list\n",
    "        texts.append(hv.Text(x, y, text, valign=\"bottom\").opts(text_color=\"black\"))\n",
    "\n",
    "    # Overlay the text elements on the plot\n",
    "    labelled_plot = bar_plot * hv.Overlay(texts)\n",
    "\n",
    "    return labelled_plot\n",
    "\n",
    "\n",
    "# Checkbox widget to control the 'is_gas' value\n",
    "is_gas_checkbox = pnw.Checkbox(name=\" Checkbox for Gas Leases\", value=False, width=250)\n",
    "# bind the 'lease_count_bars_with_labels' function to the 'is_gas' checkbox and create a DynamicMap\n",
    "lease_dmap = hv.DynamicMap(\n",
    "    pn.bind(\n",
    "        lease_count_bars_with_labels, var1=is_gas_checkbox.param.value, col1=\"is_gas\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "pn.Card(is_gas_checkbox, lease_dmap, title=\"Lease Count by District No.\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 13 - Well Completion\n",
    "\n",
    "This table contains identifying well completion information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"og_well_completion\"\n",
    ")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_well_completion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dask dataframe\n",
    "# og_well_completion_ddf = dd.read_csv(file_path_13, sep=\"}\", dtype=csv_dtypes)\n",
    "og_well_completion_df = data_tables_dict[\"og_well_completion\"].compute()\n",
    "# convert the column names to lower case\n",
    "og_well_completion_df.columns = [\n",
    "    pascal_to_snake(col) for col in og_well_completion_df.columns\n",
    "]\n",
    "og_well_completion_df[\"is_gas\"] = og_well_completion_df[\"oil_gas_code\"].map(\n",
    "    {\"G\": True, \"O\": False}\n",
    ")\n",
    "\n",
    "# drop the columns we do not need to keep\n",
    "pattern = re.compile(r\"_name|oil_gas_code|assc_cnty$\")\n",
    "columns_to_keep = [\n",
    "    col for col in og_well_completion_df.columns if not pattern.search(col)\n",
    "]\n",
    "# drop the column from the dataframe\n",
    "og_well_completion_df = og_well_completion_df[columns_to_keep]\n",
    "\n",
    "# create column lease_no_district_no which combines the lease_no and district_no\n",
    "og_well_completion_df[\"lease_no_district_no\"] = og_well_completion_df[\n",
    "    \"lease_no\"\n",
    "].astype(str) + og_well_completion_df[\"district_no\"].astype(str).str.zfill(2)\n",
    "\n",
    "# create column tx_api which combines the api_county_code and api_unique_no\n",
    "og_well_completion_df[\"tx_api\"] = og_well_completion_df[\"api_county_code\"].astype(\n",
    "    str\n",
    ").str.zfill(3) + og_well_completion_df[\"api_unique_no\"].astype(str).str.zfill(5)\n",
    "# create a column for the tx_api count\n",
    "og_well_completion_df[\"tx_api_count\"] = og_well_completion_df.groupby(\"tx_api\")[\n",
    "    \"tx_api\"\n",
    "].transform(\"count\")\n",
    "og_well_completion_df[\"lease_no_district_no_count\"] = og_well_completion_df.groupby(\n",
    "    \"lease_no_district_no\"\n",
    ")[\"lease_no_district_no\"].transform(\"count\")\n",
    "\n",
    "\n",
    "# get the number of rows and memory usage\n",
    "num_rows = og_well_completion_df.shape[0]\n",
    "memory_usage = og_well_completion_df.memory_usage(index=True, deep=True).sum()\n",
    "# num_rows, memory_usage = dask.compute(num_rows, memory_usage)\n",
    "# show first few rows and shape\n",
    "print(f\"Shape of og_well_completion_df: ({num_rows} , {len(columns_to_keep)}\")\n",
    "print(f\"Total memory usage: {format_in_B(memory_usage)}\")\n",
    "og_well_completion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_well_completion_df[\n",
    "    og_well_completion_df.lease_no_district_no.isin(top_n_gas_ld)\n",
    "].sort_values([\"tx_api_count\", \"tx_api\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table Summary Onshore Lease\n",
    "\n",
    "Summary table. (Used for query purposes on the leases in onshore counties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"og_summary_onshore_lease\"\n",
    ")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_summary_onshore_lease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active leases\n",
    "og_summary_onshore_lease_df = data_tables_dict[\"og_summary_onshore_lease\"].compute(\n",
    ")\n",
    "# convert the column names to lower case\n",
    "og_summary_onshore_lease_df.columns = [\n",
    "    pascal_to_snake(col) for col in og_summary_onshore_lease_df.columns\n",
    "]\n",
    "# create column is_gas based on the oil_gas_code column\n",
    "og_summary_onshore_lease_df[\"is_gas\"] = og_summary_onshore_lease_df[\"oil_gas_code\"].map(\n",
    "    {\"G\": True, \"O\": False}\n",
    ")\n",
    "\n",
    "# create a lease_no_district_no column\n",
    "og_summary_onshore_lease_df[\"lease_no_district_no\"] = og_summary_onshore_lease_df[\n",
    "    \"lease_no\"\n",
    "].astype(str) + og_summary_onshore_lease_df[\"district_no\"].astype(str).str.zfill(2)\n",
    "\n",
    "# drop the columns we do not need to keep\n",
    "columns_op_drop = [\"oil_gas_code\", \"lease_no\", \"district_no\"]\n",
    "og_summary_onshore_lease_df = og_summary_onshore_lease_df.drop(\n",
    "    columns_op_drop, axis=1)\n",
    "\n",
    "\n",
    "# get the number of rows and memory usage\n",
    "num_rows = og_summary_onshore_lease_df.shape[0]\n",
    "memory_usage = og_summary_onshore_lease_df.memory_usage(\n",
    "    index=True, deep=True).sum()\n",
    "# num_rows, memory_usage = dask.compute(num_rows, memory_usage)\n",
    "# show first few rows and shape\n",
    "print(\n",
    "    f\"Shape of og_summary_onshore_lease_df: ({num_rows} , {len(og_summary_onshore_lease_df.columns)}\"\n",
    ")\n",
    "print(f\"Total memory usage: {format_in_B(memory_usage)}\")\n",
    "og_summary_onshore_lease_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active wells have a cycle_year_month_max  of 202311\n",
    "og_summary_onshore_lease_df_2013 = og_summary_onshore_lease_df[\n",
    "    og_summary_onshore_lease_df[\"cycle_year_month_max\"] > 201300\n",
    "]\n",
    "og_summary_onshore_lease_df_2013[\"owner_count\"] = 0\n",
    "og_summary_onshore_lease_df_2013[\"owner_count\"] = (\n",
    "    og_summary_onshore_lease_df_2013[[\"owner_count\", \"lease_no_district_no\"]]\n",
    "    .groupby(\"lease_no_district_no\")[\"lease_no_district_no\"]\n",
    "    .transform(\"count\")\n",
    ")\n",
    "og_summary_onshore_lease_df_2013.sort_values(\n",
    "    by=[\"owner_count\", \"lease_no_district_no\",\n",
    "        \"field_no\", \"cycle_year_month_max\"],\n",
    "    ascending=False,\n",
    ").head(50)\n",
    "\n",
    "# top producer test\n",
    "og_summary_onshore_lease_df_2013[\n",
    "    og_summary_onshore_lease_df_2013[\"lease_no_district_no\"].isin(top_n_gas_ld)\n",
    "].sort_values(\n",
    "    [\"owner_count\", \"lease_no_district_no\", \"field_no\", \"cycle_year_month_max\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_summary_onshore_lease_df[\n",
    "    og_summary_onshore_lease_df[\"lease_no_district_no\"].str.contains(\n",
    "        r\"^253302$\", regex=True\n",
    "    )\n",
    "].sort_values(\"cycle_year_month_min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_lease = og_lease_cycle_ddf[[\"field_no\", \"lease_no_district_no\"]].compute()\n",
    "\n",
    "field_lease_dict = defaultdict(set)\n",
    "\n",
    "field_lease.drop_duplicates(inplace=True)\n",
    "\n",
    "for row in field_lease.itertuples():\n",
    "\n",
    "    f = getattr(row, \"field_no\")\n",
    "\n",
    "    l = getattr(row, \"lease_no_district_no\")\n",
    "\n",
    "    field_lease_dict[f].add(l)\n",
    "\n",
    "\n",
    "my_keys = list(field_lease_dict.keys())\n",
    "\n",
    "for i in range(len(my_keys)):\n",
    "\n",
    "    key1 = my_keys[i]\n",
    "\n",
    "    values1 = field_lease_dict[key1]\n",
    "\n",
    "    for j in range(i + 1, len(my_keys)):\n",
    "\n",
    "        key2 = my_keys[j]\n",
    "\n",
    "        values2 = field_lease_dict[key2]\n",
    "\n",
    "        common_values = values1.intersection(values2)\n",
    "\n",
    "        if common_values:\n",
    "\n",
    "            print(f\"Field {key1} has common values with field {key2}: {common_values}\")\n",
    "\n",
    "            found_common = True\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_lease_cycle_ddf[og_lease_cycle_ddf[\"lease_no_district_no\"]\n",
    "                   == 27780101].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_summary_onshore_lease_df[\n",
    "    og_summary_onshore_lease_df[\"lease_no_district_no\"].str.contains(\n",
    "        \"27780101\")\n",
    "].sort_values(\"cycle_year_month_min\")\n",
    "\n",
    "og_well_completion_df[\n",
    "    og_well_completion_df[\"lease_no_district_no\"].str.contains(r\"^253302$\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_lease_cycle_ddf[og_lease_cycle_ddf[\"field_no\"] == 39744500].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_well_completion_df[og_well_completion_df[\"tx_api\"] == \"39130424\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the dask dataframe to a pandas dataframe but only the columns county_no and lease_no_district_no\n",
    "my_df = og_county_lease_cycle_ddf_2019[[\"lease_no_district_no\", \"county_no\"]].compute()\n",
    "\n",
    "# create a defaultdict with set values\n",
    "county_dict = defaultdict(set)\n",
    "# iterate over the rows of the dataframe and add the lease_no_district_no to the set of the county_no dict\n",
    "for row in my_df.itertuples():\n",
    "    county_dict[getattr(row, \"county_no\")].add(getattr(row, \"lease_no_district_no\"))\n",
    "\n",
    "county_dict\n",
    "{key: len(county_dict[key]) for key in county_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Ranked Operators in Texas per Monthly Production numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the aggregated Operator Cycle table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for 20XX\n",
    "og_operator_cycle_ddf_20XX = og_operator_cycle_df[\n",
    "    og_operator_cycle_df.cycle_year_month > 201800\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get each of the prod_vol columns\n",
    "oper_prod_vol_cols = [\n",
    "    \"oper_oil_prod_vol\",\n",
    "    \"oper_gas_prod_vol\",\n",
    "    \"oper_cond_prod_vol\",\n",
    "    \"oper_csgd_prod_vol\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_operators = og_operator_cycle_ddf_20XX.groupby(\n",
    "    [\"cycle_year_month\", \"operator_name\"]\n",
    ").size()\n",
    "\n",
    "# Check if any operator_name appears more than once in the same cycle_year_month\n",
    "any_duplicates = any(duplicate_operators > 1)\n",
    "if not any_duplicates:\n",
    "    print(\"No operator_name appears more than once in the same cycle_year_month\")\n",
    "else:\n",
    "    print(\"There are some duplicate operator_name in the same cycle_year_month\")\n",
    "\n",
    "# Calculate the total production volume by summing the prod_columns\n",
    "og_operator_cycle_ddf_20XX[\"total_prod_vol\"] = og_operator_cycle_ddf_20XX[\n",
    "    oper_prod_vol_cols\n",
    "].sum(axis=1)\n",
    "\n",
    "# Filter the data for cycle_year_month 202308 and non-zero total_prod_vol\n",
    "active_oper_202308 = og_operator_cycle_ddf_20XX.loc[\n",
    "    (og_operator_cycle_ddf_20XX[\"cycle_year_month\"] == 202308)\n",
    "    & (og_operator_cycle_ddf_20XX[\"total_prod_vol\"] != 0)\n",
    "]\n",
    "print(f\"Number of active operators in 202308: {active_oper_202308.shape[0]}\")\n",
    "active_oper_202308.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame\n",
    "grouped_og_operator = og_operator_cycle_ddf_20XX.groupby(\"cycle_year_month\")\n",
    "\n",
    "\n",
    "# Define a function to get the top n rows for each product column\n",
    "def get_topn(df, n):\n",
    "    result = {}\n",
    "    for col in oper_prod_vol_cols:\n",
    "        topn = df.nlargest(n, col)[[\"operator_name\"] + oper_prod_vol_cols].copy()\n",
    "        topn[\"prod_column\"] = col\n",
    "        result[col] = topn\n",
    "    return pd.concat(result.values(), keys=result.keys())\n",
    "\n",
    "\n",
    "# Apply the function to each group\n",
    "topn_og_operator = grouped_og_operator.apply(get_topn, n=2)\n",
    "\n",
    "# Reset the index and drop the level_1 and level_2 columns\n",
    "topn_og_operator.reset_index().drop(columns=[\"level_1\", \"level_2\"]).sort_values(\n",
    "    by=[\"cycle_year_month\", \"prod_column\"], ascending=False\n",
    ").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_operators_df = pd.DataFrame(\n",
    "    topn_og_operator[\"operator_name\"].unique(), columns=[\"operator_name\"]\n",
    ").merge(operator_no_name, on=\"operator_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_operators_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top ranked operator for each product column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Lease Cycle table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare result to if we had worked from the og_lease_cycle production table\n",
    "\n",
    "# create column for the total production to filter out the non-producing operators\n",
    "og_lease_cycle_ddf[\"total_lease_prod_vol\"] = (\n",
    "    og_lease_cycle_ddf[\"lease_oil_prod_vol\"]\n",
    "    + og_lease_cycle_ddf[\"lease_gas_prod_vol\"]\n",
    "    + og_lease_cycle_ddf[\"lease_cond_prod_vol\"]\n",
    "    + og_lease_cycle_ddf[\"lease_csgd_prod_vol\"]\n",
    ")\n",
    "\n",
    "# get the lease production table and filter out the rows with cycle_year_month > 202308 and 0 production\n",
    "filtered_og_lease_cycle_ddf = og_lease_cycle_ddf.loc[\n",
    "    (og_lease_cycle_ddf.cycle_year_month <= 202308)\n",
    "    & (og_lease_cycle_ddf.cycle_year_month > 201800)\n",
    "    & (og_lease_cycle_ddf[\"total_lease_prod_vol\"] > 0)\n",
    "]\n",
    "\n",
    "# group by the operator and the cycle_year_month and sum the prod_vol columns\n",
    "grouped_og_operator_no = (\n",
    "    filtered_og_lease_cycle_ddf[\n",
    "        [\n",
    "            \"cycle_year_month\",\n",
    "            \"operator_no\",\n",
    "            \"lease_oil_prod_vol\",\n",
    "            \"lease_gas_prod_vol\",\n",
    "            \"lease_cond_prod_vol\",\n",
    "            \"lease_csgd_prod_vol\",\n",
    "        ]\n",
    "    ]\n",
    "    .groupby([\"cycle_year_month\", \"operator_no\"])[\n",
    "        \"lease_oil_prod_vol\",\n",
    "        \"lease_gas_prod_vol\",\n",
    "        \"lease_cond_prod_vol\",\n",
    "        \"lease_csgd_prod_vol\",\n",
    "    ]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_grouped_og_operator_no = grouped_og_operator_no.compute()\n",
    "computed_grouped_og_operator_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lease_prod_cols = [\n",
    "    \"lease_oil_prod_vol\",\n",
    "    \"lease_gas_prod_vol\",\n",
    "    \"lease_cond_prod_vol\",\n",
    "    \"lease_csgd_prod_vol\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = 2\n",
    "# create a empty list to store the max rows\n",
    "top_rows = []\n",
    "# go through each of the prod_columns and get the max row for each cycle_year_month\n",
    "for col in lease_prod_cols:\n",
    "    computed_grouped_og_operator_no[\"prod_column\"] = col\n",
    "    topn_indices = (\n",
    "        computed_grouped_og_operator_no.groupby(\"cycle_year_month\")[col]\n",
    "        .nlargest(topn)\n",
    "        .index\n",
    "    )\n",
    "    for _, idx in topn_indices:\n",
    "        # take the idx from topn_indices and\n",
    "        top_rows.append(computed_grouped_og_operator_no.loc[idx].to_frame().T)\n",
    "\n",
    "\n",
    "top_og_operator_to_compare = pd.concat(top_rows)\n",
    "print(f\"Top {topn} operators aggregated for each product and cycle_year_month:\")\n",
    "# merge the operator_no_name table to get the operator_name\n",
    "top_og_operator_to_compare.merge(operator_no_name, on=\"operator_no\").sort_values(\n",
    "    by=[\"cycle_year_month\", \"prod_column\"], ascending=False\n",
    ").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active leases are those which have a production for 202308\n",
    "active_leases = (\n",
    "    filtered_og_lease_cycle_ddf[[\"cycle_year_month\", \"lease_no_district_no\"]][\n",
    "        filtered_og_lease_cycle_ddf[\"cycle_year_month\"] == 202308\n",
    "    ][\"lease_no_district_no\"]\n",
    "    .unique()\n",
    "    .to_frame()\n",
    ")\n",
    "\n",
    "# get the cumulative production for each of active_leases\n",
    "active_og_lease_cycle_ddf = filtered_og_lease_cycle_ddf.merge(\n",
    "    active_leases, how=\"inner\", on=\"lease_no_district_no\"\n",
    ")\n",
    "\n",
    "# persist the cumulative production for each of active_leases\n",
    "persisted_active_og_lease_cycle_ddf = active_og_lease_cycle_ddf.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persisted_active_og_lease_cycle_ddf[\n",
    "    [\"lease_no_district_no\", \"cycle_year_month\", \"operator_no\", \"field_no\"]\n",
    "    + lease_prod_cols\n",
    "].merge(operator_no_name, on=\"operator_no\", how=\"left\").sort_values(\n",
    "    by=[\"cycle_year_month\", \"lease_oil_prod_vol\"], ascending=False\n",
    ").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_lease_sums = (\n",
    "    persisted_active_og_lease_cycle_ddf.groupby(\n",
    "        \"lease_no_district_no\")[lease_prod_cols]\n",
    "    .sum()\n",
    "    .compute()\n",
    ")\n",
    "active_lease_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top producing leases\n",
    "\n",
    "\n",
    "# concat the 10 largest active_lease_sums for each product together and then remove the duplicates\n",
    "top_active_leases = pd.concat(\n",
    "    [active_lease_sums.nlargest(10, col) for col in lease_prod_cols]\n",
    ").reset_index()\n",
    "top_active_leases = top_active_leases.drop_duplicates(\n",
    "    subset=\"lease_no_district_no\", keep=\"first\"\n",
    ")\n",
    "top_active_leases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Line Time Series Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.utils.plotting import plot_series\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.arima import AutoARIMA\n",
    "from sktime.forecasting.model_selection import (\n",
    "    ForecastingGridSearchCV,\n",
    "    SlidingWindowSplitter,\n",
    ")\n",
    "from sktime.forecasting.compose import MultiplexForecaster\n",
    "from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "from sktime.forecasting.var import VAR\n",
    "from sktime.performance_metrics.forecasting import MeanSquaredError\n",
    "from sktime.split import (\n",
    "    ExpandingWindowSplitter,\n",
    "    SingleWindowSplitter,\n",
    "    SlidingWindowSplitter,\n",
    "    temporal_train_test_split,\n",
    ")\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mask for the producing operators with all non-zero production\n",
    "producing_operators_mask = (og_operator_cycle_df[oper_prod_vol_cols] != 0).all(axis=1)\n",
    "# get the producing operators\n",
    "y = og_operator_cycle_df[producing_operators_mask][\n",
    "    [\"cycle_year_month\", \"operator_no\"] + oper_prod_vol_cols\n",
    "]\n",
    "\n",
    "# get the list of the producing operators with all non-zero production\n",
    "producing_all_operators_nos = y[\"operator_no\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[\"cycle_year_month\"] = pd.to_datetime(\n",
    "    y[\"cycle_year_month\"], format=\"%Y%m\", errors=\"coerce\"\n",
    ")\n",
    "y[\"cycle_year_month\"] = y[\"cycle_year_month\"].astype(\"period[M]\")\n",
    "y[\"cycle_year_month\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # og_operator_cycle_df\n",
    "y = y.set_index([\"operator_no\", \"cycle_year_month\"])\n",
    "y.sort_index(level=[\"operator_no\", \"cycle_year_month\"], inplace=True)\n",
    "new_y = y[y.index.get_level_values(\"operator_no\").isin(top_operators_df[\"operator_no\"])]\n",
    "y_train, y_test = temporal_train_test_split(new_y, test_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oper_no = 20528\n",
    "# create a dictionary of the operator_no and the dataframe of the operator_no\n",
    "y_dict = defaultdict(pd.DataFrame)\n",
    "# add the filtered dataframe to the dictionary\n",
    "y_dict[oper_no] = (\n",
    "    y[y[\"operator_no\"] == oper_no]\n",
    "    .set_index(\"cycle_year_month\")\n",
    "    .drop(columns=[\"operator_no\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = temporal_train_test_split(y=y_dict[oper_no], test_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fh = ForecastingHorizon(y_test.index, is_relative=False).to_relative(\n",
    "#     cutoff=y_train.index[-1]\n",
    "# )\n",
    "fh = ForecastingHorizon(list(range(6)), is_relative=False).to_absolute()\n",
    "\n",
    "cv = SingleWindowSplitter(fh=fh, window_length=len(y_train) - 6)\n",
    "cv = ExpandingWindowSplitter(fh=fh, initial_window=12, step_length=1)\n",
    "cv = SlidingWindowSplitter(fh=fh, window_length=12, step_length=1)\n",
    "\n",
    "cv.get_n_splits(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the VAR forecaster\n",
    "forecaster_var = VAR()\n",
    "forecaster_var.fit(y_train, fh=fh)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = forecaster_var.predict()\n",
    "print(f\"Predicted values: {y_pred}\")\n",
    "\n",
    "# Calculate mean squared error\n",
    "mse_score = MeanSquaredError()\n",
    "mse = mse_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Create a DataFrame for predicted values\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=y_test.columns, index=y_test.index)\n",
    "\n",
    "# Display the predicted values DataFrame\n",
    "print(\"Predicted values:\")\n",
    "display(y_pred_df)\n",
    "\n",
    "# display the the actual values, y_test\n",
    "print(\"Actual values:\")\n",
    "display(y_test)\n",
    "\n",
    "# display the difference between the actual and predicted values\n",
    "y_error = y_test - y_pred_df\n",
    "print(\"Difference between actual and predicted values:\")\n",
    "display((y_pred_df - y_test).round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the train, predicted, and test values\n",
    "for prod in oper_prod_vol_cols:\n",
    "    plot_series(\n",
    "        y_train[prod],\n",
    "        y_pred_df[prod],\n",
    "        y_test[prod],\n",
    "        labels=[\"Train\", \"Predicted\", \"Test\"],\n",
    "        title=f\"Production Volume for {prod}\",\n",
    "    )\n",
    "\n",
    "\n",
    "# overlay = (\n",
    "#     y_train.hvplot.scatter(\n",
    "#         x=\"cycle_year_month\", y=\"oper_oil_prod_vol\", logy=True, label=\"Train\"\n",
    "#     )\n",
    "#     * y_pred.hvplot.scatter(\n",
    "#         x=\"cycle_year_month\", y=\"oper_oil_prod_vol\", logy=True, label=\"Predicted\"\n",
    "#     )\n",
    "#     * y_test.hvplot.scatter(\n",
    "#         x=\"cycle_year_month\", y=\"oper_oil_prod_vol\", logy=True, label=\"Test\"\n",
    "#     )\n",
    "# )\n",
    "# overlay.opts(\n",
    "#     legend_position=\"bottom_right\",\n",
    "#     width=800,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_forecaster = ExponentialSmoothing()\n",
    "param_grid = {\n",
    "    \"sp\": [4, 6, 12],\n",
    "    \"seasonal\": [\"add\", \"mul\"],\n",
    "    \"trend\": [\"add\", \"mul\"],\n",
    "    \"damped_trend\": [True, False],\n",
    "}\n",
    "\n",
    "\n",
    "gscv = ForecastingGridSearchCV(\n",
    "    es_forecaster,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    param_grid=param_grid,\n",
    "    scoring=MeanSquaredError(square_root=True),\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "gscv.fit(y_train)\n",
    "# strt off with a blank dataframe os that no data is carried over\n",
    "y_pred = pd.DataFrame()\n",
    "y_pred = gscv.predict(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the train, predicted, and test values\n",
    "for prod in oper_prod_vol_cols:\n",
    "    plot_series(\n",
    "        y_train[prod],\n",
    "        y_pred[prod],\n",
    "        y_test[prod],\n",
    "        labels=[\"Train\", \"Predicted\", \"Test\"],\n",
    "        title=f\"Production Volume for {prod}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best params: {gscv.best_params_}\")\n",
    "print(f\"Best forecaster: {gscv.best_forecaster_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the forecast and the actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcaster = MultiplexForecaster(\n",
    "    forecasters=[\n",
    "        (\n",
    "            \"naive\",\n",
    "            NaiveForecaster(strategy=\"last\"),\n",
    "        ),\n",
    "        (\"ets\", ExponentialSmoothing(trend=\"add\", sp=12)),\n",
    "    ],\n",
    ")\n",
    "forecaster_param_grid = {\"selected_forcaster\": [\"naive\", \"ets\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = client.scheduler_info()  # get scheduler info\n",
    "\n",
    "workers = info[\"workers\"]  # get the workers\n",
    "\n",
    "print(f\"Number of workers: {len(workers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
