{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This project is part of a Capstone project for Springboard Data Science Career Track.\n",
    "\n",
    "The goal of this project is to develop a machine learning model to rank and predict the likelihood that an oil company will initiate a frac job in a county within the Permian Basin in the first quarter of 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Import statements\n",
    "\n",
    "from collections import defaultdict\n",
    "from http.client import IncompleteRead\n",
    "from time import sleep\n",
    "import concurrent.futures as cf\n",
    "from datetime import datetime\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import tempfile\n",
    "import warnings\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import URLError\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import missingno as msno\n",
    "import cartopy.crs as ccrs\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "import geoviews as gv\n",
    "import geoviews.tile_sources as gts\n",
    "import colorcet as cc\n",
    "import holoviews as hv\n",
    "import hvplot.pandas  # noqa\n",
    "import hvplot.dask  # noqa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyproj\n",
    "from fiona.io import ZipMemoryFile\n",
    "from pyvis.network import Network\n",
    "\n",
    "# from sqlalchemy import create_engine\n",
    "# from sqlalchemy.exc import SQLAlchemyError\n",
    "from tqdm import tqdm\n",
    "import dask\n",
    "from dask import persist\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client\n",
    "import panel as pn\n",
    "import panel.widgets as pnw\n",
    "from panel.template import FastListTemplate\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "gv.extension(\"bokeh\")\n",
    "pn.extension(\"tabulator\", template=\"fast\", sizing_mode=\"stretch_width\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Test initial print statement\n",
    "print(\"CapstoneJourney begins!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants\n",
    "Let's start by defining some constants that will be used throughout this notebook.\n",
    "\n",
    "Most of the data was first downloaded from external websites and then uploaded onto a cloud storage bucket. This was done to ensure consistency and availability during the project. A brief description of the data and its original source link is referenced below.\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "The following table provides an overview of the data sources used in this project:\n",
    "\n",
    "| Dataset Name | Source URL | Original Source | Description | Date Downloaded |\n",
    "|--------------|------------|-----------------|-------------|-----------------|\n",
    "| RegistryUpload Table | [link](https://fracfocus.org/data-download) | FracFocus | This table contains each disclosureâ€™s header information such as the job date, API number, location, base water volume, and total vertical depth. | 2023-11-11 |\n",
    "| RBDMSWells | [link](https://gisdata-occokc.opendata.arcgis.com/datasets/OCCOKC::rbdms-wells/about) | Oklahoma Corporation Commission | This table contains Oklahoma RBDMS statewide well data | 2023-11-23 |\n",
    "| Wolfcamp Delaware Play Boundary | [link](https://www.eia.gov/maps/maps.htm)| EIA | Permian Basin, Delaware Sub-Basin: Wolfcamp play boundary (9/4/2018) | 2023-11-19 |\n",
    "| Wolfcamp Midland Play Boundaries | [link](https://www.eia.gov/maps/maps.htm)| EIA | Wolfcamp A, B, C, and D play boundaries, Midland Basin (6/4/2020) | 2023-11-21 |\n",
    "| ShalePlay Delaware | [link](https://www.eia.gov/maps/maps.htm)| EIA |Delaware play boundary (10/8/2019)  | 2023-11-21 |\n",
    "| AboYeso GlorietaYeso Spraberry | [link](https://www.eia.gov/maps/maps.htm)| EIA | Abo-Yeso, Glorieta-Yeso, and Spraberry play boundaries (3/11/2016) | 2023-11-21 |\n",
    "| NM SLO OilGas Leases | [link](https://www.nmstatelands.org/maps-gis/gis-data-download/)| New Mexico State Land Office | Active Oil and Gas Leases (11/07/2023) | 2023-11-21 |\n",
    "| NM SLO Geologic Regions | [link](https://www.nmstatelands.org/maps-gis/gis-data-download/)| New Mexico State Land Office | Geologic Regions (01/04/2010) | 2023-11-21 |\n",
    "| NM SLO STL Status Combined | [link](https://www.nmstatelands.org/maps-gis/gis-data-download/)| New Mexico State Land Office | New Mexico State Trust Lands By Subdivision (04/14/2022) | 2023-11-21 |\n",
    "| Production Data Query Dump| [link](https://rrc.texas.gov/resource-center/research/data-sets-available-for-download/)| Railroad Commission of Texas | Production Data Query Dump (11/17/2023) | 2023-11-17 |\n",
    "| All Layers By County | [link](https://rrc.texas.gov/resource-center/research/data-sets-available-for-download/)  | Railroad Commission of Texas | Map & Associated Data: Base Map, Wells, Surveys & Pipelines layers | 2023-11-17 |\n",
    "| Oil & Gas Leases | [link](https://www.glo.texas.gov/land/land-management/gis/index.html) | Texas General Land Office | Active Leases (11/17/2023) | 2023-11-17 |\n",
    "| Oil & Gas Units | [link](https://www.glo.texas.gov/land/land-management/gis/index.html) | Texas General Land Office | Active Units (11/17/2023) | 2023-11-17 |\n",
    "| U.S. County Boundaries | [link](https://www2.census.gov/geo/tiger/TIGER2022/COUNTY/tl_2022_us_county.zip) | United States Census Bureau | County (2022-10-31). Data is downloaded directly in the code. | N/A |\n",
    "| U.S. County FIPS Codes | [link](https://en.wikipedia.org/wiki/List_of_United_States_FIPS_codes_by_county) | Wikipedia | List of United States FIPS codes by county. Data is downloaded directly in the code. | N/A |\n",
    "\n",
    "Each row in the table represents a different dataset. The columns are:\n",
    "\n",
    "- **Dataset Name**: The name of the dataset.\n",
    "- **Source URL**: The URL where the dataset can be downloaded. Click on \"link\" to access the webpage.\n",
    "- **Original Source**: The original source of the data.\n",
    "- **Description**: A brief description of the dataset.\n",
    "- **Date Downloaded**: The date when the dataset was downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "# This cell generates lists of URLs to CSV files stored in a Google Cloud Storage bucket.\n",
    "# The CSV files contain data from the FracFocus Chemical Disclosure Registry.\n",
    "\n",
    "# Generate a list of URLs to the FracFocusRegistry CSV files.\n",
    "# There are 24 files in total, named FracFocusRegistry_i.csv where i ranges from 1 to 24.\n",
    "DATA_URLS1 = [\n",
    "    f\"https://storage.googleapis.com/mrprime_dataset/fracfocus/FracFocusRegistry_{i}.csv\"\n",
    "    for i in range(1, 25)\n",
    "]\n",
    "\n",
    "# Generate a list of URLs to the registryupload CSV files.\n",
    "# There are 3 files in total, named registryupload_i.csv where i ranges from 1 to 3.\n",
    "DATA_URLS2 = [\n",
    "    f\"https://storage.googleapis.com/mrprime_dataset/fracfocus/registryupload_{j}.csv\"\n",
    "    for j in range(1, 4)\n",
    "]\n",
    "\n",
    "# URL to the readme.txt file in the bucket.\n",
    "DATA_README_URL = [\n",
    "    \"https://storage.googleapis.com/mrprime_dataset/fracfocus/readme.txt\"\n",
    "]\n",
    "\n",
    "# url to the OCC (Oklahoma) well data in th bucket\n",
    "OCC_PARQUET_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/occ/rbdms_wells.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Url for the shapefile for US counties from the Census Bureau's website.\n",
    "CENSUS_COUNTY_MAP_URL = (\n",
    "    \"https://www2.census.gov/geo/tiger/TIGER2022/COUNTY/tl_2022_us_county.zip\"\n",
    ")\n",
    "# Url for a Wikipedia page containing a table of FIPS codes for US counties.\n",
    "FIPS_WIKI_URL = (\n",
    "    \"https://en.wikipedia.org/wiki/List_of_United_States_FIPS_codes_by_county\"\n",
    ")\n",
    "# Bounds of the continental US in longitude and latitude.\n",
    "USA_BOUNDS = (-124.77, 24.52, -66.95, 49.38)\n",
    "# bounds of the continental US in Web Mercator coordinates.\n",
    "USA_BOUNDS_MERCATOR = (-13874905.0, 2870341.0, -7453304.0, 6338219.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# url for the shapefiles of Permian Basin, Delaware Sub-Basin: Wolfcamp play boundary\n",
    "WOLFCAMP_ZIP_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/eia/Wolfcamp_Delaware_Play_Boundary.zip\"\n",
    "MIDLAND_ZIP_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/eia/Wolfcamp_Midland_Play_Boundaries_EIA.zip\"\n",
    "DELAWARE_ZIP_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/eia/ShalePlay_Delaware_EIA.zip\"\n",
    "ABOYESO_ZIP_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/eia/ShalePlays_AboYeso_GlorietaYeso_Spraberry_EIA.zip\"\n",
    "# PB_ZIP_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/eia/PermianBasin_Boundary_Structural_Tectonic.zip\"\n",
    "\n",
    "basins_url_list = [\n",
    "    WOLFCAMP_ZIP_URL,\n",
    "    MIDLAND_ZIP_URL,\n",
    "    DELAWARE_ZIP_URL,\n",
    "    ABOYESO_ZIP_URL,\n",
    "    # PB_ZIP_URL,\n",
    "]\n",
    "\n",
    "\n",
    "# url for shapefiles of Polygon data set intended to delineate active oil and gas leases on New Mexico State Trust Lands.\n",
    "NM_SLO_OIL_LEASE_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/nm_slo/OilGas_Leases.zip\"\n",
    "\n",
    "# url for shapefiles of Polygon layer created to highlight general boundaries of subsurface geologic basins and uplifts of New Mexico\n",
    "NM_SLO_GEO_REGION_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/nm_slo/slo_GeologicRegions.zip\"\n",
    "# url for shapefiles of Polygons of New Mexico State Trust Lands by PLSS subdivision (quarter-quarter, lot, tract, or partial).\n",
    "NM_SLO_STL_PLSS_URL = \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/nm_slo/slo_STLStatusCombined.zip\"\n",
    "\n",
    "nm_slo_url_list = [\n",
    "    NM_SLO_OIL_LEASE_URL,\n",
    "    NM_SLO_GEO_REGION_URL,\n",
    "]  # , NM_SLO_STL_PLSS_URL]\n",
    "\n",
    "# Production data query for RRC website\n",
    "PDQ_URL = (\n",
    "    \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/rrc/PDQ_DSV.zip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Define a list of county numbers that we want to test. These numbers correspond to counties\n",
    "# that we did not include in the data folder, but they do not cover all 254 counties.\n",
    "\n",
    "# county numbers are only odd numbers\n",
    "county_nums = [str(i).zfill(3) for i in range(1, 508) if i % 2]\n",
    "\n",
    "# Generate a list of URLs to the shapefile zip files stored in a Google Cloud Storage bucket.\n",
    "# The zip files are named Shp{num}.zip, where {num} is a county number from the county_nums list.\n",
    "SHP_ZIP_URLS = [\n",
    "    f\"https://storage.googleapis.com/mrprime_dataset/capstone_journey/rrc/all_layers_rrc_20231117/Shp{num}.zip\"\n",
    "    for num in county_nums\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# url for the active leases in Texas on State land gdb\n",
    "GDB_ZIP_URLS = [\n",
    "    \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/glo/GDB_ActiveLeases.zip\",\n",
    "    \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/glo/GDB_ActiveUnits.zip\",\n",
    "    # \"https://storage.googleapis.com/mrprime_dataset/capstone_journey/glo/GDB_InactiveLeases.zip\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function definitions\n",
    "Next, let's define some functions that will be used throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def get_county_data():\n",
    "    county = gpd.read_file(CENSUS_COUNTY_MAP_URL)[\n",
    "        [\"GEOID\", \"STATEFP\", \"COUNTYFP\", \"NAME\", \"geometry\"]\n",
    "    ]\n",
    "    county.columns = county.columns.str.lower()\n",
    "    return county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def read_csv_concurrent(urls_list):\n",
    "    \"\"\"Reads a list of CSV files concurrently\"\"\"\n",
    "    # Create a thread pool\n",
    "    with cf.ThreadPoolExecutor() as executor:\n",
    "        # Use map to apply pd.read_csv to each URL\n",
    "        results = list(tqdm(executor.map(pd.read_csv, urls_list), total=len(urls_list)))\n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def extract_specific_gdf_from_local_zip(\n",
    "    zip_paths: list[str], regex_patterns: list[str]\n",
    ") -> dict[str, gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Reads shapefiles from a list of zip files and returns a dictionary\n",
    "    where the keys are the names of the shapefiles and the values are GeoDataFrames.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store the GeoDataFrames\n",
    "    shp_dict = {}\n",
    "    # compile the regex patterns\n",
    "    patterns = [re.compile(pattern) for pattern in regex_patterns]\n",
    "\n",
    "    # Loop over the list of zip file paths\n",
    "    for zip_path in zip_paths:\n",
    "        # Open the zip file\n",
    "        with ZipFile(zip_path) as z:\n",
    "            # Get the list of files in the zip file\n",
    "            zip_contents = z.namelist()\n",
    "            # Filter the list to get only the shapefiles that match any of the patterns\n",
    "            shp_files = [\n",
    "                f\n",
    "                for f in zip_contents\n",
    "                for pattern in patterns\n",
    "                if pattern.search(f) and f.endswith(\".shp\")\n",
    "            ]\n",
    "            # read the shapefiles into GeoDataFrames\n",
    "            for shp_file in shp_files:\n",
    "                # Get the name of the shapefile\n",
    "                shp_name = Path(shp_file).stem\n",
    "                # Read the shapefile into a GeoDataFrame and add it to the dictionary\n",
    "                shp_dict[shp_name] = gpd.read_file(f\"zip://{zip_path}!{shp_file}\")\n",
    "    # Return the dictionary of GeoDataFrames\n",
    "    return shp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def extract_matching_shp_files_from_zip_urls(\n",
    "    zip_urls: list[str], regex_patterns: list[str]\n",
    ") -> dict[str, gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Reads shapefiles from a list of zip file urls and returns a dictionary\n",
    "    where the keys are the names of the shapefiles and the values are GeoDataFrames.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store the GeoDataFrames\n",
    "    shp_dict = {}\n",
    "    # compile the regex patterns\n",
    "    patterns = [re.compile(pattern) for pattern in regex_patterns]\n",
    "\n",
    "    # Loop over the list of zip file urls\n",
    "    for zip_url in tqdm(zip_urls, desc=\"Processing zip files\"):\n",
    "        # download the zip file\n",
    "        with urlopen(zip_url) as u:\n",
    "            zip_data = u.read()\n",
    "        # create a ZipMemoryFile from the zip data\n",
    "        with ZipMemoryFile(zip_data) as z:\n",
    "            # get the list of files in the zip file\n",
    "            zip_files = z.listdir()\n",
    "            # filter for shapefiles that match any of the patterns\n",
    "            shp_files = [\n",
    "                f\n",
    "                for f in zip_files\n",
    "                for pattern in patterns\n",
    "                if pattern.search(f) and f.endswith(\".shp\")\n",
    "            ]\n",
    "            # read the shapefiles into GeoDataFrames\n",
    "            for shp_file in shp_files:\n",
    "                with z.open(shp_file) as f:\n",
    "                    shp_dict[Path(shp_file).stem] = gpd.GeoDataFrame.from_features(\n",
    "                        f, crs=f.crs\n",
    "                    )\n",
    "    # Return the dictionary of GeoDataFrames\n",
    "    return shp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def process_zip_url(\n",
    "    zip_url: str, patterns: list[re.Pattern]\n",
    ") -> Optional[dict[str, gpd.GeoDataFrame]]:\n",
    "    \"\"\"Downloads a zip file url and returns a dictionary of GeoDataFrames for shapefiles that match the patterns\"\"\"\n",
    "    shp_dict = {}\n",
    "    retries = 5\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            with urlopen(zip_url) as u:\n",
    "                zip_data = u.read()\n",
    "            with ZipMemoryFile(zip_data) as z:\n",
    "                zip_files = z.listdir()\n",
    "                shp_files = [\n",
    "                    f\n",
    "                    for f in zip_files\n",
    "                    for pattern in patterns\n",
    "                    if pattern.search(f) and f.endswith(\".shp\")\n",
    "                ]\n",
    "                for shp_file in shp_files:\n",
    "                    with z.open(shp_file) as f:\n",
    "                        shp_dict[Path(shp_file).stem] = gpd.GeoDataFrame.from_features(\n",
    "                            f, crs=f.crs\n",
    "                        )\n",
    "            return shp_dict\n",
    "        except (IncompleteRead, URLError) as e:\n",
    "            print(f\"Error: {e} on try {i+1} of {retries} for {zip_url}\")\n",
    "            if i < retries - 1:\n",
    "                sleep(2)\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "\n",
    "def extract_matching_shp_files_from_zip_urls_concurrent(\n",
    "    zip_urls: list[str], regex_patterns: list[str]\n",
    ") -> dict[str, gpd.GeoDataFrame]:\n",
    "    \"\"\"Reads shapefiles from a list of zip file urls and returns a dictionary\n",
    "    where the keys are the names of the shapefiles and the values are GeoDataFrames.\"\"\"\n",
    "    shp_dict = {}\n",
    "    patterns = [re.compile(pattern) for pattern in regex_patterns]\n",
    "    with cf.ThreadPoolExecutor() as executor:\n",
    "        future_to_url = {\n",
    "            executor.submit(process_zip_url, url, patterns): url for url in zip_urls\n",
    "        }\n",
    "        futures = tqdm(\n",
    "            cf.as_completed(future_to_url),\n",
    "            total=len(future_to_url),\n",
    "            desc=\"Processing URLs\",\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                shp_dict.update(result)\n",
    "    return shp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def concat_gdf_from_dict(gdf_dict: dict[str, gpd.GeoDataFrame]) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Given a dictionary of GeoDataFrames, returns a single GeoDataFrame\n",
    "    with a new column indicating the source of the data.\n",
    "    \"\"\"\n",
    "    # use a dictionary comprehension to create a new dictionary\n",
    "    gdf_data = {k: gdf.assign(source_file=k) for k, gdf in gdf_dict.items()}\n",
    "    # return the concatenated GeoDataFrame\n",
    "    return pd.concat(gdf_data.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def extract_gdfs_from_zip(zip_path: str) -> Optional[dict[str, gpd.GeoDataFrame]]:\n",
    "    \"\"\"\n",
    "    Reads shapefiles from a zip file and returns a dictionary of GeoDataFrames.\n",
    "    \"\"\"\n",
    "    gdfs = {}\n",
    "    # Open the zip file\n",
    "    with ZipFile(zip_path) as z:\n",
    "        # Get the list of files in the zip file\n",
    "        zip_contents = z.namelist()\n",
    "        # Find the shapefiles\n",
    "        shp_files = [f for f in zip_contents if f.endswith(\".shp\")]\n",
    "        for shp_file in shp_files:\n",
    "            # Read the shapefile into a GeoDataFrame\n",
    "            gdf = gpd.read_file(f\"zip://{zip_path}!{shp_file}\")\n",
    "            gdfs[shp_file] = gdf\n",
    "\n",
    "    # If no shapefile was found, return None\n",
    "    return gdfs if gdfs else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def extract_gdfs_from_zip_url(zip_url: str) -> Optional[dict[str, gpd.GeoDataFrame]]:\n",
    "    \"\"\"\n",
    "    Downloads a ZIP file from a URL, reads shapefiles from the ZIP file, and returns a dictionary of GeoDataFrames.\n",
    "    \"\"\"\n",
    "    gdfs = {}\n",
    "    # Open the URL\n",
    "    with urlopen(zip_url) as u:\n",
    "        # Read the content of the response into a byte stream\n",
    "        zip_data = u.read()\n",
    "        # Open the ZIP file from the byte stream\n",
    "        with ZipMemoryFile(zip_data) as z:\n",
    "            # Get the list of files in the ZIP file\n",
    "            zip_contents = z.listdir()\n",
    "            # Find the shapefiles\n",
    "            shp_files = [f for f in zip_contents if f.endswith(\".shp\")]\n",
    "            for shp_file in shp_files:\n",
    "                # Read the shapefile into a GeoDataFrame\n",
    "                with z.open(shp_file) as f:\n",
    "                    gdf = gpd.GeoDataFrame.from_features(f, crs=f.crs)\n",
    "                gdfs[Path(shp_file).stem] = gdf\n",
    "\n",
    "    # If no shapefile was found, return None\n",
    "    return gdfs if gdfs else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def process_shp_url(zip_url: str):\n",
    "    \"\"\"Downloads a zip file url and returns a dictionary of GeoDataFrames for shapefiles that match the patterns\"\"\"\n",
    "    shp_dict = {}\n",
    "    with urlopen(zip_url) as u:\n",
    "        zip_data = u.read()\n",
    "    with ZipMemoryFile(zip_data) as z:\n",
    "        zip_files = z.listdir()\n",
    "        shp_files = [f for f in zip_files if f.endswith(\".shp\")]\n",
    "        for shp_file in shp_files:\n",
    "            with z.open(shp_file) as f:\n",
    "                shp_dict[Path(shp_file).stem] = gpd.GeoDataFrame.from_features(\n",
    "                    f, crs=f.crs\n",
    "                )\n",
    "    return shp_dict\n",
    "\n",
    "\n",
    "def extract_gdfs_from_zip_url_concurrent(\n",
    "    zip_urls: list[str],\n",
    ") -> dict[str, gpd.GeoDataFrame]:\n",
    "    \"\"\"Reads shapefiles from a list of zip file urls and returns a dictionary\n",
    "    where the keys are the names of the shapefiles and the values are GeoDataFrames.\"\"\"\n",
    "    shp_dict = {}\n",
    "    with cf.ThreadPoolExecutor() as executor:\n",
    "        future_to_url = {executor.submit(process_shp_url, url): url for url in zip_urls}\n",
    "        futures = tqdm(\n",
    "            cf.as_completed(future_to_url),\n",
    "            total=len(future_to_url),\n",
    "            desc=\"Processing URLs\",\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "        for future in futures:\n",
    "            shp_dict.update(future.result())\n",
    "    return shp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def read_gdb_from_zip(gdb_zips_list: list[str]):\n",
    "    \"\"\"Reads a list of zip files containing geodatabases and returns a dictionary of GeoDataFrames\"\"\"\n",
    "    # initialize an empty dictionary\n",
    "    gdb_dict = {}\n",
    "    # loop through each zip file\n",
    "    for gdb_zip in gdb_zips_list:\n",
    "        with ZipFile(gdb_zip, \"r\") as z:\n",
    "            # get list of files in zip\n",
    "            files = z.namelist()\n",
    "            # filter for gdb folders\n",
    "            gdb_folders = [f for f in files if f.endswith(\".gdb/\")]\n",
    "            # if there is a gdb folder in the zip file\n",
    "            if gdb_folders:\n",
    "                # get it and read it into a GeoDataFrame\n",
    "                gdb_folder = gdb_folders[0]\n",
    "                gdb_dict[Path(gdb_folder).stem] = gpd.read_file(\n",
    "                    f\"zip://{gdb_zip}!{gdb_folder}\"\n",
    "                ).to_crs(\"EPSG:4269\")\n",
    "    # return the dictionary of GeoDataFrames\n",
    "    return gdb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def read_gdb_from_zip_url(gdb_urls_list: list[str]):\n",
    "    \"\"\"Reads a list of zip file urls containing geodatabases and returns a dictionary of GeoDataFrames\"\"\"\n",
    "    # initialize an empty dictionary\n",
    "    gdb_dict = {}\n",
    "    # loop through each zip file\n",
    "    for gdb_url in gdb_urls_list:\n",
    "        # create a temporary directory\n",
    "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "            # download the zip file\n",
    "            with urlopen(gdb_url) as u, open(f\"{tmp_dir}/data.zip\", \"wb\") as f_out:\n",
    "                f_out.write(u.read())\n",
    "            # extract the zip file\n",
    "            with ZipFile(f\"{tmp_dir}/data.zip\", \"r\") as zip_ref:\n",
    "                zip_ref.extractall(tmp_dir)\n",
    "            # get the list of extracted files\n",
    "            extracted_files = list(Path(tmp_dir).iterdir())\n",
    "            # filter for gdb folders\n",
    "            gdb_folders = [f for f in extracted_files if f.suffix == \".gdb\"]\n",
    "            # if there is a gdb folder in the extracted files\n",
    "            if gdb_folders:\n",
    "                # get it and read it into a GeoDataFrame\n",
    "                gdb_folder = gdb_folders[0]\n",
    "                gdb_dict[Path(gdb_folder).stem] = gpd.read_file(gdb_folder).to_crs(\n",
    "                    \"EPSG:4269\"\n",
    "                )\n",
    "    # return the dictionary of GeoDataFrames\n",
    "    return gdb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def process_gdb_url(gdb_url):\n",
    "    \"\"\"Downloads a zip file url containing a geodatabase and returns a GeoDataFrame\"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        # download the zip file\n",
    "        with urlopen(gdb_url) as u, open(f\"{tmp_dir}/data.zip\", \"wb\") as f_out:\n",
    "            f_out.write(u.read())\n",
    "        # extract the zip file\n",
    "        with ZipFile(f\"{tmp_dir}/data.zip\", \"r\") as zip_ref:\n",
    "            zip_ref.extractall(tmp_dir)\n",
    "        # get the list of extracted files\n",
    "        extracted_files = list(Path(tmp_dir).iterdir())\n",
    "        # filter for gdb folders\n",
    "        gdb_folders = [f for f in extracted_files if f.suffix == \".gdb\"]\n",
    "        # if there is a gdb folder in the extracted files\n",
    "        if gdb_folders:\n",
    "            # get it and read it into a GeoDataFrame\n",
    "            gdb_folder = gdb_folders[0]\n",
    "            return Path(gdb_folder).stem, gpd.read_file(gdb_folder)\n",
    "\n",
    "\n",
    "def read_gdb_from_zip_url_concurrent(gdb_urls_list: list[str]):\n",
    "    \"\"\"Reads a list of zip file urls containing geodatabases and returns a dictionary of GeoDataFrames\"\"\"\n",
    "    # initialize an empty dictionary\n",
    "    gdb_dict = {}\n",
    "    # create a ThreadPoolExecutor\n",
    "    with cf.ThreadPoolExecutor() as executor:\n",
    "        # submit the process_gdb_url function for each url and gather the results\n",
    "        future_to_url = {\n",
    "            executor.submit(process_gdb_url, url): url for url in gdb_urls_list\n",
    "        }\n",
    "        for future in cf.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                key, data = future.result()\n",
    "                gdb_dict[key] = data\n",
    "            except Exception as exc:\n",
    "                print(f\"{url} generated an exception: {exc}\")\n",
    "    # return the dictionary of GeoDataFrames\n",
    "    return gdb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "def pascal_to_snake(name: str):\n",
    "    \"\"\"Converts a string from PascalCase to snake_case\"\"\"\n",
    "    # (?<=[A-Za-z0-9]) - positive lookbehind for any alphanumeric character\n",
    "    # (?=[A-Z][a-z]) - positive lookahead for any uppercase followed by lowercase\n",
    "    pattern = re.compile(r\"(?<=[A-Za-z0-9])(?=[A-Z][a-z])\")\n",
    "    name = pattern.sub(\"_\", name).lower()\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def plot_statistics_table_nonmissing_hbar(df):\n",
    "    # Calculate the percentage of non-missing values in each column\n",
    "    missing_data_percent = (df.notnull().mean()).rename(\"Percent\")\n",
    "\n",
    "    # Create a DataFrame of the counts of non-missing values\n",
    "    if isinstance(df, dd.DataFrame):\n",
    "        non_missing_count, missing_data_percent = dask.compute(\n",
    "            df.count().rename(\"Count\"), (missing_data_percent * 100)\n",
    "        )\n",
    "    else:\n",
    "        missing_data_percent = missing_data_percent * 100\n",
    "        non_missing_count = df.notnull().sum().rename(\"Count\")\n",
    "\n",
    "    # Concatenate the two DataFrames along the columns\n",
    "    non_missing_data = pd.concat([missing_data_percent, non_missing_count], axis=1)\n",
    "\n",
    "    # Create a horizontal bar plot of the percentage of non-missing data\n",
    "    hbar_plot = non_missing_data.hvplot.barh(\n",
    "        y=\"Percent\",\n",
    "        width=800,\n",
    "        height=600,\n",
    "        title=\"Percentage of Non-Missing Data in Each Column\",\n",
    "        ylabel=\"\",\n",
    "        xlabel=\"\",\n",
    "        xaxis=\"bare\",\n",
    "        hover_cols=\"all\",\n",
    "    ).opts(\n",
    "        active_tools=[\"box_zoom\"],\n",
    "        toolbar=\"above\",\n",
    "    )\n",
    "\n",
    "    return hbar_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def unify_crs(\n",
    "    dataframe: pd.DataFrame,\n",
    "    lon_col: str = \"longitude\",\n",
    "    lat_col: str = \"latitude\",\n",
    "    crs_col: str = \"crs\",\n",
    "    final_crs: str = \"EPSG:4269\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a DataFrame with lon/lat or x/y coordinates,\n",
    "    converts the coordinates to a unified crs and combines\n",
    "    into a single GeoDataframe with a geometry column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the main columns that will be used for the conversion\n",
    "    main_cols = [lon_col, lat_col, crs_col]\n",
    "\n",
    "    # Get the other columns in the dataframe\n",
    "    other_cols = list(set(dataframe.columns) - set(main_cols))\n",
    "\n",
    "    # Create a subframe with only the main columns\n",
    "    subframe = dataframe[main_cols]\n",
    "\n",
    "    # Create a list of GeoDataFrames, each with a different CRS\n",
    "    geo_dfs = [\n",
    "        gpd.GeoDataFrame(\n",
    "            # Use the data for this CRS\n",
    "            data=data,\n",
    "            # Create a geometry column from the lon/lat columns\n",
    "            geometry=gpd.points_from_xy(x=data[lon_col].values, y=data[lat_col].values),\n",
    "            # Set the CRS for this GeoDataFrame\n",
    "            crs=pyproj.CRS(crs_val),\n",
    "            # Convert the GeoDataFrame to the final CRS\n",
    "        ).to_crs(final_crs)\n",
    "        # Do this for each unique CRS in the subframe\n",
    "        for crs_val, data in subframe.groupby(crs_col)\n",
    "    ]\n",
    "\n",
    "    # Merge the GeoDataFrames back together and return the result\n",
    "    return pd.merge(\n",
    "        # Concatenate the GeoDataFrames\n",
    "        pd.concat(geo_dfs, sort=True),\n",
    "        # Add the other columns back in\n",
    "        dataframe[other_cols],\n",
    "        # Merge on the index\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# @lru_cache(maxsize=3)\n",
    "def get_background_map(bgcolor=\"black\", alpha=0.5):\n",
    "    \"\"\"Returns a GeoViews background map\"\"\"\n",
    "    return gts.CartoLight().opts(bgcolor=bgcolor, alpha=alpha)\n",
    "\n",
    "\n",
    "def platecaree_to_mercator_vectorised(x, y):\n",
    "    \"\"\"Use Cartopy to convert PlateCarree coordinates to Mercator\"\"\"\n",
    "    return ccrs.GOOGLE_MERCATOR.transform_points(ccrs.PlateCarree(), x, y)[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def format_in_000(num):\n",
    "    \"\"\"Formats a number in thousands\"\"\"\n",
    "    for unit in [\"\", \"thousand\", \"million\", \"billion\", \"trillion\"]:\n",
    "        if abs(num) < 1000.0:\n",
    "            return f\"{num:3.2f} {unit}\"\n",
    "        num /= 1000.0\n",
    "    return f\"{num:.2f} quadrillion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def split_datetime(df, column):\n",
    "    \"\"\"Splits a datetime column into year, month, and day columns\"\"\"\n",
    "    # remove '_date' from the column name\n",
    "    column_stem = column.replace(\"_date\", \"\") if \"_date\" in column else column\n",
    "    try:\n",
    "        datetime_series = pd.to_datetime(df[column], errors=\"coerce\")\n",
    "        if datetime_series.isna().any():\n",
    "            print(f\"Errors occurred during conversion of column {column}.\")\n",
    "        df[column_stem + \"_year\"] = datetime_series.dt.year\n",
    "        df[column_stem + \"_month\"] = datetime_series.dt.month\n",
    "        df[column_stem + \"_day\"] = datetime_series.dt.day\n",
    "    except KeyError:\n",
    "        print(f\"Column {column} not found in the DataFrame.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First dataset is from FracFocus. There is also a readme file which contains the data dictionary for the dataset. Let's have a look at both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readme file with data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# get readme data\n",
    "readme = urlopen(DATA_README_URL[0]).read().decode(\"windows-1252\")\n",
    "display(readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print function goes beyond 'hello world' and takes care of the escape characters\n",
    "print(readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# We can collect all the dataframe into a list and then concatenate them\n",
    "df_list = read_csv_concurrent(DATA_URLS2)\n",
    "\n",
    "dfs = pd.concat(df_list).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "registry_df = pd.DataFrame()\n",
    "registry_df = dfs.copy()\n",
    "registry_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the missing values it is interesting to see that most missing values are from the `TVD`, `TotalBaseWaterVolume` and `TotalBaseNonWaterVolume`. One reason for this may be found in the data limitations on terms of use on the FracFocus website. It states:\n",
    "-  Disclosures submitted using the FracFocus 1.0 format (January, 2011 to May 31, 2013) will contain only header data. \n",
    "-  Disclosures submitted using the FracFocus 2.0 format (November 2012 to present) will contain both header and chemical data. NOTE: Between November, 2012 and May 31, 2013 disclosures in both 1.0 and 2.0 formats were submitted to the system. \n",
    "-  After May 31, 2013 only disclosures submitted in the 2.0 format were accepted.\n",
    "-  Data submitted appears as it was submitted by the operator or operatorâ€™s authorized agent. FracFocus does not warrant the data in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the missing data\n",
    "plot_statistics_table_nonmissing_hbar(registry_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of non-missing values in each column\n",
    "missing_data_percent = (registry_df.notna().mean() * 100).rename(\"Percent\")\n",
    "\n",
    "# Create a DataFrame of the counts of non-missing values\n",
    "non_missing_count = registry_df.notna().sum().rename(\"Count\")\n",
    "\n",
    "# Concatenate the two DataFrames along the columns\n",
    "non_missing_data = pd.concat([missing_data_percent, non_missing_count], axis=1)\n",
    "\n",
    "# Create a horizontal bar plot of the percentage of non-missing data\n",
    "barh_plot = non_missing_data.hvplot.barh(\n",
    "    y=\"Percent\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    title=\"Percentage of Non-Missing Data in Each Column\",\n",
    "    ylabel=\"\",\n",
    "    xlabel=\"\",\n",
    "    xaxis=\"bare\",\n",
    "    hover_cols=\"all\",\n",
    ").opts(\n",
    "    active_tools=[\"box_zoom\"],\n",
    "    toolbar=\"above\",\n",
    ")\n",
    "\n",
    "barh_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some of the rows of the dataframe\n",
    "display(registry_df.head(3))\n",
    "display(registry_df.sample(5, random_state=628))\n",
    "display(registry_df.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our first look at a few sample rows some things stick out immediately.\n",
    "1. The dataset may be in chronological order and the values of the `JobStartDate`/`JobEndDate` at both of the extremes may be incorrect.\n",
    "2. There may be an abundance for `StateNumber` `42` if 4 out of the 5 draws of the 200k+ rows drawn at random had a `StateNumber` of `42`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we jump into cleaning the data in the columns, let's make the columns look more pythonic by changing the column names to snake_case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "registry_df.columns = [pascal_to_snake(col) for col in registry_df.columns]\n",
    "registry_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can remove the columns with only null values. These are the last 2 columns in the dataframe, `source` and `dtmod`. Also we can drop the `total_non_base_water_volume` column since we may not have much need for it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "registry_df = registry_df.drop(\n",
    "    columns=[\"source\", \"dtmod\", \"total_base_non_water_volume\"]\n",
    ")\n",
    "registry_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will fix some of the dtypes of the columns.\n",
    "- Both the `job_start_date` and the `job_end_date` columns are object dtypes, so we will convert those to datetime dtypes and drop the timestamp.\n",
    "- We can also separate out the date components into its various components. This may come in handy for feature engineering later on.\n",
    "- The `projection` column is an object dtype. That can be converted to a string dtype and shorten to `crs` as it represents the Cooordinate Reference System used in the `latitude` and `longitude` columns values. We can dig into what CRS is later on.\n",
    "- The `federal_well` and `indian_well` columns are both boolean type columns. They may be more aptly named as `is_federal_well` and `is_indian_well` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Use the function on 'job_start_date' and 'job_end_date'\n",
    "split_datetime(registry_df, \"job_start_date\")\n",
    "split_datetime(registry_df, \"job_end_date\")\n",
    "registry_df[[col for col in registry_df.columns if re.search(\"start|end\", col)]].info(\n",
    "    memory_usage=\"deep\"\n",
    ")\n",
    "# show the values which are null still\n",
    "registry_df[\n",
    "    registry_df[[col for col in registry_df.columns if re.search(\"start|end\", col)]]\n",
    "    .isna()\n",
    "    .any(axis=1)\n",
    "].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Convert 'job_start_date' to datetime format and format it as 'YYYY-MM-DD'\n",
    "registry_df[\"job_start_date\"] = pd.to_datetime(\n",
    "    registry_df[\"job_start_date\"], errors=\"coerce\"\n",
    ").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Convert 'job_end_date' to datetime format and format it as 'YYYY-MM-DD'\n",
    "registry_df[\"job_end_date\"] = pd.to_datetime(\n",
    "    registry_df[\"job_end_date\"], errors=\"coerce\"\n",
    ").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# drop rows with null values in 'job_start_date' and 'job_end_date'\n",
    "# registry_df = registry_df.dropna(subset=[\"job_start_date\", \"job_end_date\"])\n",
    "\n",
    "\n",
    "# Rename some columns for clarity\n",
    "registry_df.rename(\n",
    "    columns={\n",
    "        \"federal_well\": \"is_federal_well\",\n",
    "        \"indian_well\": \"is_indian_well\",\n",
    "        \"projection\": \"crs\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Display the information of the DataFrame\n",
    "registry_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will look at the `api_number` column.\n",
    "We learned from the read me that \n",
    "> APINumber - The American Petroleum Institute well identification number formatted as follows xx-xxx-xxxxx0000 Where: \n",
    "> - First two digits represent the state, \n",
    "> - second three digits represent the county, \n",
    "> - third 5 digits represent the well.\n",
    "\n",
    "Theoretically, we could just grab the first two characters of the `APINnumber` and use that as the state number according to the definition of the `APINumber` above. Actually, that would not be a good idea, and here is why.<br>\n",
    "Although the column was called `APINumber`, it is not actually a number, so if it starts with a leading `0` that first character `0`, cannot be omitted from the value. Let's look at some of the rows with a single digit state numbers.\n",
    "\n",
    "Right now, \n",
    "- the `api_number` column is an object dtype, but a better option would be a `string` dtype, as `object` dtype can be mixed . We can also shorten that column name to `api`.\n",
    "- the `state_number` column and the `county_number` column are both `int64` dtypes right now. `string` type may be a stronger option.\n",
    "- `state_code` and `county_code` may be better names for the `state_number` and `county_number` columns respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows where the state_number is a single digit\n",
    "registry_df[\n",
    "    (registry_df[\"state_number\"] == 3) | (registry_df[\"state_number\"] == 5)\n",
    "].sample(5, random_state=628)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rows' `api_number` values have leading `0`, which is correct, but some do not. The rows without the leading `0` though are 13 characters long instead of 14. Maybe we can just add a leading `0` where needed until all API number values are 14 characters long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of characters in the api_number column\n",
    "registry_df[\"api_number\"].astype(\"string\").str.len().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most are 14 characters long, but some are 13 characters long, like the ones we saw above without the leading `0`. Let's assume the ones with 13 characters are missing the leading `0` and not something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Convert 'api' to string and pad it with zeros to make it 14 characters long\n",
    "registry_df[\"api\"] = registry_df[\"api_number\"].astype(\"string\").str.zfill(14)\n",
    "\n",
    "# Convert 'state_number' to string and pad it with zeros to make it 2 characters long\n",
    "registry_df[\"state_code\"] = registry_df[\"state_number\"].astype(\"string\").str.zfill(2)\n",
    "\n",
    "# Convert 'county_number' to string and pad it with zeros to make it 3 characters long\n",
    "registry_df[\"county_code\"] = registry_df[\"county_number\"].astype(\"string\").str.zfill(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# check which rows may have the api with the first two digits not matching the state number\n",
    "api_state_mismatch_mask = registry_df[\"state_code\"] != registry_df[\"api\"].str[0:2]\n",
    "# api_state_mismatch_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which rows may have the api with the first two digits not matching the state number\n",
    "registry_df[api_state_mismatch_mask][\n",
    "    [\"api_number\", \"api\", \"state_code\", \"state_name\", \"county_code\", \"county_name\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expected to get 2 rows here, since we checked the length of the `api_number` column above we saw that 1 row had 10 and another row had 12 characters. It is only two rows, so this may be an easy fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Remove leading zeros and pad to 14 digits on mismatches\n",
    "registry_df.loc[api_state_mismatch_mask, \"api\"] = (\n",
    "    registry_df.loc[api_state_mismatch_mask, \"api\"].str.lstrip(\"0\").str.ljust(14, \"0\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which rows may have the api with the first two digits not matching the state number\n",
    "registry_df[api_state_mismatch_mask][\n",
    "    [\"api_number\", \"api\", \"state_code\", \"state_name\", \"county_code\", \"county_name\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# check which rows may have the api with the 3-5 digits not matching the county number\n",
    "api_county_mismatch_mask = registry_df[\"county_code\"] != registry_df[\"api\"].str[2:5]\n",
    "registry_df[api_county_mismatch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State name should not have more than 50 possible values, given that there are only 50 states in the US. If we were to check the number of unique values in the `state_name` column, we would see 95. This is due to the variation in the way the `state_name` value is entered. Although not as obvious, we can assume the same for the `county_name` column. Luckily, the `api` includes both the `state_number` and the `county_number`. With this we can do \n",
    "1. data validation ensuring that these corresponding columns match\n",
    "2. Ensure that the `state_name` and the `county_name` columns are correct. Important to note that \n",
    "> The state codes used in an API number are DIFFERENT from another standard which is the Federal Information Processing Standard (FIPS) state code established in 1987 by NIST. ([source](https://en.wikipedia.org/wiki/API_well_number#State_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'Number of different values in state_name column: {registry_df[\"state_name\"].nunique()}'\n",
    ")\n",
    "print(\n",
    "    f'Number of different values in state_number column: {registry_df[\"state_number\"].nunique()}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# group by state_code and find the mode of the state_name\n",
    "state_code_mode = (\n",
    "    registry_df.groupby(\"state_code\")[\"state_name\"]\n",
    "    .apply(lambda x: x.mode().iloc[0])\n",
    "    .reset_index()\n",
    ")\n",
    "state_code_mode = state_code_mode.rename(columns={\"state_name\": \"state\"})\n",
    "state_code_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "registry_df = registry_df.merge(state_code_mode.rename(columns={\"state_name\": \"state\"}))\n",
    "registry_df.sample(3, random_state=628)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus our efforts in the most recent 10 years. Although more data is usually better, data too far in the past may distract whatever model we may build since unconventional drilling practices have really taken over the industry. We will also put our focus in one specific area, the Permian Basin. The Permian Basin has been instrumental in the shale boom transformation and is the most active area of exploration and production in the US presently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# create mask for from 2013 onwards\n",
    "post_2012_mask = registry_df[\"job_start_date\"] >= \"2013-01-01\"\n",
    "registry_df_post_2012 = registry_df[post_2012_mask].copy()\n",
    "\n",
    "# find all the rows with null values\n",
    "null_mask = registry_df_post_2012.isna().any(axis=1)\n",
    "registry_df_post_2012[null_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how many nans we still have in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_df_post_2012.info(memory_usage=\"deep\")\n",
    "registry_df_post_2012.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the `county_number` for the rows with a nan value in the `county_name` column, we can see why there is a nan for the `county_name`. Those numbers are most likely incorrect as small states like `North Dakota` and `Arkansas` do not have large `county_number` values. However we can still try to impute what the correct values by cross referencing with other sources or by using the `latitude` and `longitude` values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the rows with a null value for the county_name column\n",
    "registry_df_post_2012[registry_df_post_2012[\"county_name\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# get the index of one of the rows with a null value for the county_name column (3rd one down)\n",
    "index_vanorsdale = registry_df_post_2012.query(\"api_number == '03729439000000'\").index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oklahoma Commission Corporation (OOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some search engine investigating, we can learn that WFD Oil Corporation is a PC in Oklahoma. We also learn that the well name is `VANORSDOL` ,the well number is `#1-29`, and the API number is `3503729439` `0000`. We can correct some of the data which was entered incorrectly in FracFocus.\n",
    "\n",
    "The data are looking for is in this markdown cell so we can manually input it in but we will do it through code instead. We will query the api number in the data we have on the wells in OK.\n",
    "\n",
    "| Column Name | Value |\n",
    "| --- | --- |\n",
    "API\t|3503729439\n",
    "WELL_NAME|\tVANORSDOL\n",
    "WELL_NUM|\t#1-29\n",
    "OPERATOR|\tWFD OIL CORPORATION\n",
    "WELLSTATUS|\tAC\n",
    "WELLTYPE|\tOIL\n",
    "SH_LAT\t|35.749381\n",
    "SH_LON\t|-96.370355\n",
    "COUNTY\t|CREEK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# When reading the Parquet file\n",
    "occ_wells = pd.read_parquet(OCC_PARQUET_URL)\n",
    "# Convert the WKT column back to a geometry column\n",
    "occ_wells[\"geometry\"] = occ_wells[\"geometry\"].apply(lambda x: wkt.loads(x))\n",
    "\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame, specifying the CRS\n",
    "occ_wells = gpd.GeoDataFrame(\n",
    "    occ_wells, geometry=\"geometry\", crs=occ_wells[\"crs\"].iloc[0]\n",
    ")\n",
    "occ_wells.info()\n",
    "# look at 1 sample row of the dataframe\n",
    "occ_wells.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the 'api' column to string\n",
    "occ_wells[\"api\"] = occ_wells[\"api\"].astype(\"int64\").astype(\"string\")\n",
    "occ_wells.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# make a copy of the well_name column\n",
    "registry_df_post_2012[\"well\"] = registry_df_post_2012[\"well_name\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "registry_df_post_2012[registry_df_post_2012[\"api\"].str.contains(\"11533124\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# query the well_name column for 'vanors\n",
    "# occ_wells[occ_wells[\"well_name\"].str.contains(\"vanors\", case=False, na=False)]\n",
    "vanorsdol_row = occ_wells.query(\n",
    "    'well_name.fillna(\"\").str.contains(\"vanors\", case=False) & (api == \"3503729439\")',\n",
    ")[\n",
    "    [\n",
    "        \"api\",\n",
    "        \"well_name\",\n",
    "        \"well_num\",\n",
    "        \"operator\",\n",
    "        \"sh_lat\",\n",
    "        \"sh_lon\",\n",
    "        \"county\",\n",
    "    ]\n",
    "].rename(\n",
    "    columns={\"sh_lat\": \"latitude\", \"sh_lon\": \"longitude\"}\n",
    ")\n",
    "vanorsdol_row[\"well\"] = (\n",
    "    vanorsdol_row[\"well_name\"].str.title()\n",
    "    + \" \"\n",
    "    + vanorsdol_row[\"well_num\"].astype(\"string\")\n",
    ")\n",
    "index_vanorsdol = vanorsdol_row.index\n",
    "\n",
    "columns_to_replace = [\"api\", \"well\", \"latitude\", \"longitude\"]\n",
    "for col in columns_to_replace:\n",
    "    registry_df_post_2012.loc[index_vanorsdale, col] = vanorsdol_row.loc[\n",
    "        index_vanorsdol, col\n",
    "    ].values\n",
    "\n",
    "# check that the values have been replaced\n",
    "registry_df_post_2012.loc[index_vanorsdale, columns_to_replace]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the api column to 14 characters again\n",
    "registry_df_post_2012[\"api\"] = registry_df_post_2012[\"api\"].str.ljust(14, \"0\")\n",
    "\n",
    "registry_df_post_2012[registry_df_post_2012[\"county_name\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with `latitude` and `longitude` coordinates for all 4 rows with missing `county_name`, let's find out which counties they belong to spatially.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geodataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the boundary coordinates for all the counties in the US from the [census.gov](https://www.census.gov/) website. We saved the URL for this as `CENSUS_COUNTY_MAP_URL` at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# get the US countiesmap data\n",
    "county_gdf = get_county_data()\n",
    "county_gdf.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will scrape the FIPS table from wikipedia since the county dataframe does not have the state name and merge the 2 tables just for convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "fips_df = pd.read_html(FIPS_WIKI_URL)[1]\n",
    "fips_df.columns = [\"geoid\", \"county\", \"state\"]\n",
    "fips_df[\"geoid\"] = fips_df[\"geoid\"].astype(\"string\").str.zfill(5)\n",
    "fips_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "county_fips_gdf = county_gdf.merge(fips_df, on=\"geoid\")\n",
    "county_fips_gdf.sample(3, random_state=628)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick note about `GeoDataFrames`: they must have a column called `geometry` and this column contains the geometric objects. This call is what enables `geopandas` to perform spatial operations, and can also contain certain attributes like `.crs` which is the coordinate reference system.\n",
    "\n",
    "\n",
    "Commonly used datums in North America are NAD27, NAD83, and WGS84. More info [here](https://webhelp.esri.com/arcgisdesktop/9.3/index.cfm?TopicName=Projection_basics_the_GIS_professional_needs_to_know).<br>\n",
    "\n",
    "The county geodataframe uses `EPSG:4269` which is the EPSG code for the NAD83 coordinate system. Let's create a geodataframe with the `latitude` and `longitude` values that we have and put all of the points to the same CRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_fips_gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# ensures each row of the geodataframe is in the same CRS\n",
    "registry_gdf = unify_crs(registry_df_post_2012, crs_col=\"crs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# We can now perform a spatial join on the 2 GeoDataFrame.\n",
    "# fitler for rows with null county_name\n",
    "\n",
    "joined_gdf = (\n",
    "    registry_gdf[registry_gdf[\"county_name\"].isna()]\n",
    "    .sjoin(county_fips_gdf.drop(columns=[\"county\"]), how=\"left\", predicate=\"intersects\")\n",
    "    .drop(columns=[\"index_right\"])\n",
    ")\n",
    "joined_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `North Dakota` `county_number` should be `061`, which is `Mountrail` county, not `610`. \n",
    "- The `Utah` `county_number` though was actually correct. The error was the state number which should have been `42`, not `43`. This error is somewhat significant as according to the data dictionary:\n",
    "> APINumber - The American Petroleum Institute well identification number formatted as follows xx-xxx-xxxxx0000 Where: First two digits \n",
    "represent the state, second three digits represent the county, third 5 digits represent the well.<br>\n",
    "\n",
    "All this means is the `api` number is also incorrect. It should be `42317428660000` (<u><b>42</b></u>-317-42866-0000) instead of `43317428660000` (<u><b>43</b></u>-317-42866-0000).<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_code_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Let's corect theos values putting the county_code and\n",
    "registry_gdf[\"county\"] = registry_gdf[\"county_name\"].copy()\n",
    "\n",
    "# replace the county_code and county columns with the values from the joined_gdf\n",
    "registry_gdf.loc[joined_gdf.index, \"county\"] = joined_gdf[\"name\"]\n",
    "registry_gdf.loc[joined_gdf.index, \"county_code\"] = joined_gdf[\"countyfp\"]\n",
    "\n",
    "# change the api column of the last row in joined_gdf to 42317428660000 instead of 43317428660000\n",
    "# registry_gdf.loc[joined_gdf.index[-1], \"api\"] = \"42317428660000\"\n",
    "registry_gdf[\"api\"] = registry_gdf[\"api\"].replace(\"43317428660000\", \"42317428660000\")\n",
    "# correct the state_code values for where the api was changed\n",
    "registry_gdf[\"state_code\"] = registry_gdf[\"api\"].str[0:2]\n",
    "# Create a mapping from 'state_code' to 'state'\n",
    "state_mapping = state_code_mode.set_index(\"state_code\")[\"state\"].to_dict()\n",
    "\n",
    "# Use the mapping to update the 'state' column in 'registry_gdf'\n",
    "registry_gdf[\"state\"] = registry_gdf[\"state_code\"].map(state_mapping)\n",
    "\n",
    "\n",
    "# check that the values have been replaced\n",
    "registry_gdf[registry_gdf[\"county_name\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_gdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to impute the missing `county_name` values would have been by using the `well_name` in the `registry_df_post_2012` dataframe. Assuming the other wells on the same pad has the correct `state_code` and `state` values. However, this may not have worked with `Vanorsdol` as there's only one well with that name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_df_post_2012[\n",
    "    registry_df_post_2012[\"well_name\"].str.contains(\"vanors\", case=False, na=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show other wells with a similar name to rhea 1-6\n",
    "registry_df_post_2012[\n",
    "    registry_df_post_2012[\"well\"].str.contains(\"Rhea 1-6\", case=False, na=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_df_post_2012[registry_df_post_2012[\"well\"].str.contains(\"Trulson\", case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# county_fips_gdf[county_fips_gdf[\"state\"].isin(permian_states)]\n",
    "registry_df_post_2012[registry_df_post_2012[\"well\"].str.contains(\"palermo\", case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# make the well column uppercase to lower variation among for wells on the same pad\n",
    "registry_gdf[\"well\"] = registry_gdf[\"well\"].str.upper()\n",
    "# make the operator column uppercase to lower the variation among entry for the same operator\n",
    "registry_gdf[\"operator\"] = registry_gdf[\"operator_name\"].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_gdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we did for those 4 wells to find the `county` and `state` for which they belong to, we can do that for all the wells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# create 2 new columns, lat and lon in the registry_gdf for corrections\n",
    "registry_gdf[\"lat\"] = registry_gdf[\"latitude\"].copy()\n",
    "registry_gdf[\"lon\"] = registry_gdf[\"longitude\"].copy()\n",
    "\n",
    "# Check which other wells may have a state value which did not agree with the spatial join result\n",
    "registry_county_gdf = registry_gdf.sjoin(\n",
    "    county_fips_gdf, how=\"left\", predicate=\"intersects\"\n",
    ").drop(columns=[\"index_right\"])\n",
    "\n",
    "trimmed_column_set = [\n",
    "    \"api\",\n",
    "    \"well\",\n",
    "    \"state_left\",\n",
    "    \"state_right\",\n",
    "    \"county_left\",\n",
    "    \"county_right\",\n",
    "    \"county_code\",\n",
    "    \"countyfp\",\n",
    "    \"operator\",\n",
    "    \"lon\",\n",
    "    \"lat\",\n",
    "    \"geometry\",\n",
    "]\n",
    "\n",
    "# rows whichthe spatial join did not match for the both dataframes\n",
    "# mismatch_geo = registry_county_gdf[registry_county_gdf[\"state_right\"].isna()]\n",
    "mismatch_geo = registry_county_gdf[\n",
    "    registry_county_gdf[\"state_left\"] != registry_county_gdf[\"state_right\"]\n",
    "]\n",
    "print(f\"Number of rows: {len(mismatch_geo)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ones with a nan in the `state_right` column are those that we could not find a match for based on the `geometry`. We can match those for OK using the `api` number and see if the coordinates match for the wells in FracFocus match those from the OCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# get the rows with Oklahoma in the state_left column\n",
    "mismatch_geo_ok = mismatch_geo.query('state_left.str.contains(\"Oklahoma\")')\n",
    "print(f\"Number of rows from OK state: {len(mismatch_geo_ok)}\")\n",
    "if len(mismatch_geo_ok) > 1:\n",
    "    display(mismatch_geo_ok.sample(1, random_state=628))\n",
    "else:\n",
    "    display(\"No rows from OK state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Store the original index in a new column\n",
    "mismatch_geo_ok[\"original_index\"] = mismatch_geo_ok.index\n",
    "# alter api to match the format in the occ_wells dataframe\n",
    "mismatch_geo_ok[\"ok_api\"] = mismatch_geo_ok[\"api\"].str[:10]  # ok for Oklahoma\n",
    "# drop the api column\n",
    "mismatch_geo_ok.drop(columns=[\"api\"], inplace=True)\n",
    "# rename the api column in occ_wells to ok_api\n",
    "occ_wells.rename(columns={\"api\": \"ok_api\"}, inplace=True)\n",
    "# look for the api in the occ_wells dataframe and merge that row to mismatch_geo_ok\n",
    "joined_mismatch_ok = mismatch_geo_ok.merge(\n",
    "    occ_wells[\n",
    "        [\"ok_api\", \"well_name\", \"well_num\", \"operator\", \"sh_lat\", \"sh_lon\", \"county\"]\n",
    "    ],\n",
    "    how=\"left\",\n",
    "    on=\"ok_api\",\n",
    ")\n",
    "joined_mismatch_ok[[\"geometry\", \"latitude\", \"longitude\", \"sh_lat\", \"sh_lon\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# replace the lat and lon values with the sh_lat and sh_lon values\n",
    "# Replace the 'lat' and 'lon' values\n",
    "joined_mismatch_ok.loc[\n",
    "    joined_mismatch_ok[\"sh_lat\"].notna(), \"lat\"\n",
    "] = joined_mismatch_ok[\"sh_lat\"]\n",
    "joined_mismatch_ok.loc[\n",
    "    joined_mismatch_ok[\"sh_lon\"].notna(), \"lon\"\n",
    "] = joined_mismatch_ok[\"sh_lon\"]\n",
    "\n",
    "joined_mismatch_ok.set_index(\"original_index\", inplace=True)\n",
    "\n",
    "# occ_wells[occ_wells[\"api\"].isin(mismatch_geo_ok[\"ok_api\"])][\n",
    "#     [\"api\", \"well_name\", \"well_num\", \"operator\", \"sh_lat\", \"sh_lon\", \"county\"]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Update 'lat' and 'lon' in the original DataFrame\n",
    "registry_gdf.loc[joined_mismatch_ok.index, \"lat\"] = joined_mismatch_ok[\"lat\"]\n",
    "registry_gdf.loc[joined_mismatch_ok.index, \"lon\"] = joined_mismatch_ok[\"lon\"]\n",
    "# registry_gdf\n",
    "\n",
    "\n",
    "# Update 'geometry' in the original DataFrame\n",
    "registry_gdf.loc[joined_mismatch_ok.index, \"geometry\"] = [\n",
    "    Point(xy) for xy in zip(joined_mismatch_ok.lon, joined_mismatch_ok.lat)\n",
    "]\n",
    "# Check which other wells may have a state value which did not agree with the spatial join result again\n",
    "registry_county_gdf = registry_gdf.sjoin(\n",
    "    county_fips_gdf, how=\"left\", predicate=\"intersects\"\n",
    ").drop(columns=[\"index_right\"])\n",
    "\n",
    "# mismatch_geo = registry_county_gdf[registry_county_gdf[\"state_right\"].isna()]\n",
    "\n",
    "mismatch_geo = registry_county_gdf[\n",
    "    registry_county_gdf[\"state_left\"] != registry_county_gdf[\"state_right\"]\n",
    "]\n",
    "print(f\"Number of rows with mismatched state values: {mismatch_geo.shape[0]}\")\n",
    "# mismatch_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# query for the wells in Texas\n",
    "mismatch_geo_tx = mismatch_geo[trimmed_column_set].query(\n",
    "    'state_left.str.contains(\"Texas\") | state_right.str.contains(\"Texas\")'\n",
    ")\n",
    "# store the original index in a new column\n",
    "mismatch_geo_tx[\"original_index\"] = mismatch_geo_tx.index\n",
    "\n",
    "mismatch_geo_tx[\"tx_api\"] = mismatch_geo_tx[\"api\"].str[2:10]\n",
    "mismatch_geo_tx.loc[\n",
    "    :,\n",
    "    [\n",
    "        \"api\",\n",
    "        \"well\",\n",
    "        \"state_left\",\n",
    "        \"state_right\",\n",
    "        \"county_left\",\n",
    "        \"county_right\",\n",
    "        \"county_code\",\n",
    "        \"countyfp\",\n",
    "    ],\n",
    "]\n",
    "# len(mismatch_geo_tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texas Railroad Commission (RRC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Land survey data, bottom well data, and surface well data were taken from the RRC website. The data was then uploaded to GCP for easier reliabiliity. They are 254 zipfiles, one for each county, in the state of Texas. Each of those zipfiles contained various file extensions, and spatial data format that is usually contained in shapefiles, and contained info for various categories ranging from Airport lines to Offshore survey polys.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# regex patterns to identify which shapefiles to extract\n",
    "# we are grabbing the survey lines, surface wells, and bottom well locations shp files\n",
    "patterns = [r\"surv\\d{3}p\", r\"well\\d{3}s\", r\"well\\d{3}b\"]\n",
    "\n",
    "# Look at the survey lines polygons and the surface wells points. Data saved from RRC website\n",
    "shp_dict = extract_matching_shp_files_from_zip_urls_concurrent(SHP_ZIP_URLS, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def deep_getsizeof(obj):\n",
    "    \"\"\"Recursively find size of object and its elements\"\"\"\n",
    "    size = sys.getsizeof(obj)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum(deep_getsizeof(k) + deep_getsizeof(v) for k, v in obj.items())\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        size += sum(deep_getsizeof(x) for x in obj)\n",
    "    return size\n",
    "\n",
    "\n",
    "# Assume 'my_dict' is your dictionary\n",
    "total_size_in_bytes = deep_getsizeof(shp_dict)\n",
    "total_size_in_mb = total_size_in_bytes / 1e6\n",
    "print(f\"The total size of the dictionary is {total_size_in_mb:.2f} MB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the patterns to separate the gdf in the dict based on the pattern\n",
    "surv_dict = {k: shp_dict[k] for k, v in shp_dict.items() if re.search(patterns[0], k)}\n",
    "swell_dict = {k: shp_dict[k] for k, v in shp_dict.items() if re.search(patterns[1], k)}\n",
    "bwell_dict = {k: shp_dict[k] for k, v in shp_dict.items() if re.search(patterns[2], k)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texas RRC land survey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O&G well symnum is a number that indicates the type of well simplified for fewer bins\n",
    "well_symnum_dict = {\n",
    "    2: \"Permitted Location\",\n",
    "    3: \"Dry Hole\",\n",
    "    4: \"Oil/Gas\",  # oil\n",
    "    5: \"Oil/Gas\",  # gas\n",
    "    6: \"Oil/Gas\",  # oil/ gas\n",
    "    7: \"Plugged/Shut-in\",  # oil\n",
    "    8: \"Plugged/Shut-in\",  # gas\n",
    "    9: \"Canceled Location\",\n",
    "    10: \"Plugged/Shut-in\",\n",
    "    11: \"Injection/Disposal\",\n",
    "    12: \"Core Test\",\n",
    "    17: \"Storage\",  # oil\n",
    "    18: \"Storage\",  # gas\n",
    "    19: \"Plugged/Shut-in\",  # oil\n",
    "    20: \"Plugged/Shut-in\",  # gas\n",
    "    21: \"Injection/Disposal\",  # oil\n",
    "    22: \"Injection/Disposal\",  # gas\n",
    "    23: \"Injection/Disposal\",  # oil/ gas\n",
    "    73: \"Brine Mining\",\n",
    "    74: \"Water Supply\",\n",
    "    75: \"Water Supply\",  # oil\n",
    "    76: \"Water Supply\",  # gas\n",
    "    77: \"Water Supply\",  # oil/ gas\n",
    "    86: \"Horizontal\",  # Horizontal Well Surface Location\",\n",
    "    87: \"Horizontal\",  # Directional/Sidetrack Well Surface Location,\n",
    "    88: \"Storage\",\n",
    "    103: \"Storage\",  # oil/gas\n",
    "}\n",
    "# well_symnum_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the GeoDataFrames in surv_dict into a single GeoDataFrame\n",
    "surv_data_gdf = concat_gdf_from_dict(surv_dict)\n",
    "\n",
    "# Convert the column names to snake case for consistency\n",
    "surv_data_gdf.columns = [pascal_to_snake(col) for col in surv_data_gdf.columns]\n",
    "# addd a coulmn for the county_code\n",
    "surv_data_gdf[\"county_code\"] = surv_data_gdf[\"source_file\"].str.extract(r\"(\\d{3})\")\n",
    "\n",
    "# Display a sample of 3 rows from the DataFrame\n",
    "display(surv_data_gdf.sample(3))\n",
    "\n",
    "# Display information about the DataFrame, including the number of non-null entries in each column\n",
    "surv_data_gdf.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texas RRC surface well data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the GeoDataFrames in well_dict into a single GeoDataFrame\n",
    "swell_data_gdf = concat_gdf_from_dict(swell_dict)\n",
    "\n",
    "# Convert the column names to snake case for consistency\n",
    "swell_data_gdf.columns = [pascal_to_snake(col) for col in swell_data_gdf.columns]\n",
    "# get the county code from the source_file column\n",
    "swell_data_gdf[\"county_code\"] = swell_data_gdf[\"source_file\"].str.extract(r\"(\\d{3})\")\n",
    "# map the dictionary to the SYMNUM column and fill the rare values with 'Other'\n",
    "swell_data_gdf[\"well_type\"] = (\n",
    "    swell_data_gdf[\"symnum\"].map(well_symnum_dict).fillna(\"Other\")\n",
    ")\n",
    "\n",
    "# Display a sample of 3 rows from the DataFrame\n",
    "display(swell_data_gdf.sample(3))\n",
    "\n",
    "# Display information about the DataFrame, including the number of non-null entries in each column\n",
    "swell_data_gdf.info(\n",
    "    memory_usage=\"deep\"\n",
    ")  # shp_dict = extract_specific_gdf_from_zip_url(shp_zip_urls, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swell_data_gdf[\"well_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the well types on a horizontal bar chart\n",
    "swell_data_gdf[\"well_type\"].value_counts().hvplot.barh(\n",
    "    height=400,\n",
    "    width=600,\n",
    "    title=\"Surface Well Types Count\",\n",
    "    xlabel=\"\",\n",
    "    ylabel=\"\",\n",
    "    xaxis=\"bare\",\n",
    "    hover_cols=\"all\",\n",
    ").opts(\n",
    "    active_tools=[\"box_zoom\"],\n",
    "    toolbar=\"above\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot on a map the dry holes\n",
    "dry_hole_gdf = swell_data_gdf[swell_data_gdf[\"well_type\"] == \"Dry Hole\"]\n",
    "\n",
    "# vertorize the coordinates\n",
    "dry_hole_mer = platecaree_to_mercator_vectorised(\n",
    "    dry_hole_gdf[\"geometry\"].x, dry_hole_gdf[\"geometry\"].y\n",
    ")\n",
    "dry_hole_coords = pd.DataFrame(dry_hole_mer, columns=[\"x\", \"y\"])\n",
    "\n",
    "bg_map = get_background_map()\n",
    "# plot the dry holes on a map\n",
    "bg_map * gv.Points(\n",
    "    dry_hole_coords.reset_index(), [\"x\", \"y\"], [\"index\"], crs=ccrs.GOOGLE_MERCATOR\n",
    ").opts(width=800, height=600, title=\"Dry Hole Locations\", size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to plot the well types on a map\n",
    "def plot_well_types(gdf, well_type):\n",
    "    \"\"\"Plots the well types on a map\"\"\"\n",
    "    # filter the swell_data_gdf for the well_type\n",
    "    well_type_gdf = gdf[gdf[\"well_type\"] == well_type]\n",
    "    # vertorize the coordinates\n",
    "    well_type_mer = platecaree_to_mercator_vectorised(\n",
    "        well_type_gdf[\"geometry\"].x, well_type_gdf[\"geometry\"].y\n",
    "    )\n",
    "    well_type_coords = pd.DataFrame(well_type_mer, columns=[\"x\", \"y\"])\n",
    "    # plot the well types on a map\n",
    "    return get_background_map() * gv.Points(\n",
    "        well_type_coords.reset_index(),\n",
    "        [\"x\", \"y\"],\n",
    "        [\"index\"],\n",
    "        crs=ccrs.GOOGLE_MERCATOR,\n",
    "    ).opts(width=800, height=600, title=f\"{well_type} Locations\", size=1).opts(\n",
    "        active_tools=[\"box_zoom\"],\n",
    "        toolbar=\"above\",\n",
    "        xaxis=\"bare\",\n",
    "        yaxis=\"bare\",\n",
    "        tools=[\"hover\"],\n",
    "    )\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# create a partial function for the plot_well_types function\n",
    "plot_well_types_partial = partial(plot_well_types, swell_data_gdf)\n",
    "\n",
    "plot_well_types_partial(\"Plugged/Shut-in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texas RRC bottom hole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the GeoDataFrames in well_dict into a single GeoDataFrame\n",
    "bwell_data_gdf = concat_gdf_from_dict(bwell_dict)\n",
    "\n",
    "# Convert the column names to snake case for consistency\n",
    "bwell_data_gdf.columns = [pascal_to_snake(col) for col in bwell_data_gdf.columns]\n",
    "# get the county code from the source_file column\n",
    "bwell_data_gdf[\"county_code\"] = bwell_data_gdf[\"source_file\"].str.extract(r\"(\\d{3})\")\n",
    "\n",
    "# map the dictionary to the SYMNUM column and fill the rare values with 'Other'\n",
    "bwell_data_gdf[\"well_type\"] = (\n",
    "    bwell_data_gdf[\"symnum\"].map(well_symnum_dict).fillna(\"Other\")\n",
    ")\n",
    "# Display a sample of 3 rows from the DataFrame\n",
    "display(bwell_data_gdf.sample(3))\n",
    "\n",
    "# Display information about the DataFrame, including the number of non-null entries in each column\n",
    "bwell_data_gdf.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### matching the mismatched wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swell_columns = [\"api\", \"lat83\", \"long83\", \"well_type\"]\n",
    "mismatch_columns = [\"tx_api\", \"lon\", \"lat\", \"original_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch_geo_tx[mismatch_columns].merge(\n",
    "    swell_data_gdf[swell_columns], how=\"left\", left_on=\"tx_api\", right_on=\"api\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_mismatch_tx = mismatch_geo_tx[mismatch_columns].merge(\n",
    "    swell_data_gdf[swell_columns], how=\"left\", left_on=\"tx_api\", right_on=\"api\"\n",
    ")\n",
    "\n",
    "# set the index to the original_index column\n",
    "joined_mismatch_tx.set_index(\"original_index\", inplace=True)\n",
    "\n",
    "# replace the lat and lon values with the lat83 and lon83 values if not nan\n",
    "joined_mismatch_tx.loc[joined_mismatch_tx[\"lat83\"].notna(), \"lat\"] = joined_mismatch_tx[\n",
    "    \"lat83\"\n",
    "]\n",
    "joined_mismatch_tx.loc[\n",
    "    joined_mismatch_tx[\"long83\"].notna(), \"lon\"\n",
    "] = joined_mismatch_tx[\"long83\"]\n",
    "\n",
    "# Update 'lat' and 'lon' in the original DataFrame\n",
    "\n",
    "\n",
    "registry_gdf.loc[joined_mismatch_tx.index, \"lat\"] = joined_mismatch_tx[\"lat\"]\n",
    "registry_gdf.loc[joined_mismatch_tx.index, \"lon\"] = joined_mismatch_tx[\"lon\"]\n",
    "\n",
    "\n",
    "# Update 'geometry' in the original DataFrame\n",
    "registry_gdf.loc[joined_mismatch_tx.index, \"geometry\"] = [\n",
    "    Point(xy) for xy in zip(joined_mismatch_tx.lon, joined_mismatch_tx.lat)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which other wells may have a state value which did not agree with the spatial join result again\n",
    "registry_county_gdf = registry_gdf.sjoin(\n",
    "    county_fips_gdf, how=\"left\", predicate=\"intersects\"\n",
    ").drop(columns=[\"index_right\"])\n",
    "mismatch_geo = registry_county_gdf[registry_county_gdf[\"state_right\"].isna()]\n",
    "mismatch_geo.shape\n",
    "print(f\"Number of rows with mismatched state values: {mismatch_geo.shape[0]}\")\n",
    "if mismatch_geo.shape[0] > 0:\n",
    "    display(mismatch_geo.sample(3))\n",
    "else:\n",
    "    print(\"No rows with mismatched state values\")\n",
    "# display(mismatch_geo.sample(3))\n",
    "\n",
    "# drop the rows with null values in the state_right column of mismatch_geo from the registry_gdf\n",
    "print(f\"Number of rows before dropping: {registry_gdf.shape[0]}\")\n",
    "registry_gdf.drop(mismatch_geo.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rows after dropping: {registry_gdf.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_gdf.sjoin(county_fips_gdf, how=\"left\", predicate=\"intersects\").drop(\n",
    "    columns=[\"index_right\"]\n",
    ")[trimmed_column_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the row where the county_left and county_right columns do not match\n",
    "trimmed_joined_gdf = registry_gdf.sjoin(\n",
    "    county_fips_gdf, how=\"left\", predicate=\"intersects\"\n",
    ").drop(columns=[\"index_right\"])[trimmed_column_set]\n",
    "state_mismatch_mask = (\n",
    "    trimmed_joined_gdf[\"state_left\"] != trimmed_joined_gdf[\"state_right\"]\n",
    ")\n",
    "state_mismatch_gdf = trimmed_joined_gdf[state_mismatch_mask]\n",
    "print(f\"Number of rows with mismatched county values: {state_mismatch_gdf.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_mismatch_gdf.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the columns to keep\n",
    "columns_we_want = [\n",
    "    \"api\",\n",
    "    \"well\",\n",
    "    \"state_code\",\n",
    "    \"state_left\",\n",
    "    \"state_right\",\n",
    "    \"county_code\",\n",
    "    \"county_left\",\n",
    "    \"countyfp\",\n",
    "    \"name\",\n",
    "    \"operator\",\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"geoid\",\n",
    "    \"geometry\",\n",
    "]\n",
    "registry_county_gdf = registry_gdf.sjoin(\n",
    "    county_fips_gdf, how=\"left\", predicate=\"intersects\"\n",
    ").drop(columns=[\"index_right\"])[columns_we_want]\n",
    "\n",
    "print(f\"Number of rows: {registry_county_gdf.shape[0]}\")\n",
    "registry_county_gdf[\n",
    "    registry_county_gdf[\"state_left\"] != registry_county_gdf[\"state_right\"]\n",
    "][columns_we_want].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the rows where the state_left is California with the state_right being Texas\n",
    "# This would represent where the geometry said Texas but the api or original dataset had California\n",
    "# we check the api with\n",
    "cali_tx_query = registry_county_gdf.query(\n",
    "    'state_left == \"California\" & state_right == \"Texas\"'\n",
    ")\n",
    "# from the query, take the last 9 of the api (well_id) with 42 at beginning\n",
    "# with county_code from county that was matched with the geometry\n",
    "cali_tx_query[\"state_code\"] = \"42\"\n",
    "cali_tx_query[\"county_code\"] = cali_tx_query[\"countyfp\"]\n",
    "cali_tx_query[\"well_id\"] = cali_tx_query[\"api\"].str[-9:]\n",
    "cali_tx_query[\"api\"] = (\n",
    "    cali_tx_query[\"state_code\"]\n",
    "    + cali_tx_query[\"county_code\"]\n",
    "    + cali_tx_query[\"well_id\"]\n",
    ")\n",
    "\n",
    "# put api, county_code and state_code into the registry_gdf\n",
    "registry_gdf.loc[\n",
    "    cali_tx_query.index, [\"api\", \"county_code\", \"state_code\"]\n",
    "] = cali_tx_query[[\"api\", \"county_code\", \"state_code\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_county_gdf[\n",
    "    registry_county_gdf[columns_we_want][\"state_left\"]\n",
    "    != registry_county_gdf[columns_we_want][\"state_right\"]\n",
    "][columns_we_want]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swell_data_gdf[\"well_type\"].value_counts()\n",
    "# well_data_gdf[well_data_gdf[\"well_type\"] == \"Other\"][\"symnum\"].value_counts()\n",
    "# look for the mismatch_geo_tx api in the swell_data_gdf and get those rrows\n",
    "\n",
    "mismatch_geo_tx_sw = swell_data_gdf[\n",
    "    swell_data_gdf[\"api\"].isin(mismatch_geo_tx[\"tx_api\"])\n",
    "].copy()\n",
    "mismatch_geo_tx_sw\n",
    "# convert the long83 and lat83 columns to geomety Points\n",
    "mismatch_geo_tx_sw[\"geometry\"] = [\n",
    "    Point(xy) for xy in zip(mismatch_geo_tx_sw.long83, mismatch_geo_tx_sw.lat83)\n",
    "]\n",
    "# create a GeoDataFrame from the mismatch_geo_tx_sw dataframe\n",
    "mismatch_geo_tx_sw = gpd.GeoDataFrame(\n",
    "    mismatch_geo_tx_sw, geometry=\"geometry\", crs=\"EPSG:4269\"\n",
    ")\n",
    "# plot the mismatch_geo_tx_sw GeoDataFrame\n",
    "# bg_map * gv.Points(mismatch_geo_tx_sw).opts(height=500, width=800, tools=[\"hover\"])\n",
    "\n",
    "mismatch_geo_tx_sw.sjoin(county_fips_gdf, how=\"left\", predicate=\"intersects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the length of the tx_api values\n",
    "swell_data_gdf[\"api\"].astype(\"string\").str.len().value_counts()\n",
    "\n",
    "# drop the rows with the api number length of 3\n",
    "swell_data_gdf.drop(\n",
    "    swell_data_gdf[swell_data_gdf[\"api\"].astype(\"string\").str.len() == 3].index,\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_map = get_background_map()\n",
    "\n",
    "ok_counties = county_fips_gdf.query('state.str.contains(\"Oklahoma\")').copy()\n",
    "ok_counties.to_crs(\"EPSG:3857\", inplace=True)\n",
    "\n",
    "tx_counties = county_fips_gdf.query('state.str.contains(\"Texas\")').copy()\n",
    "tx_counties.to_crs(\"EPSG:3857\", inplace=True)\n",
    "\n",
    "ok_polys = gv.Polygons(ok_counties, crs=ccrs.GOOGLE_MERCATOR).opts(\n",
    "    projection=ccrs.GOOGLE_MERCATOR,\n",
    "    line_color=\"black\",\n",
    "    fill_alpha=0,\n",
    "    height=500,\n",
    "    width=500,\n",
    ")\n",
    "bg_map * ok_polys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of states that are in the Permian Basin.\n",
    "nm_tx = [\"New Mexico\", \"Texas\"]\n",
    "\n",
    "# Filter the county_fips_gdf DataFrame to include only the counties in the Permian states.\n",
    "counties_nm_tx_gdf = county_fips_gdf[county_fips_gdf[\"state\"].isin(nm_tx)]\n",
    "\n",
    "# Perform a spatial join between the registry_gdf and counties_nm_tx_gdf DataFrames.\n",
    "# This will add the data from counties_nm_tx_gdf to registry_gdf for matching locations.\n",
    "# After the join, drop the 'index_right' column as it's not needed.\n",
    "registry_nm_tx_gdf = registry_gdf.sjoin(counties_nm_tx_gdf).drop(\n",
    "    columns=[\"index_right\"]\n",
    ")\n",
    "registry_nm_tx_gdf.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_nm_tx_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_nm_tx_gdf[\"operator_name\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pivot table with the count of operators for each year\n",
    "operator_year_count = registry_nm_tx_gdf.pivot_table(\n",
    "    index=\"operator_name\", columns=\"job_start_year\", values=\"api\", aggfunc=\"count\"\n",
    ")\n",
    "# See which operators were active every year\n",
    "# operator_year_count[operator_year_count.count(axis=1) == 11]\n",
    "\n",
    "# See who has been active for the last 5 years\n",
    "operator_active_5y = operator_year_count.loc[\n",
    "    ~operator_year_count.iloc[:, -5:].isna().any(axis=1)\n",
    "].fillna(0)\n",
    "\n",
    "# do some styler table formatting from pandas\n",
    "style = operator_active_5y.style.background_gradient(\n",
    "    cmap=\"cet_CET_L2_r\", axis=1, vmin=0, vmax=operator_year_count.max().max()\n",
    ")\n",
    "# Format the numbers in the table as integers\n",
    "style = style.format(\"{:.0f}\")\n",
    "print(f\"Number of operators active for the last 5 years: {len(operator_active_5y)}\")\n",
    "style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask for rows where the first 2 characters of the api number are not 42 nor 30.\n",
    "# This is done to filter out rows that do not belong to the states we are interested in (Texas and New Mexico).\n",
    "api_mask = ~registry_nm_tx_gdf[\"api\"].str[0:2].isin([\"42\", \"30\"])\n",
    "\n",
    "# Apply the mask to the registry_nm_tx_gdf DataFrame to get the rows that match the condition.\n",
    "mismatch_state = registry_nm_tx_gdf[api_mask]\n",
    "\n",
    "# Display selected columns from the mismatch_state DataFrame.\n",
    "# These columns provide information about the well, its location, and the job start date.\n",
    "display(\n",
    "    mismatch_state[\n",
    "        [\n",
    "            \"api\",\n",
    "            \"api_number\",\n",
    "            \"state_right\",\n",
    "            \"state_name\",\n",
    "            \"state_number\",\n",
    "            \"well_name\",\n",
    "            \"operator_name\",\n",
    "            \"county_name\",\n",
    "            \"latitude\",\n",
    "            \"longitude\",\n",
    "            \"geometry\",\n",
    "            \"crs\",\n",
    "            \"job_start_date\",\n",
    "            # \"county\",\n",
    "            \"countyfp\",\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# get a background map\n",
    "bg_map = get_background_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the coordinates to Mercator\n",
    "mercator_coords = platecaree_to_mercator_vectorised(\n",
    "    registry_nm_tx_gdf[\"geometry\"].x, registry_nm_tx_gdf[\"geometry\"].y\n",
    ")\n",
    "\n",
    "# Round the coordinates and create a DataFrame\n",
    "mer_points = pd.DataFrame(np.round(mercator_coords), columns=[\"x\", \"y\"])\n",
    "\n",
    "# Create a Points object for plotting\n",
    "gpoints = gv.Points(\n",
    "    mer_points.reset_index(), [\"x\", \"y\"], [\"index\"], crs=ccrs.GOOGLE_MERCATOR\n",
    ").opts(height=600, width=800, color=\"skyblue\", size=1, tools=[\"hover\"])\n",
    "\n",
    "# Create a layout with the background map and the points\n",
    "layout = bg_map * gpoints\n",
    "layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map files taken from the EIA website.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function 'extract_gdfs_from_zip_url_concurrent' to get GeoDataFrames from the URLs in 'basins_url_list'\n",
    "# This function concurrently downloads and extracts GeoDataFrames from the given URLs\n",
    "basins_dict = extract_gdfs_from_zip_url_concurrent(basins_url_list)\n",
    "\n",
    "# Display the keys of 'basins_dict' to see the names of the basins\n",
    "display(basins_dict.keys())\n",
    "\n",
    "# Concatenate the GeoDataFrames in 'basins_dict' into a single GeoDataFrame using the function 'concat_gdf_from_dict'\n",
    "basins_gdf = concat_gdf_from_dict(basins_dict)\n",
    "\n",
    "# Convert the column names of 'basins_gdf' to snake case for consistency\n",
    "# The function 'pascal_to_snake' is used to convert PascalCase or camelCase to snake_case\n",
    "basins_gdf.columns = [pascal_to_snake(col) for col in basins_gdf.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of sub basins/ geodataframes: {len(basins_dict)}\\n\")\n",
    "\n",
    "for k, gdf in basins_dict.items():\n",
    "    print(f\"{k}| Shape:{gdf.shape}| CRS:{gdf.crs.to_string()}\")\n",
    "    display(gdf.sample())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the shapefile of the basin boundaries\n",
    "basins_dict = extract_gdfs_from_zip_url_concurrent(basins_url_list)\n",
    "display(basins_dict.keys())\n",
    "basins_gdf = concat_gdf_from_dict(basins_dict)\n",
    "# scrub the column names\n",
    "basins_gdf.columns = [pascal_to_snake(col) for col in basins_gdf.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot shale plays and basin boundaries of the different formations in the Permian Basin\n",
    "bg_map * basins_gdf.hvplot(\n",
    "    geo=True,\n",
    "    alpha=0.5,\n",
    "    title=\"Shale Plays in the Permian Basin\",\n",
    "    legend=True,\n",
    "    by=\"shale_play\",\n",
    "    muted_alpha=0.01,\n",
    ").opts(\n",
    "    tools=[\"hover\", \"tap\"],\n",
    "    legend_position=\"right\",\n",
    "    height=600,\n",
    "    width=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holoviews import opts\n",
    "\n",
    "# Dissolve the geometries of the basins_gdf GeoDataFrame into a single geometry\n",
    "\n",
    "\n",
    "shale_plays_gdf = basins_gdf[[\"geometry\"]].dissolve()\n",
    "# get the counties in the Permian Basin\n",
    "permian_counties = county_fips_gdf.intersects(shale_plays_gdf)\n",
    "# plot the counties in the Permian Basin\n",
    "bg_map * gv.Polygons(county_fips_gdf[permian_counties]).opts(\n",
    "    title=\"Counties in the Permian Basin\",\n",
    "    tools=[\"hover\"],\n",
    "    height=600,\n",
    "    width=800,\n",
    "    alpha=0.5,\n",
    "    color=\"skyblue\",\n",
    ")\n",
    "# plot an outline of the Permian Basin over the counties\n",
    "overlay = (\n",
    "    bg_map\n",
    "    * gv.Polygons(county_fips_gdf[permian_counties])\n",
    "    * gv.Path(shale_plays_gdf)\n",
    "    # * gpoints\n",
    ")\n",
    "\n",
    "overlay.opts(\n",
    "    opts.Polygons(alpha=0.5, cmap=[\"#73d2ff\"], line_color=\"gray\"),\n",
    "    opts.Path(alpha=0.5, color=\"black\"),\n",
    "    opts.Overlay(tools=[\"hover\"], height=600, width=800),\n",
    "    opts.Points(color=\"crimson\"),\n",
    ")\n",
    "\n",
    "\n",
    "# plot to confirm that the geometries have been dissolved\n",
    "# bg_map * gv.Polygons(shale_plays_gdf).opts(\n",
    "#     # geo=True,\n",
    "#     title=\"Dissolved Shale play in the Permian Basin\",\n",
    "#     height=600,\n",
    "#     width=800,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State land leases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Mexico:\n",
    "> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the extract_gdfs_from_zip_url_concurrent function to download and extract GeoDataFrames\n",
    "# from the shapefile zip files at the URLs in the shp_url_list. The function returns a dictionary\n",
    "# where the keys are the names of the shapefiles and the values are the corresponding GeoDataFrames.\n",
    "nm_slo_dict = extract_gdfs_from_zip_url_concurrent(nm_slo_url_list)\n",
    "\n",
    "# Display the keys of the land_map_dict dictionary. These are the names of the shapefiles\n",
    "# that were downloaded and extracted.\n",
    "nm_slo_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# land_map_dict = extract_gdfs_from_zip_url_concurrent(shp_url_list)\n",
    "# land_map_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the gdfs in the dictionary\n",
    "for k, gdf in nm_slo_dict.items():\n",
    "    print(f\"{k}| Shape:{gdf.shape}| CRS:{gdf.crs.to_string()}\")\n",
    "    display(gdf.sample(3))\n",
    "    display(gdf.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 separate gdfs instead of concatenating them as they have distinct columns\n",
    "nm_slo_gdfs = list(nm_slo_dict.values())\n",
    "# first one is the geologic regions\n",
    "nm_slo_geo = nm_slo_gdfs[0]\n",
    "# scrub the columns\n",
    "nm_slo_geo.columns = [pascal_to_snake(col) for col in nm_slo_geo.columns]\n",
    "\n",
    "# Define  a dictionary for the opts to include in plot function\n",
    "poly_opts = dict(\n",
    "    alpha=0.8,\n",
    "    height=600,\n",
    "    width=800,\n",
    "    line_width=0,\n",
    "    line_color=\"lightgray\",\n",
    "    tools=[\"hover\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Adjust opts for this plot\n",
    "poly_opts_copy = poly_opts.copy()\n",
    "poly_opts_copy[\"line_width\"] = 1\n",
    "\n",
    "# plot the geologic regions gdf\n",
    "bg_map * gv.Polygons(nm_slo_geo.to_crs(\"EPSG:4269\"), vdims=[\"label\"]).opts(\n",
    "    **poly_opts_copy, cmap=[\"#73d2ff\"] * 256, title=\"New Mexico Geologic Regions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second one is the oil and gas leases on New Mexico State Trust Lands\n",
    "nm_slo_lease = nm_slo_gdfs[1]\n",
    "# scrub the columns\n",
    "nm_slo_lease.columns = [pascal_to_snake(col) for col in nm_slo_lease.columns]\n",
    "# create a new column for the area of the lease\n",
    "nm_slo_lease[\"area\"] = nm_slo_lease[\"geometry\"].area\n",
    "# groupby the ogrid_nam and sum the area\n",
    "# add the transformed area to the gdf\n",
    "nm_slo_lease[\"ogrid_area\"] = (\n",
    "    nm_slo_lease.groupby(\"ogrid_nam\")[\"area\"].transform(\"sum\") / 1e6\n",
    ")\n",
    "\n",
    "# plot the oil and gas leases gdf for New Mexico State Trust Lands\n",
    "bg_map * gv.Polygons(\n",
    "    nm_slo_lease.to_crs(\"EPSG:4269\"), vdims=[\"ogrid_nam\", \"ogrid_area\"]\n",
    ").opts(\n",
    "    **poly_opts,\n",
    "    cnorm=\"eq_hist\",\n",
    "    colorbar=True,\n",
    "    title=\"Oil and Gas Leases on New Mexico State Trust Lands\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(land_map_dict.values())\n",
    "# [gdf['geometry'] for gdf in land_map_dict.values()]\n",
    "random_color_list = [\n",
    "    \"#\" + \"\".join([random.choice(\"0123456789ABCDEF\") for j in range(6)])\n",
    "    for i in range(len(nm_slo_dict))\n",
    "]\n",
    "plots = []\n",
    "new_map = get_background_map()\n",
    "plots.append(new_map)\n",
    "for color, (name, gdf) in zip(random_color_list, nm_slo_dict.items()):\n",
    "    # Add new column with the name of the shapefile for the hover tool\n",
    "    gdf[\"label\"] = name\n",
    "    plot = gv.Polygons(gdf.to_crs(\"EPSG:4269\"), vdims=[\"label\"]).opts(\n",
    "        tools=[\"hover\"], height=600, width=800, alpha=0.5, title=\"\"\n",
    "    )\n",
    "    plots.append(plot)\n",
    "\n",
    "overlay = hv.Overlay(plots)\n",
    "overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm_slo_lease.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the GeoDataFrames in well_dict into a single GeoDataFrame\n",
    "swell_data_gdf = concat_gdf_from_dict(swell_dict)\n",
    "\n",
    "# Convert the column names to snake case for consistency\n",
    "swell_data_gdf.columns = [pascal_to_snake(col) for col in swell_data_gdf.columns]\n",
    "# get the county code from the source_file column\n",
    "swell_data_gdf[\"county_code\"] = swell_data_gdf[\"source_file\"].str.extract(r\"(\\d{3})\")\n",
    "\n",
    "# Display a sample of 3 rows from the DataFrame\n",
    "display(swell_data_gdf.sample(3))\n",
    "\n",
    "# Display information about the DataFrame, including the number of non-null entries in each column\n",
    "swell_data_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column to the surv_data_gdf with the county number\n",
    "# the county_number wil be the numbers in the source_file column\n",
    "surv_data_gdf[\"county_number\"] = surv_data_gdf[\"source_file\"].str.extract(r\"(\\d{3})\")\n",
    "\n",
    "# using just the geometry and the county_number columns, intersect with the permian basin gdf\n",
    "surv_permian_gdf = surv_data_gdf[[\"geometry\", \"county_number\"]].sjoin(\n",
    "    shale_plays_gdf[[\"geometry\"]], how=\"inner\", predicate=\"intersects\"\n",
    ")\n",
    "# see which counties are in the permian basin\n",
    "pb_county_numbers = surv_permian_gdf[\"county_number\"].unique().tolist()\n",
    "\n",
    "# plot the survey lines in the permian basin\n",
    "bg_map * gv.Polygons(\n",
    "    surv_permian_gdf.to_crs(\"EPSG:4269\"), vdims=[\"county_number\"]\n",
    ").opts(\n",
    "    tools=[\"hover\"],\n",
    "    height=600,\n",
    "    width=800,\n",
    "    alpha=0.5,\n",
    "    line_width=0,\n",
    "    title=\"Permian Basin Survey Lines\",\n",
    ")\n",
    "\n",
    "# surv_data_gdf.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how land survey polygon data looks on map\n",
    "pb_plot = shale_plays_gdf.hvplot(geo=True, color=\"red\", alpha=0.5, line_width=0).opts(\n",
    "    height=600, width=800\n",
    ")\n",
    "survey_plot = surv_data_gdf.hvplot(geo=True, color=\"blue\", alpha=0.5, line_width=0)\n",
    "\n",
    "# bg_map * survey_plot * pb_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the intersection of the survey polygons and the Permian Basin polygon\n",
    "survey_pb_gdf = gpd.overlay(surv_data_gdf, shale_plays_gdf, how=\"intersection\")\n",
    "survey_pb_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# survey_pb_gdf.explore()\n",
    "# surv_data_gdf.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join of the registry_gdf(from fracfocus) and surv_data_gdf\n",
    "registry_join_gdf = gpd.sjoin(\n",
    "    registry_gdf[\n",
    "        [\n",
    "            \"geometry\",\n",
    "            \"api\",\n",
    "            \"operator_name\",\n",
    "            \"well_name\",\n",
    "            \"state\",\n",
    "            \"county_name\",\n",
    "            \"county_number\",\n",
    "        ]\n",
    "    ],\n",
    "    surv_data_gdf,\n",
    ").drop(columns=[\"index_right\"])\n",
    "\n",
    "\n",
    "registry_join_gdf.sort_values(by=\"api\")\n",
    "\n",
    "registry_join_gdf.county_name.value_counts()\n",
    "\n",
    "# create a well_id column from the api column\n",
    "\n",
    "registry_join_gdf[\"tx_api\"] = registry_join_gdf[\"api\"].str[2:10]\n",
    "\n",
    "\n",
    "# merge the welltype column from well_data_gdf to registry_join_gdf on the api_short column\n",
    "swell_data_gdf[\"tx_api\"] = swell_data_gdf[\"api\"].copy()\n",
    "\n",
    "registry_join_gdf = (\n",
    "    registry_join_gdf.merge(swell_data_gdf[[\"tx_api\", \"well_type\"]], on=\"tx_api\")\n",
    "    .drop(columns=[\"scrap_file\", \"level4_sur\"])\n",
    "    .rename(columns={\"level2_blo\": \"block\"})\n",
    ")\n",
    "# registry_join_gdf.explore()\n",
    "# plot polygons using geoviews\n",
    "# bg_map * gv.Polygons(registry_join_gdf.to_crs(\"EPSG:4269\"), vdims=[\"well_type\"]).opts(\n",
    "#     **poly_opts, color=\"well_type\", title=\"Well Types in the Permian Basin\"\n",
    "# )\n",
    "\n",
    "bg_map * registry_join_gdf.hvplot(\n",
    "    geo=True,\n",
    "    by=\"well_type\",\n",
    "    alpha=0.8,\n",
    "    legend=\"right\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    size=1,\n",
    "    muted_alpha=0.01,\n",
    "    tools=[\"box_select\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geodatabase files taken from the Texas GLO (General Land Office.)\n",
    "\n",
    "These files contained both the Oil and Gas Leases (active only), managed by the Texas GLO, and Oil & Gas units (active only) which is Oil and Gas pooling agreements managed by the Texas GLO. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the geodataframe of the active leases\n",
    "# active_gdb_dict = read_gdb_from_zip_url(gdb_zip_urls)\n",
    "\n",
    "# get the geodataframe of the active leases using concurrent futures\n",
    "active_gdb_dict = read_gdb_from_zip_url_concurrent(GDB_ZIP_URLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_gdb_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, gdf in active_gdb_dict.items():\n",
    "    print(f\"{k}| Shape:{gdf.shape}| CRS:{gdf.crs.to_string()}\")\n",
    "    display(gdf.sample(3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the active lease geodatabase\n",
    "active_leases_gdf = active_gdb_dict[\"OAG_Leases_Active\"]\n",
    "# clean column names\n",
    "active_leases_gdf.columns = [pascal_to_snake(col) for col in active_leases_gdf.columns]\n",
    "active_leases_gdf.describe(include=\"all\").T.sort_values(\n",
    "    by=\"unique\", ascending=False\n",
    ").fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the columns with the date in it using regex\n",
    "date_cols = [col for col in active_leases_gdf.columns if re.search(r\"date\", col)]\n",
    "# add any other columns that should be dates\n",
    "date_cols.extend([\"lease_input\"])\n",
    "\n",
    "date_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the date columns to datetime\n",
    "active_leases_gdf[date_cols] = pd.concat(\n",
    "    [pd.to_datetime(active_leases_gdf[col]) for col in date_cols], axis=1\n",
    ")\n",
    "# active_leases_gdf[date_cols] = active_leases_gdf[date_cols].fillna(\n",
    "#     pd.Timestamp(\"1900-06-28\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the columns interested in seeing\n",
    "columns_of_interest = date_cols + [\n",
    "    \"county\",\n",
    "    \"geometry\",\n",
    "    \"land_type\",\n",
    "    \"primary_term_year\",\n",
    "    \"original_lessee\",\n",
    "    \"lessor\",\n",
    "    \"field_name\",\n",
    "    \"lease_type\",\n",
    "    \"lease_status\",\n",
    "    \"lease_number\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_leases_gdf[columns_of_interest].info()\n",
    "active_leases_gdf[active_leases_gdf[columns_of_interest].isna().any(axis=1)][\n",
    "    columns_of_interest\n",
    "].sort_values(by=\"effective_date\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_leases_gdf_trimmed = active_leases_gdf[columns_of_interest]\n",
    "\n",
    "active_leases_gdf_trimmed[\"lease_type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_date_cols = list(set(columns_of_interest) - set(date_cols))\n",
    "pd.concat(\n",
    "    [\n",
    "        active_leases_gdf[non_date_cols],\n",
    "        active_leases_gdf[date_cols].astype(\n",
    "            str\n",
    "        ),  # the .explore() does not work with NaT in datetime columns\n",
    "    ],\n",
    "    axis=1,\n",
    ").explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the active units geodatabase\n",
    "active_units_gdf = active_gdb_dict[\"OAG_Units_Active\"]\n",
    "# clean column names\n",
    "active_units_gdf.columns = [pascal_to_snake(col) for col in active_units_gdf.columns]\n",
    "active_units_gdf.describe(include=\"all\").T.sort_values(\n",
    "    by=\"unique\", ascending=False\n",
    ").fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_units_gdf.field_name.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_units_gdf.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Production Data Query Dump from RRC\n",
    "\n",
    "| Table | Description|\n",
    "|---|---|\n",
    "|GP_COUNTY | General purpose table that stores county information.|\n",
    "|GP_DATE_RANGE_CYCLE | General purpose table of PDQ data range ( Jan. 1993-current production report month/year). |\n",
    "|GP_DISTRICT | General purpose table that contains district information. |\n",
    "|OG_COUNTY_CYCLE | Contains production report data reported by lease and month (YYYYMM) aggregated by the county in which the wells are located.  |This is an estimate only based on allowables and potentials.\n",
    "|OG_COUNTY_LEASE_CYCLE | Contains production report data reported by lease and month (YYYYMM) aggregated by lease and county in which the wells  |are located. This is an estimate only based on allowables and potentials.\n",
    "|OG_DISTRICT_CYCLE | Contains production report data reported by lease and month (YYYYMM) aggregated by the completion district for the lease ID. |\n",
    "|OG_FIELD_CYCLE | Contains production report data reported by lease and month (YYYYMM) aggregated by the field in which the well(s) for the lease  |are completed.\n",
    "|OG_FIELD_DW | Table of field identifying data. |\n",
    "|OG_LEASE_CYCLE | Contains production report data reported by lease and month (YYYYMM). |\n",
    "|OG_LEASE_CYCLE_DISP | Contains production report disposition data reported by lease and month (YYYYMM). |\n",
    "|OG_OPERATOR_CYCLE | Contains production report data reported by lease and month (YYYYMM) aggregated by the operator of the lease. |\n",
    "|OG_OPERATOR_DW | This table contains identifying operator information. |\n",
    "|OG_REGULATORY_LEASE_DW | This table contains identifying lease information. |\n",
    "|OG_SUMMARY_MASTER_LARGE | Summary table. (Used for query purposes at the operator level) |\n",
    "|OG_SUMMARY_ONSHORE_LEASE | Summary table. (Used for query purposes on the leases in onshore counties) |\n",
    "|OG_WELL_COMPLETION | This table contains identifying well completion information. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data i from GCS by downloading the zip file to disk and then unipping it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The ID of your GCS object\n",
    "    # source_blob_name = \"storage-object-name\"\n",
    "\n",
    "    # The path to which the file should be downloaded\n",
    "    # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Construct a client side representation of a blob.\n",
    "    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
    "    # any content from Google Cloud Storage. As we don't need additional data,\n",
    "    # using `Bucket.blob` is preferred here.\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print(\n",
    "        \"Downloaded storage object {} from bucket {} to local file {}.\".format(\n",
    "            source_blob_name, bucket_name, destination_file_name\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"mrprime_dataset\"\n",
    "blob_name = \"capstone_journey/rrc/PDQ_DSV.zip\"\n",
    "dl_file = Path(\"../data/PDQ_DSV.zip\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if not dl_file.parent.exists():\n",
    "    # If the directory does not exist, create it\n",
    "    dl_file.parent.mkdir(parents=True)\n",
    "\n",
    "download_blob(bucket_name, blob_name, str(dl_file))\n",
    "\n",
    "# Create a new directory with the same name as the stem of the zip file\n",
    "extract_dir = dl_file.parent / dl_file.stem\n",
    "extract_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Extract the zip file\n",
    "with ZipFile(dl_file, \"r\") as zip_ref:\n",
    "    for name in zip_ref.namelist():\n",
    "        zip_ref.extract(name, extract_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data as parquet files from GCS. We were able to bring in the data without loading it on to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# Create a client\n",
    "client = storage.Client()\n",
    "\n",
    "# Define the bucket name and the prefix\n",
    "bucket_name = \"mrprime_dataset\"\n",
    "prefix = \"capstone_journey/rrc/processed/\"\n",
    "\n",
    "# Get the bucket\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "# Get the blobs (files) in the bucket that match the prefix\n",
    "blobs = bucket.list_blobs(prefix=prefix)\n",
    "\n",
    "# Get the URLs of the data tables\n",
    "data_tables = set(\n",
    "    blob.name.rsplit(\"/\", 2)[1] for blob in blobs if \"_DATA_TABLE.parquet\" in blob.name\n",
    ")\n",
    "\n",
    "# Load each data table into a separate DataFrame\n",
    "dfs = {\n",
    "    data_table: dd.read_parquet(\n",
    "        f\"gs://{bucket_name}/{prefix}{data_table}/*.parquet\", engine=\"pyarrow\"\n",
    "    )\n",
    "    for data_table in tqdm(data_tables)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortent the keys for conciseness\n",
    "dfs = {k.split(\".\")[0]: v for k, v in dfs.items()}\n",
    "data_table_list = list(dfs.keys())\n",
    "display(data_table_list)\n",
    "dfs[data_table_list[0]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables = [\n",
    "#     \"GP_COUNTY_DATA_TABLE\",\n",
    "#     \"GP_DISTRICT_DATA_TABLE\",\n",
    "#     \"GP_DATE_RANGE_CYCLE_DATA_TABLE\",\n",
    "#     \"OG_COUNTY_CYCLE_DATA_TABLE\",\n",
    "#     \"OG_COUNTY_LEASE_CYCLE_DATA_TABLE\",\n",
    "#     \"OG_LEASE_CYCLE_DATA_TABLE\",\n",
    "#     \"OG_LEASE_CYCLE_DISP_DATA_TABLE\",\n",
    "#     \"OG_DISTRICT_CYCLE_DATA_TABLE\",\n",
    "#     \"OG_FIELD_CYCLE_DATA_TABLE\",\n",
    "#     \"OG_OPERATOR_CYCLE_DATA_TABLE\",\n",
    "#     \"OG_FIELD_DW_DATA_TABLE\",\n",
    "#     \"OG_OPERATOR_DW_DATA_TABLE\",\n",
    "#     \"OG_REGULATORY_LEASE_DW_DATA_TABLE\",\n",
    "#     \"OG_WELL_COMPLETION_DATA_TABLE\",\n",
    "#     \"OG_SUMMARY_MASTER_LARGE_DATA_TABLE\",\n",
    "#     \"OG_SUMMARY_ONSHORE_LEASE_DATA_TABLE\",\n",
    "# ]\n",
    "\n",
    "\n",
    "short_table = [item.replace(\"_DATA_TABLE\", \"\").lower() for item in data_table_list]\n",
    "\n",
    "\n",
    "table_zip = zip(short_table, data_table_list)\n",
    "\n",
    "\n",
    "# Make the short table names the keys with dfs dict values\n",
    "data_tables_dict = {k: dfs[v] for k, v in table_zip}\n",
    "list(data_tables_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# general purpose table paths\n",
    "gp_county_path = Path(\"../data/PDQ_DSV/GP_COUNTY_DATA_TABLE.dsv\")\n",
    "gp_district_path = Path(\"../data/PDQ_DSV/GP_DISTRICT_DATA_TABLE.dsv\")\n",
    "gp_date_range_path = Path(\"../data/PDQ_DSV/GP_DATE_RANGE_CYCLE_DATA_TABLE.dsv\")\n",
    "\n",
    "\n",
    "# production data estimates based potentials and allowables paths\n",
    "og_county_cycle_path = Path(\"../data/PDQ_DSV/OG_COUNTY_CYCLE_DATA_TABLE.dsv\")\n",
    "og_county_lease_cycle_path = Path(\n",
    "    \"../data/PDQ_DSV/OG_COUNTY_LEASE_CYCLE_DATA_TABLE.dsv\"\n",
    ")\n",
    "\n",
    "# production disposition data by lease and month paths\n",
    "og_lease_cycle_disp_path = Path(\"../data/PDQ_DSV/OG_LEASE_CYCLE_DISP_DATA_TABLE.dsv\")\n",
    "\n",
    "\n",
    "# production report data by lease paths\n",
    "og_lease_cycle_path = Path(\"../data/PDQ_DSV/OG_LEASE_CYCLE_DATA_TABLE.dsv\")\n",
    "\n",
    "# production data by lease aggregated by another feature\n",
    "og_district_cycle_path = Path(\"../data/PDQ_DSV/OG_DISTRICT_CYCLE_DATA_TABLE.dsv\")\n",
    "og_field_cycle_path = Path(\"../data/PDQ_DSV/OG_FIELD_CYCLE_DATA_TABLE.dsv\")\n",
    "og_operator_cycle_path = Path(\"../data/PDQ_DSV/OG_OPERATOR_CYCLE_DATA_TABLE.dsv\")\n",
    "\n",
    "\n",
    "# identifying tables paths\n",
    "og_field_dw_path = Path(\"../data/PDQ_DSV/OG_FIELD_DW_DATA_TABLE.dsv\")\n",
    "og_operator_dw_path = Path(\"../data/PDQ_DSV/OG_OPERATOR_DW_DATA_TABLE.dsv\")\n",
    "og_regulatory_lease_dw_path = Path(\n",
    "    \"../data/PDQ_DSV/OG_REGULATORY_LEASE_DW_DATA_TABLE.dsv\"\n",
    ")\n",
    "# well completion data\n",
    "og_well_completion_path = Path(\"../data/PDQ_DSV/OG_WELL_COMPLETION_DATA_TABLE.dsv\")\n",
    "\n",
    "\n",
    "# summary tables\n",
    "og_summary_master_large_path = Path(\n",
    "    \"../data/PDQ_DSV/OG_SUMMARY_MASTER_LARGE_DATA_TABLE.dsv\"\n",
    ")\n",
    "og_summary_onshore_lease_path = Path(\n",
    "    \"../data/PDQ_DSV/OG_SUMMARY_ONSHORE_LEASE_DATA_TABLE.dsv\"\n",
    ")\n",
    "\n",
    "pdq_dsv_paths = [\n",
    "    gp_county_path,\n",
    "    gp_district_path,\n",
    "    gp_date_range_path,\n",
    "    og_county_cycle_path,\n",
    "    og_county_lease_cycle_path,\n",
    "    og_lease_cycle_path,\n",
    "    og_lease_cycle_disp_path,\n",
    "    og_district_cycle_path,\n",
    "    og_field_cycle_path,\n",
    "    og_operator_cycle_path,\n",
    "    og_field_dw_path,\n",
    "    og_operator_dw_path,\n",
    "    og_regulatory_lease_dw_path,\n",
    "    og_well_completion_path,\n",
    "    og_summary_master_large_path,\n",
    "    og_summary_onshore_lease_path,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# def get_csv_dtypes(path, sep=\"}\", nrows=10000):\n",
    "#     \"\"\"Reads in a csv file and returns the dtypes of the columns\"\"\"\n",
    "#     data = pd.read_csv(path, sep=sep, nrows=nrows)\n",
    "#     return data.dtypes.to_dict()\n",
    "from convert_to_parquet import get_csv_dtypes_for_all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def format_in_B(num):\n",
    "    \"\"\"Formats a number in thousands\"\"\"\n",
    "    for unit in [\"\", \"k\", \"M\", \"G\", \"T\"]:\n",
    "        if abs(num) < 1024:\n",
    "            return f\"{num:3.2f} {unit}B\"\n",
    "        num /= 1024\n",
    "    return f\"{num:.1f} TB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def get_table_overview_from_frame(table_name_dict, table_name):\n",
    "    \"\"\"Returns an overview of the data in a dask dataframe\"\"\"\n",
    "\n",
    "    # get the dask dataframe from the dictionary\n",
    "    ddf = table_name_dict[table_name]\n",
    "    # get the number of rows and columns\n",
    "    num_rows = ddf.shape[0]\n",
    "    # get the size on disk\n",
    "    size = ddf.memory_usage(deep=True).sum()\n",
    "    # get the column names\n",
    "    columns = ddf.columns.tolist()\n",
    "    # get the column dtypes\n",
    "    dtypes = ddf.dtypes.to_list()\n",
    "    # get the number of partitions\n",
    "\n",
    "    # compute values and put these properties in 1 row of a dataframe\n",
    "    num_rows, size = dask.compute(num_rows, size)\n",
    "    # create a dataframe with the properties\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"table\": [table_name],\n",
    "            \"rows\": [f\"{num_rows:,}\"],\n",
    "            \"cols\": [len(columns)],\n",
    "            \"columns\": [columns],\n",
    "            \"size\": [format_in_B(size)],\n",
    "            \"col_dtypes\": [dtypes],\n",
    "        }\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def get_table_overview(path, path_name, column_types={}):\n",
    "    \"\"\"\n",
    "    Returns an overview of the data in the file at the given path.\n",
    "\n",
    "    This function returns a dictionary containing the file name, the number of rows,\n",
    "    the column names, the first five rows of the data, and the column data types.\n",
    "    It uses Dask to efficiently compute the number of rows in the file, which makes\n",
    "    it suitable for large files.\n",
    "\n",
    "    Parameters:\n",
    "    path (Path or str): The path to the data file.\n",
    "\n",
    "    Returns:\n",
    "    dict: A pandas DataFrame containing the file overview.\n",
    "    \"\"\"\n",
    "    # column_types = {}\n",
    "    # Initialize ddf as an empty Dask DataFrame\n",
    "    while True:\n",
    "        try:\n",
    "            ddf = dd.read_csv(path, sep=\"}\", dtype=column_types)\n",
    "            num_rows = len(ddf)\n",
    "            break\n",
    "        except ValueError as e:\n",
    "            match = re.search(r\"dtype=\\{'(.*?)': '(.*?)'\", str(e))\n",
    "            # match = re.search(r\"dtype=\\{'(.*?)': 'object'\", str(e))\n",
    "            if match:\n",
    "                column_name = match.group(1)\n",
    "                column_types[column_name] = \"object\"\n",
    "\n",
    "    big_sample_data = pd.read_csv(path, sep=\"}\", nrows=10000, dtype=column_types)\n",
    "    if num_rows < 4:\n",
    "        sample_data = big_sample_data.sample(num_rows)\n",
    "    else:\n",
    "        sample_data = big_sample_data.sample(3)\n",
    "    # columns = sample_data.columns.tolist()\n",
    "\n",
    "    # column_dtypes = ddf.dtypes.tolist()\n",
    "    column_dtypes = ddf.dtypes.to_dict()\n",
    "    file_size = format_in_B(path.stat().st_size)\n",
    "\n",
    "    overview = pd.DataFrame(\n",
    "        {\n",
    "            \"table_var\": [path_name],\n",
    "            \"file_name\": [path.stem],\n",
    "            \"num_rows\": [f\"{num_rows:,}\"],\n",
    "            \"num_cols\": [len(column_dtypes)],\n",
    "            \"size_on_disk\": [file_size],\n",
    "            \"cols\": [list(column_dtypes.keys())],\n",
    "            \"col_dtypes\": [list(column_dtypes.values())],\n",
    "            \"file_path\": [path],\n",
    "            \"file_extension\": [path.suffix],\n",
    "            \"sample_data\": [sample_data.to_json()],\n",
    "        },\n",
    "    )\n",
    "    return overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def get_table_description(path, column_types={}):\n",
    "    \"\"\"\n",
    "    Returns a Dask dataframe with the statistical description of the data in the file at the given path.\n",
    "    \"\"\"\n",
    "    columns_needed = pd.read_csv(path, sep=\"}\", nrows=1).columns.tolist()\n",
    "    # create a new dict with only the keys needed\n",
    "    column_types_needed = {k: column_types[k] for k in columns_needed}\n",
    "    # Read the CSV file into a Dask dataframe\n",
    "    ddf = dd.read_csv(path, sep=\"}\", dtype=column_types_needed)\n",
    "\n",
    "    # Compute the descriptive statistics for the numeric columns\n",
    "    desc_ddf = ddf.describe(include=\"all\")\n",
    "    # add row with value in each of the columns in the describe dataframe for the dtype\n",
    "    num_dtypes = dd.from_pandas(\n",
    "        pd.DataFrame(desc_ddf.dtypes.apply(lambda x: x.name), columns=[\"dtype\"]).T,\n",
    "        npartitions=1,\n",
    "    )\n",
    "    # Return the descriptive statistics as a tuple of Dask dataframes\n",
    "    desc_ddf = dd.concat([desc_ddf, num_dtypes])\n",
    "\n",
    "    return desc_ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def get_table_description_from_frame(table_name_dict, table_name):\n",
    "    \"\"\"Returns a Dask Dataframe with the statistical description of the data in the table\"\"\"\n",
    "    # get the dask dataframe from the dictionary\n",
    "    ddf = table_name_dict[table_name]\n",
    "    # compute the descriptive statistics for the numeric columns\n",
    "    desc_ddf = ddf.describe(include=\"all\")\n",
    "    # add row with value in each of the columns in the describe dataframe for the dtype\n",
    "    ddf_dtypes = dd.from_pandas(\n",
    "        pd.DataFrame(desc_ddf.dtypes.apply(lambda x: x.name), columns=[\"dtype\"]).T,\n",
    "        npartitions=1,\n",
    "    )\n",
    "    # Return the descriptive statistics as a tuple of Dask dataframes\n",
    "    desc_ddf = dd.concat([desc_ddf, ddf_dtypes])\n",
    "    return desc_ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def get_data_for_plots(ddf):\n",
    "    \"\"\"takes in a dask dataframe and returns the df data to be used for plotting\"\"\"\n",
    "    # define order for the columns\n",
    "    column_order = [\n",
    "        \"dtype\",\n",
    "        \"count\",\n",
    "        \"unique\",\n",
    "        \"top\",\n",
    "        \"freq\",\n",
    "        \"mean\",\n",
    "        \"std\",\n",
    "        \"min\",\n",
    "        \"25%\",\n",
    "        \"50%\",\n",
    "        \"75%\",\n",
    "        \"max\",\n",
    "    ]\n",
    "    # get the data for the plots\n",
    "    computed_ddf = ddf.compute()\n",
    "    return computed_ddf.T[column_order].sort_values(by=\"dtype\", ascending=False)\n",
    "\n",
    "\n",
    "def plot_statistics_table_nonmissing_hbar(ddf, title=\"\"):\n",
    "    \"\"\"Plots a barh plot and a table in a layout.\"\"\"\n",
    "\n",
    "    # state opts of barh plot\n",
    "    hbar_opts = dict(\n",
    "        title=title,\n",
    "        ylabel=\"\",\n",
    "        xlabel=\"\",\n",
    "        xaxis=\"bare\",\n",
    "        tools=[\"hover\"],\n",
    "    )\n",
    "    df = pd.DataFrame()\n",
    "    df = get_data_for_plots(ddf)\n",
    "    # turn the count column into an int dtype column\n",
    "    df[\"count\"] = pd.to_numeric(df[\"count\"]).astype(int)\n",
    "    # round the values of the floats to integers\n",
    "    float_cols = [\"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"]\n",
    "    for col in float_cols:\n",
    "        df[col] = pd.to_numeric(df[col].fillna(\"0.0\"), errors=\"coerce\").astype(int)\n",
    "    # df[float_cols] = df[float_cols].round(0)\n",
    "    # create a horizontal bartplot of the count column using hvplot\n",
    "    df[\"fraction_nonmissing\"] = round(df[\"count\"] / df[\"count\"].max(), 4)\n",
    "\n",
    "    element_height = (1 + df.shape[0]) * 33\n",
    "\n",
    "    # set the index to the query_field column\n",
    "\n",
    "    # hv_table = df.hvplot.table(\n",
    "    #     df.reset_index().columns.tolist(), width=1110, height=element_height\n",
    "    # )\n",
    "    table_panel = pnw.Tabulator(df.iloc[::-1], height=element_height)\n",
    "\n",
    "    barh_plot = df.hvplot.barh(\n",
    "        y=\"fraction_nonmissing\", height=element_height, **hbar_opts\n",
    "    ).opts(active_tools=[\"box_zoom\"], toolbar=\"above\")\n",
    "    barh_panel = pn.panel(barh_plot)\n",
    "\n",
    "    # return a panel row with the tabulator table and the bar plot\n",
    "    return pn.Row(\n",
    "        barh_panel,\n",
    "        table_panel,\n",
    "        sizing_mode=\"stretch_width\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def display_table_overview_from_frame(\n",
    "    persisted_overviews, table_name_dict, table_name, nrows=15, column_types={}\n",
    "):\n",
    "    \"\"\"Display an overview of the data in a Dask DataFrame.\"\"\"\n",
    "    over_view_table = persisted_overviews[table_name]\n",
    "    # over_view_table = get_table_overview_from_frame(\n",
    "    #     table_name_dict, table_name, column_types\n",
    "    # )\n",
    "\n",
    "    # display the shape of the table with decorative ~*~ around it\n",
    "    num_rows = over_view_table[\"rows\"].values[0]\n",
    "    num_cols = over_view_table[\"cols\"].values[0]\n",
    "    print(f\"~*~ {table_name} ~*~\")\n",
    "    print(f\"{num_rows} rows, {num_cols} columns\")\n",
    "\n",
    "    # get ddf from the dictionary\n",
    "    ddf = table_name_dict[table_name]\n",
    "    # display nrows of the dask dataframe\n",
    "    display(ddf.head(nrows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def display_table_overview(path, nrows=15, column_types={}):\n",
    "    \"\"\"\n",
    "    Displays an overview of the data in the file at the given path.\n",
    "\n",
    "    This function prints the file name, the number of rows, the column names,\n",
    "    and the first five rows of the data. It uses Dask to efficiently compute\n",
    "    the number of rows in the file, which makes it suitable for large files.\n",
    "\n",
    "    Parameters:\n",
    "    path (Path or str): The path to the data file.\n",
    "    \"\"\"\n",
    "    # make a decorative line\n",
    "    decor_length = len(path.name) + 8\n",
    "    decor = \"~*~\" * (decor_length // 3)\n",
    "\n",
    "    print(f\"{decor}\")\n",
    "    print(f\"File: {path.name}\")\n",
    "    print(f\"{decor}\")\n",
    "\n",
    "    # column_types = {}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            ddf = dd.read_csv(path, sep=\"}\", dtype=column_types)\n",
    "            print(f\"Number of rows: {len(ddf):,}\")\n",
    "            break\n",
    "        except ValueError as e:\n",
    "            # print(e)\n",
    "            # If a ValueError occurs, parse the error message to get the column name\n",
    "            match = re.search(r\"dtype=\\{'(.*?)': '(.*?)'\", str(e))\n",
    "            # match = re.search(r\"dtype=\\{'(.*?)': 'object'\", str(e))\n",
    "            if match:\n",
    "                column_name = match.group(1)\n",
    "                print(f\"Problematic column: {column_name}\")\n",
    "                # Add the problematic column to the column types dictionary\n",
    "                column_types[column_name] = \"object\"\n",
    "\n",
    "    column_dtypes = ddf.dtypes.to_dict()\n",
    "    print(f\"File size: {format_in_B(path.stat().st_size)}\")\n",
    "    print(f\"Number of columns: {len(column_dtypes)}\")\n",
    "    # display(pd.DataFrame.from_dict(column_dtypes, orient=\"index\", columns=[\"dtype\"]))\n",
    "    print()\n",
    "\n",
    "    print(f\"Sample of data in {path.name}:\")\n",
    "    display(pd.read_csv(path, delimiter=\"}\", nrows=nrows, dtype=column_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# define a function that combines get_table_description, plot_statistics_table_nonmissing_hbar\n",
    "# returns a panel card with the table and the bar plot\n",
    "def get_table_card(path, column_types={}):\n",
    "    \"\"\"returns a panel card with the table and the bar plot\"\"\"\n",
    "    results = get_table_description(path, column_types)\n",
    "    bar_table = plot_statistics_table_nonmissing_hbar(results, title=path.stem)\n",
    "    return pn.Card(bar_table, title=path.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def get_table_card_from_frame(table_name_dict, table_name):\n",
    "    \"\"\"returns a panel card with the table and the bar plot\"\"\"\n",
    "    results = get_table_description_from_frame(table_name_dict, table_name)\n",
    "    bar_table = plot_statistics_table_nonmissing_hbar(results, title=table_name)\n",
    "    return pn.Card(bar_table, title=table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a default diction to store the dtypes of the columns\n",
    "csv_dtypes = get_csv_dtypes_for_all_files()\n",
    "\n",
    "# # loop through the paths and get the dtypes of the columns\n",
    "# for fpath in pdq_dsv_paths:\n",
    "#     temp_dict = get_csv_dtypes(fpath)\n",
    "#     # only update values if the key is not in the dictionary\n",
    "#     for k, v in temp_dict.items():\n",
    "#         if k not in csv_dtypes:\n",
    "#             csv_dtypes[k] = v\n",
    "\n",
    "# csv_dtypes = {k: v.name for k, v in csv_dtypes.items()}\n",
    "# # csv_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable name is the path stem before _DATA_TABLE in lowercase\n",
    "# regular_expression?\n",
    "pattern = r\"(\\w+)_DATA_TABLE\"\n",
    "\n",
    "# find the variable pair from in the pdq_dsv_paths\n",
    "variable_pairs = [\n",
    "    (my_path, re.match(pattern, my_path.stem.upper()).group(1).lower())\n",
    "    for my_path in pdq_dsv_paths\n",
    "    if re.match(pattern, my_path.stem.upper())\n",
    "]\n",
    "# variable_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview list of data tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of 1-row snips of the size of the data tables\n",
    "overview_dfs_list = [\n",
    "    get_table_overview(*pair, column_types=csv_dtypes) for pair in tqdm(variable_pairs)\n",
    "]\n",
    "# concaternate summary rows into a single dataframe\n",
    "overview_dfs = pd.concat(overview_dfs_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_dfs.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dask Client "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some tables are as large as 11GB, and this is why we use Dask. Dask enables us to handle large data files by spliting the data into chunks so that all the data is not loaded into memory at once. Dask also enables parallel processing when possible and enable us to manage the computational resources via a `Client` object. It can do a lot more too but that is outside our scope for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dask client with specified configuration\n",
    "# This client will use 4 workers, each with 1 thread and a memory limit of 2GB\n",
    "# The Dask diagnostic dashboard will be served at the address \":8788\"\n",
    "client = Client(\n",
    "    n_workers=4, threads_per_worker=1, memory_limit=\"3GB\", dashboard_address=\":8788\"\n",
    ")\n",
    "\n",
    "# Print the link to the Dask diagnostic dashboard\n",
    "# This link can be used to monitor the progress and performance of Dask computations\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persisted_overviews = {}\n",
    "\n",
    "for table_name in tqdm(data_tables_dict.keys()):\n",
    "    overview_table = get_table_overview_from_frame(data_tables_dict, table_name)\n",
    "    (persisted_overview,) = persist(overview_table)\n",
    "    persisted_overviews[table_name] = persisted_overview\n",
    "\n",
    "persisted_overviews_list = [\n",
    "    persisted_overviews[table_name] for table_name in data_tables_dict\n",
    "]\n",
    "overview_dfs_from_frame = pd.concat(persisted_overviews_list, ignore_index=True)\n",
    "overview_dfs_from_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 0 - County\n",
    "\n",
    "General purpose table that stores county information.\n",
    "\n",
    "Below is the district_no, a number representing the RRC district_name in the RRC system associated with lease reporting.\n",
    "\n",
    " DISTRICT_no (RRC VALUE) | DISTRICT_NAME |\n",
    "|--|--|\n",
    "01|01 \n",
    "02|02 \n",
    "04|04 \n",
    "05|05 \n",
    "06|06 \n",
    "07|6E (oil only)\n",
    "|08|7B\n",
    "|10|08\n",
    "|11|8A\n",
    "|13|09\n",
    "|14|10\n",
    "This value is not used. 12 | 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display_table_overview_from_frame(persisted_overviews, data_tables_dict, \"gp_county\")\n",
    "get_table_card_from_frame(data_tables_dict, \"gp_county\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dask dataframe\n",
    "# gp_county_ddf = dd.read_csv(file_path_0, sep=\"}\", dtype=csv_dtypes)\n",
    "gp_county_ddf = data_tables_dict[\"gp_county\"]\n",
    "\n",
    "gp_county_ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the the column names to snake case\n",
    "gp_county_ddf.columns = [pascal_to_snake(col) for col in gp_county_ddf.columns]\n",
    "# create column is_onshore based on on_shore_flag column\n",
    "gp_county_ddf[\"is_onshore\"] = gp_county_ddf[\"on_shore_flag\"].map(\n",
    "    {\"Y\": True, \"N\": False}\n",
    ")\n",
    "gp_county_ddf[\"is_onshore_assc_cnty\"] = gp_county_ddf[\"onshore_assc_cnty_flag\"].map(\n",
    "    {\"Y\": True, \"N\": False}\n",
    ")\n",
    "\n",
    "# create a small table to link the district_no to the district_name\n",
    "gp_district_name = (\n",
    "    gp_county_ddf[[\"district_no\", \"district_name\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=\"district_no\")\n",
    "    .reset_index(drop=True)\n",
    "    .compute()\n",
    ")\n",
    "gp_district_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some of the extra columns\n",
    "column_to_drop = [\n",
    "    \"county_fips_code\",\n",
    "    \"district_name\",\n",
    "    \"on_shore_flag\",\n",
    "    \"onshore_assc_cnty_flag\",\n",
    "]\n",
    "columns_to_keep = [col for col in gp_county_ddf.columns if col not in column_to_drop]\n",
    "gp_county_ddf = gp_county_ddf[columns_to_keep]\n",
    "gp_county_ddf.info(memory_usage=\"deep\")\n",
    "gp_county_ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of counties in each district\n",
    "district_county_count = (\n",
    "    gp_county_ddf.groupby(\"district_no\")\n",
    "    .county_no.nunique()\n",
    "    .rename(\"county_count\")\n",
    "    .compute()\n",
    ")\n",
    "\n",
    "district_county_count.hvplot.bar(\n",
    "    title=\"Count of Counties in each District\", xlabel=\"\", ylabel=\"\", hover_cols=\"all\"\n",
    ").opts(\n",
    "    toolbar=\"above\",\n",
    "    active_tools=[\"box_zoom\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gp_county_ddf[\"is_onshore_assc_cnty\"] = gp_county_ddf[\"is_onshore_assc_cnty\"].astype(\n",
    "#     int\n",
    "# )\n",
    "# gp_county_ddf[\"is_onshore\"] = gp_county_ddf[\"is_onshore\"].astype(int)\n",
    "\n",
    "counts = (\n",
    "    gp_county_ddf.groupby([\"district_no\", \"is_onshore\", \"is_onshore_assc_cnty\"])\n",
    "    .count()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_labels(var_1, var_2, col1, col2):\n",
    "    # Filter the data for the given 'is_onshore' value\n",
    "    data = counts[(counts[col1] == var_1) & (counts[col2] == var_2)]\n",
    "\n",
    "    # Create a bar plot\n",
    "    bar_plot = data.hvplot.bar(\n",
    "        x=\"district_no\",\n",
    "        y=\"county_no\",\n",
    "        stacked=True,\n",
    "        xlabel=\"\",\n",
    "        yaxis=\"bare\",\n",
    "        title=f\"Count of Counties in each District No. where {col1}={var_1} & {col2}={var_2}\",\n",
    "    )\n",
    "\n",
    "    # Create a list to hold the text elements\n",
    "    texts = []\n",
    "\n",
    "    # Loop over the DataFrame and create a text element for each row\n",
    "    for row in data.itertuples():\n",
    "        # The position of the text is the center of the bar\n",
    "        x = getattr(row, \"district_no\")\n",
    "        y = getattr(row, \"county_no\") / 2\n",
    "        # The text is the height of the bar\n",
    "        text = str(getattr(row, \"county_no\"))\n",
    "        # Create the text element and add it to the list\n",
    "        texts.append(hv.Text(x, y, text).opts(text_color=\"white\"))\n",
    "\n",
    "    # Overlay the text elements on the plot\n",
    "    labelled_plot = bar_plot * hv.Overlay(texts)\n",
    "\n",
    "    return labelled_plot\n",
    "\n",
    "\n",
    "# Create a DynamicMap with the 'plot_with_labels' function\n",
    "dynamic_map = hv.DynamicMap(\n",
    "    lambda var_1, var_2: plot_with_labels(\n",
    "        var_1, var_2, \"is_onshore\", \"is_onshore_assc_cnty\"\n",
    "    ),\n",
    "    kdims=[\n",
    "        hv.Dimension(\"var_1\", label=\"is_onshore\"),\n",
    "        hv.Dimension(\"var_2\", label=\"is_onshore_assc_cnty\"),\n",
    "    ],\n",
    ").redim.range(\n",
    "    var_1=(0, 1),\n",
    "    var_2=(0, 1),\n",
    ")\n",
    "dynamic_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gp_county_ddf[gp_county_ddf[\"is_onshore\"] == 0].compute()\n",
    "gp_county_ddf[gp_county_ddf[\"is_onshore_assc_cnty\"] == 1].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check that each county is only in 1 district\n",
    "district_counties_dict = (\n",
    "    gp_county_ddf.compute().groupby(\"district_no\")[\"county_no\"].apply(set).to_dict()\n",
    ")\n",
    "# district_counties_dict[1]\n",
    "\n",
    "keys = list(district_counties_dict.keys())\n",
    "found_common = False\n",
    "\n",
    "# Convert the values in district_counties_dict to sets\n",
    "district_counties_dict = {k: set(v) for k, v in district_counties_dict.items()}\n",
    "\n",
    "# Iterate over each key in the list\n",
    "for i in range(len(keys)):\n",
    "    # Get the key and values for the current index\n",
    "    key1 = keys[i]\n",
    "    values1 = district_counties_dict[key1]\n",
    "\n",
    "    # Iterate over all subsequent keys in the list\n",
    "    for j in range(i + 1, len(keys)):\n",
    "        # Get the key and values for the subsequent index\n",
    "        key2 = keys[j]\n",
    "        values2 = district_counties_dict[key2]\n",
    "\n",
    "        # Check if any value from the first pair is in the values of the second pair\n",
    "        common_values = values1.intersection(values2)\n",
    "        if common_values:\n",
    "            print(f\"Key {key1} has common values with key {key2}: {common_values}\")\n",
    "            found_common = True\n",
    "if not found_common:\n",
    "    print(\"No common values found\")\n",
    "\n",
    "# district_counties_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 1\n",
    "\n",
    "General purpose table that contains district information.\n",
    "\n",
    "Includes the info of the office phone number and which town they are located in per that district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display_table_overview_from_frame(persisted_overviews, data_tables_dict, \"gp_district\")\n",
    "get_table_card_from_frame(data_tables_dict, \"gp_district\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 2\n",
    "\n",
    "General purpose table of PDQ data range (Jan. 1993-current production report month/year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_tables_dict[\"gp_date_range_cycle\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 3 - County Cycle\n",
    "\n",
    "Contains production report data reported by lease and month (YYYYMM) aggregated by the county in which the wells are located. This is an estimate only based on allowables and potentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"og_county_cycle\"\n",
    ")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_county_cycle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns with any null value in the first row\n",
    "og_county_cycle_ddf = data_tables_dict[\"og_county_cycle\"]\n",
    "og_county_cycle_ddf = og_county_cycle_ddf.drop(\n",
    "    og_county_cycle_ddf.columns[og_county_cycle_ddf.isnull().any()], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# change column to lower case\n",
    "og_county_cycle_ddf.columns = [\n",
    "    pascal_to_snake(col) for col in og_county_cycle_ddf.columns\n",
    "]\n",
    "# create column is_gas based on the oil_gas_code column\n",
    "og_county_cycle_ddf[\"is_gas\"] = (\n",
    "    og_county_cycle_ddf[\"oil_gas_code\"].map({\"G\": True, \"O\": False}).astype(\"int8\")\n",
    ")\n",
    "\n",
    "non_null_columns = [\n",
    "    col\n",
    "    for col in og_county_cycle_ddf.columns\n",
    "    if col\n",
    "    not in [\"district_name\", \"county_name\", \"cycle_month\", \"cycle_year\", \"oil_gas_code\"]\n",
    "]\n",
    "# drop the columns we don't want from the ddf\n",
    "og_county_cycle_ddf = og_county_cycle_ddf[non_null_columns]\n",
    "# drop rows with the cycle_year_month < 2013\n",
    "og_county_cycle_ddf = og_county_cycle_ddf[og_county_cycle_ddf.cycle_year_month > 201300]\n",
    "\n",
    "# get shape of the ddf\n",
    "num_rows = og_county_cycle_ddf.shape[0]\n",
    "num_cols = og_county_cycle_ddf.shape[1]\n",
    "\n",
    "# get the size of the ddf\n",
    "size = og_county_cycle_ddf.memory_usage(deep=True).sum()\n",
    "\n",
    "# compute values and put these properties in 1 row of a dataframe\n",
    "num_rows, size = dask.compute(num_rows, size)\n",
    "\n",
    "print(\n",
    "    f\"Shape of og_county_cycle_ddf: ({og_county_cycle_ddf.shape[0].compute()} , {og_county_cycle_ddf.shape[1]})\"\n",
    ")\n",
    "print(f\"Size of og_county_cycle_ddf: {format_in_B(size)}\")\n",
    "og_county_cycle_ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 4 - County Lease Cycle\n",
    "\n",
    "Contains production report data reported by lease and month (YYYYMM) aggregated by lease and county in which the wells are located. This is an estimate only based on allowables and potentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"og_county_lease_cycle\"\n",
    ")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_county_lease_cycle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 5 - Lease Cycle\n",
    "\n",
    "Contains production report data reported by lease and month (YYYYMM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"og_lease_cycle\"\n",
    ")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_lease_cycle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dask dataframe\n",
    "og_lease_cycle_ddf = data_tables_dict[\"og_lease_cycle\"]\n",
    "# read in the dask dataframe\n",
    "# og_lease_cycle_ddf = dd.read_csv(file_path_5, sep=\"}\", dtype=csv_dtypes)\n",
    "# convert the column names to lower case\n",
    "og_lease_cycle_ddf = og_lease_cycle_ddf.rename(columns=str.lower)\n",
    "columns_list = og_lease_cycle_ddf.columns.tolist()\n",
    "# drop certain columns based on substrings\n",
    "# create column is_gas based on the oil_gas_code column\n",
    "og_lease_cycle_ddf[\"is_gas\"] = (\n",
    "    og_lease_cycle_ddf[\"oil_gas_code\"].map({\"G\": True, \"O\": False}).astype(\"int8\")\n",
    ")\n",
    "\n",
    "# Define the regex pattern\n",
    "pattern = re.compile(r\"_code$|_name$|cycle_month|cycle_year$|^district_no|lease_no$\")\n",
    "\n",
    "# Filter the columns\n",
    "columns_to_keep = [col for col in columns_list if not pattern.search(col)]\n",
    "og_lease_cycle_ddf = og_lease_cycle_ddf[columns_to_keep]\n",
    "# filter out the row with cycle_year_month < 201300\n",
    "og_lease_cycle_ddf = og_lease_cycle_ddf[og_lease_cycle_ddf.cycle_year_month > 201300]\n",
    "\n",
    "# get shape\n",
    "num_rows = og_lease_cycle_ddf.shape[0]\n",
    "num_cols = og_lease_cycle_ddf.shape[1]\n",
    "# lease_no are unique within districts\n",
    "num_lease_districts = og_lease_cycle_ddf.lease_no_district_no.nunique()\n",
    "# get the memory size of the new filtered ddf\n",
    "mem_size = og_lease_cycle_ddf.memory_usage(deep=True).sum()\n",
    "\n",
    "num_rows, num_lease_districts, mem_size = dask.compute(\n",
    "    num_rows, num_lease_districts, mem_size\n",
    ")\n",
    "print(f\"Shape of og_lease_cycle_ddf: ({num_rows} , {num_cols})\")\n",
    "print(f\"Number of unique lease_no_district_no: {num_lease_districts:,}\")\n",
    "print(f\"Memory size of og_lease_cycle_ddf: {format_in_B(mem_size)}\")\n",
    "og_lease_cycle_ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_lease_cycle_ddf[\"district_no\"] = (\n",
    "    og_lease_cycle_ddf[\"lease_no_district_no\"].astype(str).str[-2:]\n",
    ")\n",
    "# total production numbers by district\n",
    "og_lease_cycle_ddf.groupby(\"district_no\")[\n",
    "    [\n",
    "        \"lease_oil_prod_vol\",\n",
    "        \"lease_gas_prod_vol\",\n",
    "        \"lease_cond_prod_vol\",\n",
    "        \"lease_csgd_prod_vol\",\n",
    "    ]\n",
    "].sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 6 - Lease Cycle Disposition\n",
    "\n",
    "Contains production report disposition data reported by lease and month (YYYYMM).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"og_lease_cycle_disp\"\n",
    ")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_lease_cycle_disp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dask dataframe\n",
    "og_lease_cycle_disp_ddf = data_tables_dict[\"og_lease_cycle_disp\"]\n",
    "# og_lease_cycle_disp_ddf = dd.read_csv(file_path_6, sep=\"}\", dtype=csv_dtypes)\n",
    "# convert the column names to lower case\n",
    "og_lease_cycle_disp_ddf.columns = [\n",
    "    pascal_to_snake(col) for col in og_lease_cycle_disp_ddf.columns\n",
    "]\n",
    "# columns_list = og_lease_cycle_disp_ddf.columns.tolist()\n",
    "\n",
    "# og_lease_cycle_disp_ddf = og_lease_cycle_disp_ddf.rename(columns=str.lower)\n",
    "# create column is_gas based on the oil_gas_code column\n",
    "og_lease_cycle_disp_ddf[\"is_gas\"] = (\n",
    "    og_lease_cycle_disp_ddf[\"oil_gas_code\"].map({\"G\": True, \"O\": False}).astype(\"int8\")\n",
    ")\n",
    "og_lease_cycle_disp_ddf[\"lease_no_district_no\"] = og_lease_cycle_disp_ddf[\n",
    "    \"lease_no\"\n",
    "].astype(str) + og_lease_cycle_disp_ddf[\"district_no\"].astype(str).str.zfill(2)\n",
    "og_lease_cycle_disp_ddf[\"lease_no_district_no\"] = og_lease_cycle_disp_ddf[\n",
    "    \"lease_no_district_no\"\n",
    "].astype(int)\n",
    "\n",
    "# Define the regex pattern\n",
    "pattern = re.compile(r\"_code$|_name$|cycle_month|cycle_year$|^district_no|lease_no$\")\n",
    "\n",
    "# Filter the columns\n",
    "columns_to_keep = [\n",
    "    col for col in og_lease_cycle_disp_ddf.columns if not pattern.search(col)\n",
    "]\n",
    "og_lease_cycle_disp_ddf = og_lease_cycle_disp_ddf[columns_to_keep]\n",
    "# filter out the row with cycle_year_month < 201300\n",
    "og_lease_cycle_disp_ddf = og_lease_cycle_disp_ddf[\n",
    "    og_lease_cycle_disp_ddf.cycle_year_month > 201300\n",
    "]\n",
    "\n",
    "num_rows = og_lease_cycle_disp_ddf.shape[0]\n",
    "num_cols = og_lease_cycle_disp_ddf.shape[1]\n",
    "# group by district_no and get the nunique of lease_no\n",
    "num_lease_districts = og_lease_cycle_disp_ddf[\"lease_no_district_no\"].nunique()\n",
    "memory_usage = og_lease_cycle_disp_ddf.memory_usage(index=True, deep=True).sum()\n",
    "num_rows, num_lease_districts, memory_usage = dask.compute(\n",
    "    num_rows, num_lease_districts, memory_usage\n",
    ")\n",
    "print(\n",
    "    f\"Shape of og_lease_cycle_disp_ddf: ({num_rows} , {num_cols})\\nNumber of unique lease_no_district_no: {num_lease_districts}\"\n",
    ")\n",
    "print(f\"Total number of leases: {num_lease_districts}\")\n",
    "print(f\"Total memory usage: {format_in_B(memory_usage)}\")\n",
    "# og_lease_cycle_disp_ddf.info(memory_usage=\"deep\")\n",
    "og_lease_cycle_disp_ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# og_lease_cycle_disp_ddf.memory_usage(index=True, deep=True).compute()\n",
    "og_lease_cycle_disp_ddf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 7 - District Cycle\n",
    "\n",
    "Contains production report data reported by lease and month (YYYYMM) aggregated by the completion district for the lease ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"og_district_cycle\"\n",
    ")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_district_cycle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dask dataframe\n",
    "og_district_cycle_ddf = data_tables_dict[\"og_district_cycle\"]\n",
    "# og_district_cycle_ddf = dd.read_csv(file_path_7, sep=\"}\", dtype=csv_dtypes)\n",
    "# convert the column names to lower case\n",
    "og_district_cycle_ddf = og_district_cycle_ddf.rename(columns=str.lower)\n",
    "# get list of columns\n",
    "columns_list = og_district_cycle_ddf.columns.tolist()\n",
    "\n",
    "# drop the columns we don't want from columns list\n",
    "columns_list = [\n",
    "    col\n",
    "    for col in columns_list\n",
    "    if col not in [\"district_name\", \"cycle_month\", \"cycle_year\"]\n",
    "]\n",
    "# drop the column from the dataframe\n",
    "og_district_cycle_ddf = og_district_cycle_ddf[columns_list]\n",
    "\n",
    "# drop  row with cycle_year < 2013\n",
    "og_district_cycle_ddf = og_district_cycle_ddf[\n",
    "    og_district_cycle_ddf.cycle_year_month > 201300\n",
    "]\n",
    "num_rows = og_district_cycle_ddf.shape[0]\n",
    "memory_usage = og_district_cycle_ddf.memory_usage(index=True, deep=True).sum()\n",
    "# compute the number of rows and memory usage\n",
    "num_rows, memory_usage = dask.compute(num_rows, memory_usage)\n",
    "# show first few rows and shape\n",
    "print(f\"Shape of og_district_cycle_ddf: ({num_rows} , {len(columns_list)}\")\n",
    "print(f\"Total memory usage: {format_in_B(memory_usage)}\")\n",
    "\n",
    "og_district_cycle_ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 8 - Field Cycle\n",
    "\n",
    "Contains production report data reported by lease and month (YYYYMM) aggregated by the field in which the well(s) for the lease are completed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"og_field_cycle\"\n",
    ")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_field_cycle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dask dataframe\n",
    "og_field_cycle_ddf = data_tables_dict[\"og_field_cycle\"]\n",
    "\n",
    "# og_field_cycle_ddf = dd.read_csv(file_path_8, sep=\"}\", dtype=csv_dtypes)\n",
    "\n",
    "# convert the column names to lower case\n",
    "\n",
    "og_field_cycle_ddf = og_field_cycle_ddf.rename(columns=str.lower)\n",
    "\n",
    "# drop the columns we don't want from columns list\n",
    "\n",
    "columns_list = og_field_cycle_ddf.columns.tolist()\n",
    "\n",
    "columns_to_keep = [\n",
    "    col\n",
    "    for col in columns_list\n",
    "    if col not in [\"district_name\", \"county_name\", \"cycle_month\", \"cycle_year\"]\n",
    "]\n",
    "\n",
    "# drop the column from the dataframe\n",
    "\n",
    "og_field_cycle_ddf = og_field_cycle_ddf[columns_to_keep]\n",
    "\n",
    "# drop  row with cycle_year_month < 201300\n",
    "\n",
    "og_field_cycle_ddf = og_field_cycle_ddf[og_field_cycle_ddf.cycle_year_month > 201300]\n",
    "\n",
    "# drop the rows with null values for the field_name column\n",
    "\n",
    "og_field_cycle_ddf = og_field_cycle_ddf.dropna(subset=[\"field_name\"])\n",
    "\n",
    "\n",
    "# get new number of rows and memory usage\n",
    "\n",
    "num_rows = og_field_cycle_ddf.shape[0]\n",
    "\n",
    "memory_usage = og_field_cycle_ddf.memory_usage(index=True, deep=True).sum()\n",
    "\n",
    "# compute the number of rows and memory usage\n",
    "\n",
    "num_rows, memory_usage = dask.compute(num_rows, memory_usage)\n",
    "\n",
    "# show first few rows and shape\n",
    "\n",
    "print(\n",
    "    f\"Shape of og_field_cycle_ddf: ({num_rows} , {len(columns_list)})\\nTotal memory usage: {format_in_B(memory_usage)}\"\n",
    ")\n",
    "\n",
    "\n",
    "og_field_cycle_ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 9 - Operator Cycle\n",
    "\n",
    "Contains production report data reported by lease and month (YYYYMM) aggregated by the operator of the lease.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"og_operator_cycle\"\n",
    ")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_operator_cycle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dask dataframe\n",
    "# og_operator_cycle_ddf = dd.read_csv(file_path_9, sep=\"}\", dtype=csv_dtypes)\n",
    "og_operator_cycle_ddf = data_tables_dict[\"og_operator_cycle\"]\n",
    "# convert the column names to lower case\n",
    "og_operator_cycle_ddf = og_operator_cycle_ddf.rename(columns=str.lower)\n",
    "# drop the 'cycle_month' and 'cycle_year' columns\n",
    "columns_to_keep = [\n",
    "    col\n",
    "    for col in og_operator_cycle_ddf.columns\n",
    "    if col not in [\"cycle_month\", \"cycle_year\"]\n",
    "]\n",
    "og_operator_cycle_ddf = og_operator_cycle_ddf[columns_to_keep]\n",
    "# filter out the row with cycle_year_month < 201300\n",
    "og_operator_cycle_ddf = og_operator_cycle_ddf[\n",
    "    og_operator_cycle_ddf.cycle_year_month > 201300\n",
    "]\n",
    "# drop the rows with null values for the operator_name column\n",
    "og_operator_cycle_ddf = og_operator_cycle_ddf.dropna(subset=[\"operator_name\"])\n",
    "# show first few rows and get shape\n",
    "num_rows = og_operator_cycle_ddf.shape[0]\n",
    "num_cols = og_operator_cycle_ddf.shape[1]\n",
    "memory_usage = og_operator_cycle_ddf.memory_usage(index=True, deep=True).sum()\n",
    "num_rows, memory_usage = dask.compute(num_rows, memory_usage)\n",
    "print(\n",
    "    f\"Shape of og_operator_cycle_ddf: ({num_rows} , {num_cols})\\nTotal memory usage: {format_in_B(memory_usage)}\"\n",
    ")\n",
    "og_operator_cycle_ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column for the total production to filter out the non producing operators\n",
    "og_operator_cycle_ddf[\"total_oper_prod_vol\"] = (\n",
    "    og_operator_cycle_ddf[\"oper_oil_prod_vol\"]\n",
    "    + og_operator_cycle_ddf[\"oper_gas_prod_vol\"]\n",
    "    + og_operator_cycle_ddf[\"oper_cond_prod_vol\"]\n",
    "    + og_operator_cycle_ddf[\"oper_csgd_prod_vol\"]\n",
    ")\n",
    "og_operator_cycle_ddf[\"is_producing\"] = og_operator_cycle_ddf[\"total_oper_prod_vol\"] > 0\n",
    "producing_operator_nos = (\n",
    "    og_operator_cycle_ddf[og_operator_cycle_ddf[\"is_producing\"]][\"operator_no\"]\n",
    "    .unique()\n",
    "    .values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 10 - Field DW\n",
    "\n",
    "\n",
    "Table of field identifying data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path_10 = overview_dfs.loc[10, \"file_path\"]\n",
    "# display_table_overview(file_path_10, column_types=csv_dtypes)\n",
    "# get_table_description(file_path_10, column_types=csv_dtypes).compute().T\n",
    "# get_table_card(file_path_10, column_types=csv_dtypes)\n",
    "display_table_overview_from_frame(persisted_overviews, data_tables_dict, \"og_field_dw\")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_field_dw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dask dataframe\n",
    "# og_field_dw_df = dd.read_csv(file_path_10, sep=\"}\", dtype=csv_dtypes)\n",
    "og_field_dw_ddf = data_tables_dict[\"og_field_dw\"]\n",
    "\n",
    "# can load the whole dataframe into memory as it is small\n",
    "og_field_dw_df = og_field_dw_ddf.compute()\n",
    "\n",
    "# convert the column names to lower case\n",
    "og_field_dw_df = og_field_dw_df.rename(columns=str.lower)\n",
    "\n",
    "columns_list = og_field_dw_df.columns.tolist()\n",
    "\n",
    "# drop the columns with all nans and get the columns list\n",
    "substrings = {\"modify\", \"remarks\", \"rev_rule\", \"create\", \"district_name\"}\n",
    "columns_list = [\n",
    "    col for col in columns_list if not any(substring in col for substring in substrings)\n",
    "]\n",
    "\n",
    "# convert field_no to an int then zfill(8)\n",
    "og_field_dw_df[\"field_no\"] = (\n",
    "    og_field_dw_df[\"field_no\"].astype(int).astype(str).str.zfill(8)\n",
    ")\n",
    "\n",
    "\n",
    "# convert the o_discovery_dt and g_discovery_dt to datetime\n",
    "def parse_dates(date_string):\n",
    "    if pd.isnull(date_string):\n",
    "        return pd.NaT\n",
    "    dt = datetime.strptime(date_string, \"%d-%b-%y\")\n",
    "    if dt.year > 2023:  # replace with the current year\n",
    "        dt = dt.replace(year=dt.year - 100)\n",
    "    return dt\n",
    "\n",
    "\n",
    "og_field_dw_df[\"o_discovery_dt\"] = og_field_dw_df[\"o_discovery_dt\"].apply(parse_dates)\n",
    "og_field_dw_df[\"g_discovery_dt\"] = og_field_dw_df[\"g_discovery_dt\"].apply(parse_dates)\n",
    "\n",
    "\n",
    "# drop the column from the dataframe\n",
    "og_field_dw_df = og_field_dw_df[columns_list]\n",
    "# get the number of rows and memory usage\n",
    "num_rows = og_field_dw_df.shape[0]\n",
    "memory_usage = og_field_dw_df.memory_usage(index=True, deep=True).sum()\n",
    "\n",
    "# show first few rows and shape\n",
    "print(f\"Shape of og_field_dw_df: ({og_field_dw_df.shape}\")\n",
    "print(f\"Total memory usage: {format_in_B(memory_usage)}\")\n",
    "og_field_dw_df.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 11 - Operator DW\n",
    "\n",
    "This table contains identifying operator information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path_11 = overview_dfs.loc[11, \"file_path\"]\n",
    "# display_table_overview(file_path_11, column_types=csv_dtypes)\n",
    "# # get_table_description(file_path_11, column_types=csv_dtypes).compute().T\n",
    "# get_table_card(file_path_11, column_types=csv_dtypes)\n",
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"og_operator_dw\"\n",
    ")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_operator_dw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dask dataframe\n",
    "# og_operator_dw_ddf = dd.read_csv(file_path_11, sep=\"}\", dtype=csv_dtypes)\n",
    "og_operator_dw_ddf = data_tables_dict[\"og_operator_dw\"]\n",
    "\n",
    "# load in as pandas dataframe as it is small\n",
    "og_operator_dw_df = og_operator_dw_ddf.compute()\n",
    "\n",
    "\n",
    "# convert the column names to lower case\n",
    "og_operator_dw_df.columns = [pascal_to_snake(col) for col in og_operator_dw_df.columns]\n",
    "\n",
    "# drop the columns with all nans and other useless columns\n",
    "pattern = re.compile(r\"modify|efile|record|create\")\n",
    "columns_to_keep = [col for col in og_operator_dw_df.columns if not pattern.search(col)]\n",
    "\n",
    "# convert p5Llast_filed_dt to datetime\n",
    "og_operator_dw_df[\"p5_last_filed_dt\"] = pd.to_datetime(\n",
    "    og_operator_dw_df[\"p5_last_filed_dt\"].astype(str), errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# drop the column from the dataframe\n",
    "og_operator_dw_df = og_operator_dw_df[columns_to_keep]\n",
    "\n",
    "# strip the excess space from the Letter in the p5_status_code column\n",
    "og_operator_dw_df[\"p5_status_code\"] = og_operator_dw_df[\"p5_status_code\"].str.strip()\n",
    "\n",
    "# get the number of rows and memory usage\n",
    "num_rows = og_operator_dw_df.shape[0]\n",
    "memory_usage = og_operator_dw_df.memory_usage(index=True, deep=True).sum()\n",
    "\n",
    "# show first few rows and shape\n",
    "print(f\"Shape of og_operator_dw_df: ({num_rows} , {len(columns_to_keep)})\")\n",
    "print(f\"Total memory usage: {format_in_B(memory_usage)}\")\n",
    "og_operator_dw_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a table with the operator_no and operator_name\n",
    "rrc_pattern = \"RAILROAD COMMISSION\"\n",
    "operator_no_name = og_operator_dw_df[\n",
    "    ~og_operator_dw_df[\"operator_name\"].str.contains(rrc_pattern, regex=True)\n",
    "][[\"operator_no\", \"operator_name\"]]\n",
    "\n",
    "\n",
    "operator_no_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producing_operator_nos_list = producing_operator_nos.compute().tolist()\n",
    "og_operator_dw_df[og_operator_dw_df[\"operator_no\"].isin(producing_operator_nos_list)]\n",
    "\n",
    "# get the operator_name of the producing_operator_nos\n",
    "# filtered_df = og_operator_dw_ddf[\n",
    "#     og_operator_dw_ddf[\"operator_no\"].isin(producing_operator_nos)\n",
    "# ].persist()\n",
    "\n",
    "# filtered_df.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 12 - Regulatory Lease DW\n",
    "\n",
    "This table contains identifying lease information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"og_regulatory_lease_dw\"\n",
    ")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_regulatory_lease_dw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dask dataframe\n",
    "# og_regulatory_lease_dw_ddf = dd.read_csv(file_path_12, sep=\"}\", dtype=csv_dtypes)\n",
    "og_regulatory_lease_dw_ddf = data_tables_dict[\"og_regulatory_lease_dw\"]\n",
    "# convert the column names to lower case\n",
    "og_regulatory_lease_dw_ddf.columns = [\n",
    "    pascal_to_snake(col) for col in og_regulatory_lease_dw_ddf.columns\n",
    "]\n",
    "# create column is_gas based on the oil_gas_code column\n",
    "og_regulatory_lease_dw_ddf[\"is_gas\"] = og_regulatory_lease_dw_ddf[\"oil_gas_code\"].map(\n",
    "    {\"G\": True, \"O\": False}\n",
    ")\n",
    "# drop the columns we do not need to keep\n",
    "pattern = re.compile(r\"field_name|operator_name|district_name|oil_gas_code|field_name\")\n",
    "columns_to_keep = [\n",
    "    col for col in og_regulatory_lease_dw_ddf.columns if not pattern.search(col)\n",
    "]\n",
    "# drop the column from the dataframe\n",
    "og_regulatory_lease_dw_ddf = og_regulatory_lease_dw_ddf[columns_to_keep]\n",
    "# get the number of rows and memory usage\n",
    "num_rows = og_regulatory_lease_dw_ddf.shape[0]\n",
    "memory_usage = og_regulatory_lease_dw_ddf.memory_usage(index=True, deep=True).sum()\n",
    "num_rows, memory_usage = dask.compute(num_rows, memory_usage)\n",
    "# show first few rows and shape\n",
    "print(\n",
    "    f\"Shape of og_regulatory_lease_dw_ddf: ({num_rows} , {len(columns_to_keep)})\\nTotal memory usage: {format_in_B(memory_usage)}\"\n",
    ")\n",
    "og_regulatory_lease_dw_ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_regulatory_lease_dw_df = og_regulatory_lease_dw_ddf.compute()\n",
    "\n",
    "\n",
    "counts_df = (\n",
    "    og_regulatory_lease_dw_df.groupby([\"district_no\", \"is_gas\"])[\"lease_no\"]\n",
    "    .count()\n",
    "    .reset_index()\n",
    "    .set_index(\"district_no\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df[counts_df[\"is_gas\"] == 1][\"lease_no\"].hvplot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.state.kill_all_servers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas\n",
    "\n",
    "\n",
    "def plot_data(gas):\n",
    "    filtered_data = og_regulatory_lease_dw_df[\n",
    "        og_regulatory_lease_dw_df[\"is_gas\"] == gas\n",
    "    ]\n",
    "    counts_df = filtered_data.groupby(\"district_no\")[\"lease_no\"].count().reset_index()\n",
    "    counts_df.columns = [\"district_no\", \"lease_count\"]\n",
    "    return counts_df.hvplot.bar(\n",
    "        x=\"district_no\",\n",
    "        y=\"lease_count\",\n",
    "        title=f\"Count of Leases in each District whre is_gas={gas}\",\n",
    "    ).opts(\n",
    "        toolbar=\"above\",\n",
    "        active_tools=[\"box_zoom\"],\n",
    "    )\n",
    "\n",
    "\n",
    "is_gas_slider = pn.widgets.IntSlider(name=\"is_gas\", start=0, end=1, step=1, value=0)\n",
    "ibars = pn.panel(pn.bind(plot_data, gas=is_gas_slider), width=880)\n",
    "\n",
    "pn.Card(is_gas_slider, ibars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.dask\n",
    "import hvplot.pandas\n",
    "\n",
    "lease_count = (\n",
    "    og_regulatory_lease_dw_ddf.compute()\n",
    "    .groupby([\"district_no\", \"is_gas\"])[\"lease_no\"]\n",
    "    .count()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "# create a dynamic map to plot the number of leases in each district bar plot and is_gas widget controller\n",
    "def lease_plot_with_labels(var1, col1):\n",
    "    # Filter the data for the given 'is_gas' value\n",
    "    data = lease_count[(lease_count[col1] == var1)]\n",
    "\n",
    "    # Create a bar plot\n",
    "    bar_plot = data.hvplot.bar(\n",
    "        x=\"district_no\",\n",
    "        y=\"lease_no\",\n",
    "        stacked=True,\n",
    "        xlabel=\"\",\n",
    "        yaxis=\"bare\",\n",
    "        title=f\"Count of Leases in each District No. where {col1}={var1}\",\n",
    "        hover_cols=\"all\",\n",
    "    )\n",
    "\n",
    "    # Create a list to hold the text elements\n",
    "    texts = []\n",
    "\n",
    "    # Loop over the DataFrame and create a text element for each row\n",
    "    for row in data.itertuples():\n",
    "        # The position of the text is center of bar\n",
    "        x = getattr(row, \"district_no\")\n",
    "        y = getattr(row, \"lease_no\") * 1.01\n",
    "        # The text is the height of the bar\n",
    "        text = str(getattr(row, \"lease_no\"))\n",
    "        # Create the text element and add it to the list\n",
    "        texts.append(hv.Text(x, y, text, valign=\"bottom\").opts(text_color=\"black\"))\n",
    "\n",
    "    # Overlay the text elements on the plot\n",
    "    labelled_plot = bar_plot * hv.Overlay(texts)\n",
    "\n",
    "    return labelled_plot\n",
    "\n",
    "\n",
    "lease_dmap = hv.DynamicMap(\n",
    "    lambda var1: lease_plot_with_labels(var1, \"is_gas\"),\n",
    "    kdims=[\n",
    "        hv.Dimension(\"var1\", label=\"is_gas\"),\n",
    "    ],\n",
    ").redim.range(var1=(0, 1))\n",
    "pn.panel(lease_dmap).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 13 - Well Completion\n",
    "\n",
    "This table contains identifying well completion information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.state.kill_all_servers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"og_well_completion\"\n",
    ")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_well_completion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dask dataframe\n",
    "# og_well_completion_ddf = dd.read_csv(file_path_13, sep=\"}\", dtype=csv_dtypes)\n",
    "og_well_completion_ddf = data_tables_dict[\"og_well_completion\"]\n",
    "# convert the column names to lower case\n",
    "og_well_completion_ddf.columns = [\n",
    "    pascal_to_snake(col) for col in og_well_completion_ddf.columns\n",
    "]\n",
    "og_well_completion_ddf[\"is_gas\"] = og_well_completion_ddf[\"oil_gas_code\"].map(\n",
    "    {\"G\": True, \"O\": False}\n",
    ")\n",
    "\n",
    "# drop the columns we do not need to keep\n",
    "pattern = re.compile(r\"_name|oil_gas_code\")\n",
    "columns_to_keep = [\n",
    "    col for col in og_well_completion_ddf.columns if not pattern.search(col)\n",
    "]\n",
    "# drop the column from the dataframe\n",
    "og_well_completion_ddf = og_well_completion_ddf[columns_to_keep]\n",
    "\n",
    "# create column lease_no_district_no which combines the lease_no and district_no\n",
    "og_well_completion_ddf[\"lease_no_district_no\"] = og_well_completion_ddf[\n",
    "    \"lease_no\"\n",
    "].astype(str) + og_well_completion_ddf[\"district_no\"].astype(str).str.zfill(2)\n",
    "\n",
    "# create column tx_api which combines the api_county_code and api_unique_no\n",
    "og_well_completion_ddf[\"tx_api\"] = og_well_completion_ddf[\"api_county_code\"].astype(\n",
    "    str\n",
    ").str.zfill(3) + og_well_completion_ddf[\"api_unique_no\"].astype(str).str.zfill(5)\n",
    "# create a column for the tx_api count\n",
    "og_well_completion_ddf[\"tx_api_count\"] = og_well_completion_ddf.groupby(\"tx_api\")[\n",
    "    \"tx_api\"\n",
    "].transform(\"count\")\n",
    "og_well_completion_ddf[\"lease_no_district_no_count\"] = og_well_completion_ddf.groupby(\n",
    "    \"lease_no_district_no\"\n",
    ")[\"lease_no_district_no\"].transform(\"count\")\n",
    "\n",
    "\n",
    "# get the number of rows and memory usage\n",
    "num_rows = og_well_completion_ddf.shape[0]\n",
    "memory_usage = og_well_completion_ddf.memory_usage(index=True, deep=True).sum()\n",
    "num_rows, memory_usage = dask.compute(num_rows, memory_usage)\n",
    "# show first few rows and shape\n",
    "print(f\"Shape of og_well_completion_ddf: ({num_rows} , {len(columns_to_keep)}\")\n",
    "print(f\"Total memory usage: {format_in_B(memory_usage)}\")\n",
    "og_well_completion_ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_well_completion_ddf[\n",
    "    og_well_completion_ddf.lease_no_district_no == \"313711\"\n",
    "].compute().sort_values([\"tx_api_count\", \"tx_api\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 14 - Summary Master Large\n",
    "\n",
    "Summary table. (Used for query purposes at the operator level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"og_summary_master_large\"\n",
    ")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_summary_master_large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 15 - Summary Onshore Lease\n",
    "\n",
    "Summary table. (Used for query purposes on the leases in onshore counties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display_table_overview_from_frame(\n",
    "    persisted_overviews, data_tables_dict, \"og_summary_onshore_lease\"\n",
    ")\n",
    "get_table_card_from_frame(data_tables_dict, \"og_summary_onshore_lease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active leases\n",
    "og_summary_onshore_lease_ddf = data_tables_dict[\"og_summary_onshore_lease\"]\n",
    "# convert the column names to lower case\n",
    "og_summary_onshore_lease_ddf.columns = [\n",
    "    pascal_to_snake(col) for col in og_summary_onshore_lease_ddf.columns\n",
    "]\n",
    "# create a lease_no_district_no column\n",
    "og_summary_onshore_lease_ddf[\"lease_no_district_no\"] = og_summary_onshore_lease_ddf[\n",
    "    \"lease_no\"\n",
    "].astype(str) + og_summary_onshore_lease_ddf[\"district_no\"].astype(str).str.zfill(2)\n",
    "# get the number of rows and memory usage\n",
    "num_rows = og_summary_onshore_lease_ddf.shape[0]\n",
    "memory_usage = og_summary_onshore_lease_ddf.memory_usage(index=True, deep=True).sum()\n",
    "num_rows, memory_usage = dask.compute(num_rows, memory_usage)\n",
    "# show first few rows and shape\n",
    "print(\n",
    "    f\"Shape of og_summary_onshore_lease_ddf: ({num_rows} , {len(og_summary_onshore_lease_ddf.columns)}\"\n",
    ")\n",
    "print(f\"Total memory usage: {format_in_B(memory_usage)}\")\n",
    "og_summary_onshore_lease_ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active wells have a cycle_year_month_max  of 202311\n",
    "og_summary_onshore_lease_ddf[\n",
    "    og_summary_onshore_lease_ddf[\"cycle_year_month_max\"] == 202311\n",
    "].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Ranked Operators in Texas per Monthly Production numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the aggregated Operator cycle table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by cycle_year_month and get the top 5 operators for each of the prod_vol columns for 2023\n",
    "# filter for 20XX\n",
    "og_operator_cycle_ddf_20XX = og_operator_cycle_ddf[\n",
    "    og_operator_cycle_ddf.cycle_year_month > 201300\n",
    "]\n",
    "# get each of the prod_vol columns\n",
    "prod_columns = [\n",
    "    \"oper_oil_prod_vol\",\n",
    "    \"oper_gas_prod_vol\",\n",
    "    \"oper_cond_prod_vol\",\n",
    "    \"oper_csgd_prod_vol\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(og_operator_cycle_ddf_2023)\n",
    "# og_operator_cycle_ddf_2023.head()\n",
    "duplicate_operators = (\n",
    "    og_operator_cycle_ddf_20XX.groupby([\"cycle_year_month\", \"operator_name\"])\n",
    "    .size()\n",
    "    .compute()\n",
    ")\n",
    "\n",
    "# Check if any operator_name appears more than once in the same cycle_year_month\n",
    "any_duplicates = any(duplicate_operators > 1)\n",
    "\n",
    "print(\n",
    "    f\"Any operator_name appears more than once in the same cycle_year_month: {any_duplicates}\"\n",
    ")\n",
    "\n",
    "og_operator_cycle_ddf_20XX[\"total_prod_vol\"] = og_operator_cycle_ddf_20XX[\n",
    "    prod_columns\n",
    "].sum(axis=1)\n",
    "og_operator_cycle_ddf_20XX.loc[\n",
    "    (og_operator_cycle_ddf_20XX[\"cycle_year_month\"] == 202308)\n",
    "    & (og_operator_cycle_ddf_20XX[\"total_prod_vol\"] != 0)\n",
    "].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame\n",
    "og_operator_cycle_ddf_20XX = og_operator_cycle_ddf_20XX[\n",
    "    og_operator_cycle_ddf_20XX[\"cycle_year_month\"] <= 202308\n",
    "]\n",
    "\n",
    "# Group the DataFrame\n",
    "grouped_og_operator = og_operator_cycle_ddf_20XX.groupby(\"cycle_year_month\")\n",
    "\n",
    "\n",
    "# Define a function to get the top 3 rows for each product column\n",
    "def get_topn(df):\n",
    "    result = {}\n",
    "    for col in prod_columns:\n",
    "        topn = df.nlargest(1, col)[[\"operator_name\"] + prod_columns].copy()\n",
    "        topn[\"prod_column\"] = col\n",
    "        result[col] = topn\n",
    "    return pd.concat(result.values(), keys=result.keys())\n",
    "\n",
    "\n",
    "# Apply the function to each group\n",
    "topn_og_operator = grouped_og_operator.apply(\n",
    "    get_topn,\n",
    "    meta=pd.DataFrame(\n",
    "        {\n",
    "            \"operator_name\": pd.Series(dtype=\"object\"),\n",
    "            \"oper_oil_prod_vol\": pd.Series(dtype=\"float64\"),\n",
    "            \"oper_gas_prod_vol\": pd.Series(dtype=\"float64\"),\n",
    "            \"oper_cond_prod_vol\": pd.Series(dtype=\"float64\"),\n",
    "            \"oper_csgd_prod_vol\": pd.Series(dtype=\"float64\"),\n",
    "            \"prod_column\": pd.Series(dtype=\"object\"),\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Compute the result and sort by cycle_year_month\n",
    "result = topn_og_operator.compute().sort_values(\n",
    "    by=[\"cycle_year_month\", \"prod_column\"], ascending=False\n",
    ")\n",
    "result.reset_index().drop(columns=[\"level_1\", \"level_2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lease cycle table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare result to if we had worked from the lease production table\n",
    "\n",
    "# create column for the total production to filter out the non-producing operators\n",
    "og_lease_cycle_ddf[\"total_lease_prod_vol\"] = (\n",
    "    og_lease_cycle_ddf[\"lease_oil_prod_vol\"]\n",
    "    + og_lease_cycle_ddf[\"lease_gas_prod_vol\"]\n",
    "    + og_lease_cycle_ddf[\"lease_cond_prod_vol\"]\n",
    "    + og_lease_cycle_ddf[\"lease_csgd_prod_vol\"]\n",
    ")\n",
    "\n",
    "# get the lease production table and filter out the rows with cycle_year_month > 202308 and 0 production\n",
    "filtered_og_lease_cycle_ddf = og_lease_cycle_ddf.loc[\n",
    "    (og_lease_cycle_ddf.cycle_year_month <= 202308)\n",
    "    & (og_lease_cycle_ddf[\"total_lease_prod_vol\"] > 0)\n",
    "]\n",
    "\n",
    "# group by the operator and the cycle_year_month and sum the prod_vol columns\n",
    "grouped_og_operator_no = (\n",
    "    filtered_og_lease_cycle_ddf[\n",
    "        [\n",
    "            \"cycle_year_month\",\n",
    "            \"operator_no\",\n",
    "            \"lease_oil_prod_vol\",\n",
    "            \"lease_gas_prod_vol\",\n",
    "            \"lease_cond_prod_vol\",\n",
    "            \"lease_csgd_prod_vol\",\n",
    "        ]\n",
    "    ]\n",
    "    .groupby([\"cycle_year_month\", \"operator_no\"])[\n",
    "        \"lease_oil_prod_vol\",\n",
    "        \"lease_gas_prod_vol\",\n",
    "        \"lease_cond_prod_vol\",\n",
    "        \"lease_csgd_prod_vol\",\n",
    "    ]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active leases are those which have a production for 202308\n",
    "active_leases = (\n",
    "    filtered_og_lease_cycle_ddf[[\"cycle_year_month\", \"lease_no_district_no\"]][\n",
    "        filtered_og_lease_cycle_ddf[\"cycle_year_month\"] == 202308\n",
    "    ][\"lease_no_district_no\"]\n",
    "    .unique()\n",
    "    .compute()\n",
    ")\n",
    "\n",
    "# get the cumulative production for each of active_leases\n",
    "active_og_lease_cycle_ddf = filtered_og_lease_cycle_ddf[\n",
    "    filtered_og_lease_cycle_ddf[\"lease_no_district_no\"].isin(active_leases)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persisted_active_og_lease_cycle_ddf = active_og_lease_cycle_ddf.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lease_prod_cols = [\n",
    "    \"lease_oil_prod_vol\",\n",
    "    \"lease_gas_prod_vol\",\n",
    "    \"lease_cond_prod_vol\",\n",
    "    \"lease_csgd_prod_vol\",\n",
    "]\n",
    "filtered_og_lease_cycle_ddf[\n",
    "    filtered_og_lease_cycle_ddf[\"lease_no_district_no\"] == 313711\n",
    "][lease_prod_cols + [\"cycle_year_month\", \"operator_no\"]].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lease_prod_cols = [\n",
    "    \"lease_oil_prod_vol\",\n",
    "    \"lease_gas_prod_vol\",\n",
    "    \"lease_cond_prod_vol\",\n",
    "    \"lease_csgd_prod_vol\",\n",
    "]\n",
    "active_lease_sums = (\n",
    "    persisted_active_og_lease_cycle_ddf.groupby(\"lease_no_district_no\")[lease_prod_cols]\n",
    "    .sum()\n",
    "    .compute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top producing leases\n",
    "for col in lease_prod_cols:\n",
    "    display(active_lease_sums.nlargest(10, col))\n",
    "# active_lease_sums.nlargest(10, \"lease_oil_prod_vol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_grouped_og_operator_no = grouped_og_operator_no.compute()\n",
    "\n",
    "lease_prod_cols = [\n",
    "    \"lease_oil_prod_vol\",\n",
    "    \"lease_gas_prod_vol\",\n",
    "    \"lease_cond_prod_vol\",\n",
    "    \"lease_csgd_prod_vol\",\n",
    "]\n",
    "\n",
    "# create a empty list to store the max rows\n",
    "max_rows = []\n",
    "# go through each of the prod_columns and get the max row for each cycle_year_month\n",
    "for col in lease_prod_cols:\n",
    "    computed_grouped_og_operator_no[\"prod_column\"] = col\n",
    "    max_index = computed_grouped_og_operator_no.groupby(\"cycle_year_month\")[\n",
    "        col\n",
    "    ].idxmax()\n",
    "\n",
    "    max_rows.append(computed_grouped_og_operator_no.loc[max_index])\n",
    "\n",
    "result = pd.concat(max_rows)\n",
    "# merge the operator_no_name table to get the operator_name\n",
    "result.merge(operator_no_name, on=\"operator_no\").sort_values(\n",
    "    by=[\"cycle_year_month\", \"prod_column\"], ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = client.scheduler_info()  # get scheduler info\n",
    "\n",
    "workers = info[\"workers\"]  # get the workers\n",
    "\n",
    "print(f\"Number of workers: {len(workers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
